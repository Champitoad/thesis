\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\section{Interactive theorem provers}\labsec{itps}

\todo{ Explain the concept of tactic, and how it relates to traditional
  inference rules when read bottom-up.}

\section{Proof theory}

\section{This thesis}

In this thesis, we are concerned mainly with the question of providing a
\emph{direct-manipulation} interface for building formal proofs
\emph{dynamically} and \emph{incrementally}. To this effect, we explore new ways
to represent and interact with the \emph{proof state}, that is the mental state
in which a human finds herself when searching for, or unrolling the steps of a
proof. Following the LCF tradition of proof assistants
\cite{doi:10.1098/rsta.1984.0067}, we assume that the underlying data modelling
this mental state consists at its core of a set of \emph{conclusions} waiting
for evidence, each having their own set of \emph{hypotheses} or
\emph{assumptions}.

Another important data is the \emph{arguments} used for already justified
conclusions. Typically one might want a \emph{static} representation of
arguments, to ease reviewing, modification or reuse of a proof at a later time.
These activities are generally regrouped under the term of \emph{proof
evolution}. Until the very last chapter, we will not consider the question of
having such a static representation. Our hope is that once we find an adequate
representation for the proof state geared towards \emph{dynamic} manipulation,
the sole data of the \emph{transitions} between states through a set of
canonical \emph{proof actions} will be enough to reconstruct both a
machine-readable formal proof object, and a human-readable proof text.

\section{Traditional approach}

\textbf{Summary:} Instead of designing an interface that facilitates the
learning and usage of a traditional proof language (natural deduction, sequent
calculus), we aim for an interface that facilitates logical reasoning broadly
construed.

In a sense, our methodology for designing proving interfaces goes in the
opposite direction compared to traditional approaches.

Indeed, one usually starts with a canonical static representation of partial
proofs\sidenote{e.g. $λ$-terms with meta-variables in dependently-typed proof
assistants like Coq, Lean and Agda, or a trusted base of tactics in simply-typed
proof assistants like HOL.}, and then builds user-facing abstractions on top of
it. These include proof languages which can be more or less
declarative\sidenote{A very declarative proof language would be that of the
Mizar proof assistant, while a more imperative approach can be found in tactic
languages, such as Ltac in the Coq proof assistant. There are also intermediate
solutions, like the Isar proof language implemented in the Isabelle/Isar system,
which is both declarative and executable interactively.}, but also extensible
notation and macro systems \cite{ullrich_beyond_2022} for concise,
human-readable representations of mathematical objects. Ultimately, all these
abstractions are compiled to low-level proof objects in the kernel language, a
process called \emph{elaboration}.

Our point is that the initial choice of kernel language can influence the design
of higher-level abstractions, either unconsciously or consciously: typically,
one will want to stay close to the kernel representation in order to mitigate
the complexity of the elaboration process, since it could impact negatively the
speed and memory usage of the system. This is especially true of abstractions
used in structuring the purely logical, non-domain specific parts of proofs,
since those occur in virtually every development. This can be observed at two
different levels:
\begin{itemize}
  \item the level of \emph{statements}, where specific constructors (typically
  logical connectives) need a specific interface, and are not always treated
  uniformly\todo{example of various tactics for the "same" action (intro/elim)
  but with differing syntax (\texttt{intros} vs. \texttt{split}, \texttt{apply}
  vs. \texttt{destruct}...), and of their shortcomings (\texttt{apply} only
  works with $∀$ and $\limp$)};
  \item the level of \emph{inference rules}, which are usually modelled after
  standard \kl{proof formalisms} such as natural deduction and sequent
  calculus\sidenote{this is especially true for dependently-typed proof
  assistants, where the correspondance with natural deduction is immediate
  through the Curry-Howard isomorphism.}, and where many pervasive commands for
  manipulating the proof state map directly to these rules\sidenote{This applies
  again mostly to dependently-typed systems: Agda is a pathological example, in
  that one writes directly proof terms which are identified with the proof state.
  More to the point are Coq, Lean and HOL, which share similar sets of
  primitives modelled after the \emph{introduction/elimination} rule duality.
  Declarative proof language like Isar seem to make an explicit effort to
  avoid this schema, by following a higher-level approach based on forward
  rather than backward reasoning, which is closer to mathematical practice.}. 
\end{itemize}

This focus on the formal, low-level proof object is likely rooted in the
historical origins of proof assistants, with projects such as Automath, Mizar
and LCF\todo{add citations}. Retrospectively, they shared a common conceptual
framework which was the direct consequence of two scientific and technological
developments:
\begin{itemize}
  \item the advent of mathematical logic and proof theory at the dawn of the
  20\textsuperscript{th} century, motivated by a foundational crisis concerning
  first the unity of various branches of mathematics, and then the very notion
  of mathematical \emph{truth} following the discovery of Russell's paradox;
  \item the invention and democratization of computers, and more specifically of
  \emph{programming languages} as their principal means of interaction with
  humans.
\end{itemize}

However, programming languages are known to incur a great learning curve on
users unfamiliar with programming and algorithmic thinking. Even without the
algorithmic aspect, which is usually absent from proof languages, the use of a
formal language with a rigid syntax is problematic for most users at the
learning stage. And the foundational nature of standard \kl{proof formalisms}, on
which most proof languages are based, only accentuates this fact by exposing
low-level primitives which do not capture the way mathematicians build and think
about proofs. Indeed, those were designed to solve foundational
questions\sidenote{For instance, Gentzen invented natural deduction, and later
sequent calculus in order to obtain finitist proofs of soundness
for basic systems of arithmetic, through his well-known \textit{Haupstatz} or
\emph{cut-elimination} theorem.} rather than model the way mathematicians
effectively reason.

\section{Modern interfaces}

We argue that this historical influence has created a \emph{blind spot} for a
more modern approach to proving interfaces. Nowadays, the vast majority of
interactions between humans and computers happen through \emph{graphical user
interfaces}, which have proved to make software systems a lot more intuitive and
accessible to people without any specific training or education. This is likely
one of the important roadblocks towards a wider adoption of proof assistants by
researchers and students in mathematics, but also in other domains where formal
logical thinking could be applied such as the modelling and verification of
computer systems, or even legal systems.

\section{Existing solutions to overcome poor proof languages}

\begin{itemize}
  \item Maximizing \emph{custom automation} with a rich domain-specific language
  for implementing proof search procedure (Ltac). Works well in the domain of
  program verification, where one can rely on both the rich inductive structure
  of object programs, and the programming expertise of the user. But less suited
  to the structure of standard mathematics, which are built around the rich
  logical interleaving of theorems and definitions, rather than complex
  inductive definitions. Or more sociologically, does not resemble the actual
  practice of mathematicians on paper, and is unusable by non-programmers, which
  constitute a large portion of potential users in both research and education.
  \item More \emph{declarative} proof languages, agnostic to the underlying
  logic (Isar). Solves most of the previously mentioned flaws, but is still very
  verbose and sensitive to syntax errors, and does not tackle the question of a
  more guided interface, which could improve both the learning curve and the
  speed of the proof building process.
\end{itemize}

\section{Roadmap}

In this thesis, we start our exploration in the design of proof-building
interfaces with graphical interaction principles inspired by standard approaches
to logic and proof theory: statements are represented by symbolic formulas made
from the usual logical connectives, and inference rules are in direct
correspondance, or simulate closely the flow of argumentation found in Gentzen's
sequent calculus.

Then we gradually stray away from tradition, the first step being the adoption
of a more flexible approach to argumentation as offered by the \emph{deep
inference} paradigm. This allows us to build proofs through \emph{drag-and-drop}
actions with the \emph{subformula linking} technique of K. Chaudhuri, and even
to imagine a nested and metaphorical representation of \emph{subgoals} with our
own \emph{bubble calculus}. But there are still some defects in these systems,
which confine the reasoner to weird habits that restrict their freedom.

Finally we fully embrace the idea of manipulating directly iconic subgoals
instead of symbolic formulas, by reviving an old system from the genius and
avant-gardist logician C. S. Peirce: the \emph{existential graphs}. We add a
layer of playfulness and intuitionism by devising the \emph{flower calculus}, a
fun and alternative way to draw and manipulate the graphs as flowers connected
by sprinklers in nested gardens. But more importantly, the flower calculus
exhibits the latent \emph{analyticity} of Peirce's graphs, thus linking them
with the systems of Gentzen despite a very different approach to the
representation and use of statements. This allows us to design a new kind of
direct manipulation interface that merges very tightly two concepts/structures,
which have always been segregated ontologically: formulas and proofs. Hopefully,
this foundation will enable new ways to build and maintain formal proofs, that
are less buraucratic and more enjoyable to humans.

\todo{Explain the concept of iconicity somewhere}

\section{How to read this thesis}

\todo{ Introduce a special format for long digressions in sidenotes, so that the
reader can safely ignore them. }

\todo{ Advise the reader to read with a PDF reader that supports color,
  hyperlink jumping and hyperlink preview. We tend to cross-reference a lot
  ideas from different chapters. }