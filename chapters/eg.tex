\setchapterpreamble[u]{\margintoc}
\chapter{Existential Graphs}
\labch{eg}

\epigraph{The System of Existential Graphs which I have now sufficiently
described --- or, at any rate, have described as well as I know how, leaving the
further perfection of it to others --- greatly facilitates the solution of
problems of Logic, as will be seen in the sequel, not by any mysterious
properties, but simply by substituting for the symbols in which such problems
present themselves, concrete visual figures concerning which we have merely to
say whether or not they admit certain describable relations of their parts.
Diagrammatic reasoning is the only really fertile reasoning. If logicians would
only embrace this method, we should no longer see attempts to base their science
on the fragile foundations of metaphysics or a psychology not based on logical
theory; and there would soon be such an advance in logic that every science
would feel the benefit of it.}{\textbf{Charles S. Peirce}, \textit{Prolegomena
to an Apology for Pragmaticism}, 1906}


\begin{scope}\knowledgeimport{eg}


C. S. Peirce is famous for his contributions to \kl{symbolic} logic, including
among others his eponymous law for \kl{classical} logic, and his pioneering work
on the algebra of relations and quantification \cite{peirce_algebra_1885}. But
far less widespread are his achievements in the realm of \kl{diagrammatic}
logic, or \emph{\kl{iconic} logic} as Shin calls it
\cite{10.7551/mitpress/3633.001.0001}. He dedicated a large chunk of his life to
the investigation of graphical systems, starting in 1882 with the
\emph{\kl{entitative graphs}} and culminating with the \emph{\kl{existential
graphs}} (\kl{EGs}), which he developed from 1896 until his death in 1914
\sidecite{Roberts+1973}. Interestingly, Peirce perceived \kl{existential graphs}
as his \textit{``chef d'oeuvre''}, and that they \textit{``ought to be the logic
of the future''}\sidenote{Both citations are sourced in
\cite[p.~11]{Roberts+1973}.}.

Recent works have started to realize this vision: for example Sowa based his
conceptual graphs for computerized knowledge representation on \kl{EGs}
\sidecite{sowa_conceptual_1976}; Brady, Trimble
\sidecite{brady_categorical_2000,brady_string_nodate}, Gianluca, Rocco
\sidecite{caterina_new_2020} and Haydon, Sobociński
\sidecite{pietarinen_compositional_2020} proposed various reconstructions of
\kl{EGs} through the lens of \emph{topology} and \emph{category theory}; lastly,
Melliès, Zeilberger \sidecite{mellies_bifibrational_2016} and Bonchi et al.
\sidecite{bonchi_diagrammatic_2024} refined respectively the interpretations of
\cite{brady_string_nodate} and \cite{pietarinen_compositional_2020} by making
further connections with \emph{linear logic} \sidecite{girard-linear-1987} and
\emph{linear bicategories}. The full story has yet to be told, but we hope that
our work will constitute one more step towards the vision Peirce had in mind.

In this chapter, we propose a self-contained exposition of \kl{EGs}, that tries
at the same time to be faithful to the original presentation of the systems by
Peirce, and more modern in some aspects of their formalization. The goal will be
to familiarize the reader with the unique approach to proofs inherent to
\kl{EGs}, which can be difficult to relate to more standard frameworks like
\kl[Hilbert system]{Hilbert} and \kl[sequent calculus]{Gentzen} \kl{proof
systems}, and even \kl{deep inference} \kl{proof systems} like the \kl{calculus
of structures}. This shall prove useful to get a good understanding of the
historical and technical foundations behind our \emph{\kl{flower calculus}}, to
be introduced in \refch{flowers}.

\newpage

The chapter is organized as follows: we start in \refsec{alpha} by presenting
the \kl{diagrammatic} syntax of the system \kl{Alpha} of \kl{EGs} for
\kl{classical} propositional logic. In \refsec{illative}, we introduce the
\kl{inference rules} of \kl{Alpha} for manipulating \kl{EGs}, called
\emph{\kl{illative transformations}} by Peirce. In \refsec{multisets}, we give
an equivalent formulation of the syntax and rules of \kl{Alpha} as a
\emph{multiset} \kl{rewriting system}. In \refsec{atomicity}, we formalize a
variant of the \kl[Iteration]{(De)iteration} principle described by Peirce in
\sidecite{peirce_prolegomena_1906} that eliminates the need for the
\kl{Double{-}cut} principle, and discuss how it was motivated by Peirce's quest
for \emph{\kl{illative atomicity}}. In \refsec{eg-soundness}, we take advantage
of our reformulation to give a simple proof of soundness for \kl{Alpha}, based
on a direct truth-evaluation of graphs. In \refsec{eg-completeness} we give a
syntactic proof of completeness for \kl{Alpha}, by simulating the \kl{calculus
of structures} \kl{SKS} of Brünnler and Tiu \sidecite{brunnler_local_2001}. In
this way, \kl{Alpha} is shown to have subsystems that inherit the
\emph{locality} property of \kl{SKS}, and where the \kl{Deletion} and
\kl{Insertion} rules are respectively \kl{admissible} for \emph{provability} and
\emph{refutability}, making \kl{Alpha} \emph{\kl{analytic}}. In \refsec{beta},
we illustrate the original mechanism of \emph{\kl{lines of identity}} used by
Peirce to handle quantifiers and equality in his \kl{Beta} system. We end in
\refsec{gardens} by showing how to recast \kl{lines of identity} in a more
traditional binder-based syntax.

\section{Alpha graphs}\labsec{alpha}

\AP
Peirce designed in total three systems of \kl{EGs}, which he called respectively
\intro{Alpha}, \intro{Beta} and \intro{Gamma}. They were invented
chronologically in that order, which also captures their relationship in terms
of complexity: \kl{Alpha} is the foundation on which the other systems are
built, and can today be understood as a \kl{diagrammatic} calculus for \kl{classical}
\emph{propositional} logic. As we will see in \refsec{beta}, \kl{Beta}
corresponds to a variable-free representation of \emph{predicate} logic without
function symbols, and with primitive support for \emph{equality}. The last
system \kl{Gamma} is more experimental, with various unfinished features that
have been interpreted as attempts to capture \emph{modal}
\sidecite{zeman_graphical_1964} and \emph{\kl{higher-order}} logics.

\begin{digression}
At the end of his life, Peirce pushed his experimentations beyond the scope of
\emph{logic} in the contemporary sense of the word, with so-called
\emph{tinctured} \kl{existential graphs} \cite[Chapter~6]{Roberts+1973}.
Roughly, the idea was to represent a variety of \emph{modes of expression} with
different background shades on the \kl{sheet of assertion}, not unlike our
graphical depiction of \emph{\kl{saturated}} \kl(symm){solutions} in
\reffig{graphical-B}, or the background colors used for the various kinds of
text boxes in this document. In addition to the usual act of asserting the truth
of a proposition, one could for instance express a \emph{subjective} or
\emph{objective} possibility, or signify an interrogative or imperative mood,
all by using different colors. For print in publications, he would in fact use
\emph{heraldic tinctures} instead of colors, hence the ``tinctured''
qualificative. The precise rules, meaning and purpose of tinctured \kl{EGs}
remain elusive to this day, and might constitute the most esoteric part of
Peirce's work.
\end{digression}
% We see a possible connection with the \kl{type-theoretical} concept of
% \emph{judgment}, which was rehabilitated by Martin-Löf from the philosophy of
% Kant \cite{Martin-Lof1996-MAROTM-7}, who was of great influence on Peirce's own
% philosophy \cite{sep-peirce}.}.

\subsection{Icons}

\paragraph{Sheet of Assertion}

\AP The most fundamental concept of \kl{Alpha} is the \intro{sheet of
assertion}, denoted by $\intro*\SA$ thereafter. It is the space where statements
are scribed by the reasoner, typically a sheet of paper, a blackboard, or a
computer display. In a \kl{proof assistant}, this would either be the buffer of
a text editor where the user writes her theories, or the \kl{proof view}
displaying \kl{goals} to be proved, depending on who the reasoner is (the user
or the computer, respectively). This last analogy suggests an important property
of $\SA$: it must offer a \emph{virtually infinite} amount of space, so that one
can perform as much reasoning as needed. Just like a Turing machine has an
infinite tape, so that one can perform as much computation as needed. In
\kl{symbolic} logic, this is captured by the fact that formulas, although
usually finite, can have an unbounded size.

\newpage
As its name indicates, scribing a statement on $\SA$ amounts to \emph{asserting
its truth}. Thus very naturally, the empty $\SA$ where nothing is scribed will
denote vacuous truth, traditionally \kl{symbolized} by the formula $\top$.

% \begin{remark}
%   Peirce had an \emph{externalist} conception of truth, where the assertions
% made by the \process{Graphist}, i.e. the person scribing on $\SA$, refer to the
% world \emph{outside} of the sheet (the universe of discourse). By not scribing
% anything, the \process{Graphist} thus refrains from asserting anything about the
% world, only assuming implicit truths. This is illustrated by the following
% quote, reminiscent of an interaction between a \kl{proof assistant} (the
% \process{Graphist}) and its user (\process{the Interpreter})
% \cite[p.~92]{Roberts+1973}: \textit{``One of the parties, called the Graphist,
% is responsible for scribing the original graphs at the beginning of the
% investigation or discussion; the other, called the Interpreter, draws inferences
% from these graphs by changing them in accordance with the permissions of the
% system. The [...] sheet, before anything is scribed on it, represents whatever
% is taken for granted at the outset by the Graphist and Interpreter''}.
% \end{remark}

\paragraph{Juxtaposition}

As we know from \kl{natural deduction}, asserting the truth of the conjunction $a
\land b$ of two propositions $a$ and $b$, amounts to asserting \emph{both} the
truth of $a$ and the truth of $b$. In \kl{Alpha}, there is no need to introduce
the \kl{symbolic} connective $\land$, since one can just write both $a$ and $b$ at
distinct locations on $\SA$:
$$a~~~b$$
\AP
More generally, one might consider any two portions $G$ and $H$ of $\SA$, and
interpret their \intro{juxtaposition} $G~H$ as signifying that we assert the
truth of their conjunction.
% This leads us to formulate the first fundamental principle of \kl{Alpha}:
% $$\text{\emph{Any portion of $\SA$ is a graph.}}$$
% In particular, the entire $\SA$ itself is a graph.

\paragraph{Cuts}

\AP
Asserting the truth of the negation $\neg a$ of a proposition $a$, amounts to
\emph{denying} the truth of $a$. Using the original notation of Peirce, this is
done in \kl{Alpha} by \emph{enclosing} $a$ in a closed curve like so:
$$\pcut{a}$$
Peirce called such curves \intro{cuts}\sidenote{Not to be confused with the name
given to instances of the \kl(rule){cut} rule in \kl{sequent calculus}.}, because
they ought to be seen as literal \kl{cuts} in the paper sheet that embodies
$\SA$. Note that they do not need to be circles: all that matters is that $a$ is
in a separate area from the rest of $\SA$. This is precisely the content of the
\emph{Jordan curve theorem} in topology, and thus we can take \kl{cuts} to be
arbitrary Jordan curves. This entails in particular that \kl{cuts} cannot
intersect each other, but can be freely nested inside each other. Then as for
\kl{juxtaposition}, one can replace $a$ by any \intro{graph} $G$ --- i.e. any
portion of $\SA$ --- as long as the \kl{cut} does not intersect other \kl{cuts}
in $G$.

\subsection{Relationship with formulas}

With just these two \kl{icons}, \kl{juxtaposition} and \kl{cuts}, one can
therefore assert the truth of any proposition made up of conjunctions and
negations and built from atomic propositions. Importantly, the only \kl{symbols}
needed for doing so are letters $a, b, c\ldots$ denoting atomic propositions,
that is ``pure'' \kl{symbols} that do not have any logical meaning associated to
them.

\AP Now, it is well-known that $\{\land,\neg\}$ is \emph{functionally complete},
meaning that any boolean truth function can be expressed as the composition of
boolean conjunctions and negations. In particular, the \kl{symbolic} definitions
of absurdity $\bot \defeq \neg\top$, \kl{classical} disjunction $A \lor B \defeq
\neg(\neg A \land \neg B)$ and \kl{classical} implication $A \limp B \defeq \neg
(A \land \neg B)$ can be expressed by the following three
\kl{graphs}\sidenote{Note the resemblance with the translation of formulas as
\kl(symm){solutions} in \reffig{bubbles-native}, in particular for
\kl(bubble){negative} disjunctions.}:
\begin{equation*}
  \pcut{\phantom{A}}
  \quad\quad\quad
  \pcut{\pcut{A}~~~\pcut{B}}
  \quad\quad\quad
  \pcut{A~~~\pcut{B}}
\end{equation*}
Thus one can easily encode any propositional formula into a \kl{classically}
equivalent \kl{graph}. Conversely, one can translate any \kl{graph} into a
\kl{classically} equivalent formula, as has been shown for instance in
\sidecite{10.7551/mitpress/3633.001.0001}. In fact, there are usually many
possible formula readings of a given \kl{graph}. One reason is that
\kl{juxtaposition} of \kl{graphs} is a \emph{variadic} operation, as opposed to
conjunction of formulas which is \emph{dyadic}: thus formulas that only differ
up to \emph{associativity} are associated to the same \kl{graph}. Also, thanks
to the topological nature of $\SA$, \kl{juxtaposition} is naturally
\emph{commutative}: the locations of two juxtaposed \kl{graphs} do not matter,
as long as they live in the same area delimited by a \kl{cut}. \AP The
combination of these properties is called the \intro{isotropy} of $\SA$ in
\sidecite{minghui_graphical_2019}, and is captured in traditional \kl{proof
theory} through the use of \emph{(multi)sets} for modelling
\kl(sequent){contexts} in sequents.

\begin{remark}\labremark{eg-entitative}
  \AP
  In a first version of \kl{EGs} called \intro{entitative graphs}, Peirce used
  \kl{juxtaposition} to denote \emph{disjunction} instead of conjunction.
  Although $\{\lor,\neg\}$ is also functionally complete, Peirce quickly grew
  unsatisfied with these \kl{entitative graphs}, stating that \kl{EGs} formed
  ``a far preferable system on the whole'' (Ms 280, pp.~21--22). I find it
  interesting that more contemporary works in logic have also made the choice to
  take conjunction and negation as their primitive operations, like the
  tensorial logic of Melliès \cite{mellies_micrological_2017}, or the
  realizability constructions for linear logic in Girard's transcendental syntax
  \cite{eng_stellar_2020}.
\end{remark}

\section{Illative transformations}\labsec{illative}

\paragraph{Deep inference}

\AP In order to have a \kl{proof system}, one needs a collection of
\emph{\kl{inference rules}} for deducing true statements from other true
statements. In \kl{Alpha}, \kl{inference rules} are implemented by what Peirce
called \intro{illative transformations} on \kl{graphs}. In modern terminology,
they correspond to \kl{rewriting rules} that can be applied to any
\kl{subgraph}. By measuring the \kl{depth} of a \kl{subgraph} as the number of
\kl{cuts} in which it is enclosed, we thus have that the rules of \kl{Alpha} are
applicable on \kl{subgraphs} of arbitrary \kl{depth}. This makes \kl{Alpha} deserving
of the title of \kl{deep inference} system.

\paragraph{Polarity}

\begin{marginfigure}
  $$\ncut{a~~~\pcut{\ncut{b}~~~c}}$$
  \caption{Peirce's notation for emphasizing negative areas}
  \labfig{peirce-neg-areas}
\end{marginfigure}

\begin{marginfigure}
  $$\oncut{a~~~\opcut{\oncut{b}~~~c}}$$
  \caption{Drawing negative areas literally in negative}
  \labfig{our-neg-areas}
\end{marginfigure}

\AP Before introducing the rules, let us make a small change in the way we
depict the \kl{graphs}. The idea is that we want to visualize more clearly the
\kl{polarity} of any \kl{subgraph} $G$, understood as the \emph{parity} of the
number of \kl{cuts} (negations) enclosing $G$. In one of his unpublished
manuscripts (Ms 514), Peirce did this by \emph{shading} \kl{negative} areas ---
those enclosed in an odd number of \kl{cuts} --- in gray, as illustrated in
\reffig{peirce-neg-areas} \sidecite{sowa_peirces_2011}. Unconstrained by
hand-drawing, one could adopt an even more \kl{iconic} notation, where
\kl{negative} areas are \emph{literally} drawn like a \kl{negative} in
photography, by inverting white and black. The example of
\reffig{peirce-neg-areas} would then be drawn as in \reffig{our-neg-areas}.
However in this thesis, we will stick to Peirce's notation, which is both less
straining for the eyes by being less contrasted, and more economical in ink for
print.

A nice advantage of these notations is that they remove the need to count
manually the number of \kl{cuts} starting from the top-level of $\SA$: the
information is immediately apparent in the \kl{subgraph}, and thus completely
\emph{local}\sidenote{A similar device is used in the \kl{deep inference} system
\intro{ISp} of Tiu \cite{tiu_local_2006}, where the polarities of substructures
are attached to them as explicit labels.}.

\begin{remark}\labremark{eg-polarity}
  Whereas in \kl{bubble calculi} the concept of \kl(bubble){polarity} was
  understood as a property of \emph{objects} --- i.e. utterances of propositions
  --- by assigning them opposite colors (blue and red), the previous notations
  for \kl{graphs} suggest that it is instead a property of the \emph{space} in
  which objects reside. This is more natural from the point of view of
  \emph{game semantics}: for instance in a game of chess, the two players can
  easily exchange their roles by switching places or rotating the board by 180°,
  rather than by repainting laboriously each piece in the opposite color.
\end{remark}

\paragraph{Inference rules}

Quite surprisingly, Peirce showed that one only needs five \kl{inference rules}
to get a \emph{strongly complete} system, in the sense that if the truth of a
\kl{graph} $G$ entails the truth of another \kl{graph} $H$, then $G$ can always
be rewritten into $H$ by applying exclusively instances of these five
rules\sidenote{Of course Peirce did not show completeness formally in the sense
of modern \kl{model theory}, although Sowa argues in
\cite[Section~4]{sowa_peirces_2011} that he had started to develop his own
\kl{model theory} equivalent to Tarski's (but closer to the
\emph{game-theoretical semantics} of Hintikka \cite{Hintikka1973-HINLLA}).
% One can find a modern categorical treatment of completeness with respect to
% Boolean algebras, based on a rigorous formalization of the geometry and algebra
% of \kl{Alpha}, in \cite{brady_categorical_2000}.
}. A nice way to understand the rules of \kl{Alpha} is as \emph{edition
principles}, like the most basic \kl{actions} one executes pervasively when editing
text on a computer\sidenote{Even though computers did not exist yet in Peirce's
time! In fact, Martin Irvine argues in \cite{irvine_semiotics_2022} that Peirce
anticipated many developments in computer science and information technologies,
such as the use of electrical switches to compute boolean functions, whose
invention is usually attributed to Claude Shannon.}. The first two rules are the
most powerful and mysterious in all systems of \kl{EGs}, and can be applied in areas
of any \kl{polarity}:
\begin{description}
  \itemAP[\intro{Iteration} \textit{(Copy \& Paste)}]
    A \kl{graph} $G$ may be duplicated at any \kl{depth} inside of a juxtaposed \kl{graph} $H$.
    Using our notation for holed \kl{contexts} from previous chapters, this can be
    represented schematically like so:
    \begin{mathpar}
      G~~~H\select{\phantom{G}} ~~~\step{}~~~ G~~~H\select{G}
      \and
      \nsheet{G~~~H\select{\phantom{G}} ~~~\step{}~~~ G~~~H\select{G}}
    \end{mathpar}
    % In particular, $H$ can be taken to be any empty portion of $\SA$ in the same
    % area as $G$, giving that $G ~\step{}~ G~G$.
    It can be seen as a deep generalization of the \kl(rule){axiom} rule of
    \kl{sequent calculus}, where the top-level occurrence of $G$ justifies the
    occurrence of $G$ located inside $H\hole$\sidenote{This might also be
    related to the notion of \emph{justified move} in game semantics, where the
    nesting of \kl{cuts} in the \kl{context} $H\hole$ corresponds to a sequence
    of alternating moves between Player and Opponent.}. Note that while in the
    \kl(rule){axiom} rule, the justifying (resp. justified) occurrence must be a
    \kl{negative} hypothesis (resp. a \kl{positive} conclusion), the
    \kl{Iteration} rule also allows the opposite relationship of a conclusion
    justifying a hypothesis, thus exhibiting one aspect of the \kl(rule){cut}
    rule of \kl{sequent calculus}.
  \itemAP[\intro{Deiteration} \textit{(Factorization)}]
    Formally, this is the converse of \kl{Iteration}:
    \begin{mathpar}
      G~~~H\select{G} ~~~\step{}~~~ G~~~H\select{\phantom{G}}
      \and
      \nsheet{G~~~H\select{G} ~~~\step{}~~~ G~~~H\select{\phantom{G}}}
    \end{mathpar}
    Its interpretation as an edition principle is a bit trickier, but it can be
    understood as a form of \emph{sharing} of information. Indeed, it roughly
    says that a \kl{subgraph} $G$ can be erased if it already occurs ``higher'' on
    $\SA$. Also this does precisely the opposite of copy-pasting, which is known
    in software engineering as \emph{factorization}\sidenote{This is closely
    related to the kind of factorization at work in \kl{bubble calculi}. In
    particular, the fact that the factorizing occurence is higher and usually
    outside of a \kl{cut} is very reminiscent of the \emph{outward} flow rules of
    \kl{system~B} (those whose name ends with $\ua$ in
    \reffig{sequent-B}); and the deduplicating effect makes \kl{Deiteration}
    even closer to the variant of the same rules in \kl{Binv}
    (\reffig{sequent-B-inv}).}.
    
    Compared to \kl{sequent calculus}, it can be seen as a deep generalization of the
    \emph{\kl{contraction}} rule, the base case where $H\hole = \hole$ giving $G~G
    ~\step{}~ G$.
\end{description}
The applicability of the next two rules depends on the \kl{polarity} of the
\kl{subgraph}'s area:
\begin{description}
  \itemAP[\intro{Insertion}]
    Any \kl{graph} $G$ may be inserted in a \kl{negative} area:
    $$\nsheet{~~~\step{}~~~ G}$$
    This is akin to a \emph{\kl{weakening}} rule, stating that one might add
    (useless) hypotheses at will. The closest equivalent we found in the
    \kl{deep inference} literature is indeed the \kl{weakening} rule
    \rsf{wl{\da}} of \kl{ISp} in \sidecite{tiu_local_2006}.
  \itemAP[\intro{Deletion}]
    Any \kl{graph} $G$ occurring in a \kl{positive} area may be erased:
    $$G ~~~\step{}~~~$$
    
    This is exactly the dual of \kl{Insertion}, stating that if a proposition is
    known to be true, then one might as well refrain from asserting it. It is
    the only \emph{non-\kl{analytic}} rule of the system when reading rules from
    conclusion to premiss, since $G$ does not already appear in the right-hand
    side. It is thus strongly related to the \kl(rule){cut} rule of \kl{sequent
    calculus}, which it can simulate together with the \kl{Deiteration} rule.
    % It is strongly related to the cut rule of \kl{sequent calculus}, as will be
    % demonstrated shortly.
\end{description}
The last rule is more of a \emph{space management} principle that works as an
\emph{isotopy}, i.e. a bidirectional topological deformation:
\begin{description}
  \itemAP[\intro{Double{-}cut}]
    A \emph{double-\kl{cut}} may be inserted or erased around any \kl{graph} $G$:
    \begin{mathpar}
      \ncut{\pcut{G}} ~~~\leftrightarrow~~~ G
      \and
      \nsheet{\pcut{\ncut{G}} ~~~\leftrightarrow~~~ G}
    \end{mathpar}
    The bidirectional arrow $\leftrightarrow$ expresses that the rule can be
    applied in both directions.
    % \sidenote{We could have merged \kl{Iteration} and \kl{Deiteration} in this
    % way, but chose to follow the original presentation of the rules by Peirce
    % instead, as exposed in \cite{Roberts+1973}.}
    Logically, this corresponds to the \kl{classical} equivalence $\neg\neg A
    \semequiv{} A$, where in particular the deletion direction $\neg\neg A \limp A$
    is not true \kl{intuitionistically}. Topollogically, the double-\kl{cut} forms a
    \emph{ring}, that separates $G$ from the rest of $\SA$ while preserving its
    \kl{polarity}. Then the two directions of the rules can be understood as the
    following dual \emph{homotopies}:
    \begin{description}
      \item[Contraction] The ring is created by cutting $\SA$ around
      $G$, and then \emph{contracting} the inner area where $G$ resides on
      itself. This effectively ``pulls apart'' $G$ from the rest of the sheet,
      leaving apparent in the empty space of the ring whatever lies behind
      $\SA$. Peirce thought of \kl{positive} and \kl{negative} areas as being the
      \emph{recto} and \emph{verso} of $\SA$, respectively. Thus in the \kl{positive}
      version of the rule (on the left), the ring would represent \kl{negative} empty
      space on the verso of $\SA$.
      \item[Expansion] The ring is erased by \emph{expanding} the inner
      area where $G$ resides towards the outer border of the ring. Unfolding the
      \kl{metaphor} to its conclusion, the inner area is then ``glued back'' to the
      rest of $\SA$\sidenote{This is reminiscent of the \emph{absorption} rules
      $\{\kl{a},\kl{a{-}},\kl{a{+}}\}$ of \kl{system~B}, as is very clear in
      their graphical presentation (\reffig{graphical-B}).}.
    \end{description}
\end{description}

\reffig{eg-peirce-law} shows a derivation of Peirce's law with the rules of
\kl{Alpha}. Note that the direction of arrows has been reversed compared to the
above presentation: as usual, we prefer to read rules from conclusion to
premiss, starting from the \kl{goal} to prove --- here the \kl{graph} associated to the
formula $((a \limp b) \limp a) \limp a$ --- that we reduce to the empty \kl{goal},
represented by the empty $\SA$. Also, the reader unfamiliar with \kl{EGs} might find
it hard to convince herself that all the steps followed in the derivation are
sound logically. We suggest her to either build a \emph{syntactic} intuition for
the rules by practicing them on various tautologies of propositional logic, or
to wait until we give a formal \emph{semantic} proof of soundness in
\refsec{eg-soundness}.

\begin{marginfigure}
  \input{figures/eg-peirce-law.tex}
  \caption{A derivation of Peirce's law in \kl{Alpha}}
  \labfig{eg-peirce-law}
\end{marginfigure}


\section{Graphs as multisets}\labsec{multisets}

\subsection{Syntax}

As noted by various authors\sidenote{See for instance the Tree Existential
Graphs of Roberts and Pronovost \cite{roberts_existential_1992}, or
\cite[Section~2.2]{brady_categorical_2000}.}, the nesting of \kl{cuts} on $\SA$
induces a \emph{tree} structure on \kl{graphs}: each \kl{cut} constitutes a
node, whose children are either leaves corresponding to atomic propositions
residing in the area of the \kl{cut}, or nodes corresponding to nested
\kl{cuts}. Empty \kl{cuts} have no children, and thus also form leaves of the
tree. Then $\SA$ may be seen either as a forest of atoms and \kl{cuts}, or as a
rooted tree whose root represents $\SA$, and is distinguished from \kl{cut}
nodes. This can be captured by the following grammar:
\begin{mathpar}
  \SA \Coloneq G 
  \and
  G, H, K \Coloneq g_1, \ldots, g_n
  \and
  g, h, k \Coloneq a \mid \acut{G}
\end{mathpar}

\begin{example}
The \kl{graph} of \reffig{peirce-neg-areas} may be written as either one of the
following expressions:
\begin{mathpar}

  \acut{a, \acut{\acut{b}, c}} \and
  \acut{a, \acut{c, \acut{b}}} \and
  \acut{\acut{\acut{b}, c}, a} \and
  \acut{\acut{c, \acut{b}}, a}
\end{mathpar}
\end{example}

To abstract from the specific order in which nodes are sequenced in this
notation, and thus represent faithfully the \kl{isotropy} of $\SA$, we formally
define the graphs of \kl{Alpha} as (recursive) \emph{finite multisets}:

\begin{scope}\knowledgeimport{alpha}

\begin{definition}[Graph]\labdef{alpha-graph} 
  \AP
  Given a denumerable set of atomic propositions $\intro*\atoms$, the sets of
  \intro{nodes} $\intro*\anodes$ and \intro{graphs} $\intro*\agraphs$ are
  defined mutually inductively as follows:
  \begin{description}
    \itemAP[(Spot)] If $a \in \atoms$, then $a \in \anodes$;
    \itemAP[(Area)] If $G \subset \anodes$ is a finite multiset, then $G
    \in \agraphs$;
    \itemAP[(Enclosure)] If $G \in \agraphs$, then $\intro*\acut{G} \in
    \anodes$.
  \end{description}
\end{definition}
The terminology written in parentheses is the one used by Peirce to denote the
same concepts in \sidecite{peirce_prolegomena_1906}. Note the similarity with
the definitions of \kl{bubbles} (enclosures) and \kl(symm){solutions} (areas).
The main difference between \kl{graphs} and \kl(symm){solutions}, is that in the
former formulas (\kl{ions}) are restricted to atoms (spots), and they are not
\emph{\kl(bubble){polarized}} (see \refremark{eg-polarity}).

\subsection{Rules}

The five rules of \kl{Alpha} can now be formalized as multiset \kl{rewriting rules}
on \kl{graphs}. But first, we need a notion of \emph{context} in which rules
apply:

\begin{definition}[Context]
  \AP
  A \intro{context} $G\hole$ is a \kl{graph} which contains exactly one
  occurrence of a special node written $\hole$, called its \reintro{hole}. The
  \kl{hole} can always be \emph{filled} (substituted) with any other \kl{graph}
  $H$ or \kl{context} $K\hole$, producing a new \kl{graph} $\cfill{G}{H}$ or \kl{context}
  $\cfill{G}{K\hole}$. In particular, filling with the empty \kl{graph}
  $\emptyset$ will yield a \kl{graph} $\cfill{G}{\phantom{G}}$, which is just
  $G\hole$ with its \kl{hole} removed.
\end{definition}

Then to reason by induction on \kl{contexts}, we need to define formally how to
measure their \emph{\kl{depth}}. It turns out the only way to increase the \kl{depth} of
an \kl{graph} is to insert \emph{\kl{cuts}}, and thus the \kl{depth} of a
\kl{context} coincides with its number of \emph{inversions}, i.e. the number of
\kl{cuts} enclosing its \kl{hole}:

\begin{definition}[Depth]
  \AP
  The \intro{depth} $\intro*\sdepth{-}$ of \kl{contexts} is defined recursively
  by:
  \begin{align*}
    \sdepth{H, \hole} &= 0 \\
    \sdepth{H, \acut{G\hole}} &= \sdepth{G\hole} + 1
  \end{align*}
\end{definition}

\begin{definition}[Inversions]\labdef{eg-inv}

  The number of \emph{inversions} of a \kl{context} $G\hole$ is defined by
  $\inv(G\hole) = \sdepth{G\hole}$.
\end{definition}

\begin{definition}[Polarity]
  \AP
  \phantomintro{polarity}
  We say that a \kl{context} $G\hole$ is \intro{positive} if $\inv(G\hole)$ is
  even, and \intro{negative} otherwise. We denote \kl{positive} and \kl{negative} \kl{contexts}
  respectively by $G^+\hole$ and $G^-\hole$.
\end{definition}

\begin{figure}
  \input{figures/alpha.tex}
  \caption{Inductive presentation of the rules of \kl{Alpha}}
  \labfig{alpha}
\end{figure}

The inductive version of the rules of \kl{Alpha} is given in \reffig{alpha}, as
a set of unary \kl{inference rules} on \kl{graphs}: when read \emph{top-down},
they correspond to usual inferences from premiss to conclusion, as we first
introduced them in \refsec{illative}.
% This gives them a \emph{static}, \emph{a posteriori} meaning, since this is
% typically how you would check the validity of an established inference
% \emph{after the fact}.
But as already mentioned there, we will rather emphasize their \emph{bottom-up}
reading: then they express the different ways in which one may choose to
simplify a \kl{goal}.

\begin{definition}[Derivation]
  \AP
  We write $G \intro*\step{} H$ to indicate a rewrite \emph{step} in \kl{Alpha},
  that is an instance of some rule from \reffig{alpha} with $H$ as premiss and
  $G$ as conclusion. A \emph{derivation} $G \reintro*\nsteps{n}{} H$ is a
  sequence of rewrite steps $G_0 \step{} G_1 \ldots \step{} G_n$ with $G_0 = G$,
  $G_n = H$ and $n \geq 0$. Generally the length $n$ of the derivation does not
  matter, and we just write $G \reintro*\steps{} H$.
\end{definition}

\begin{definition}[Proof]\labdef{eg-proof}
  A \emph{proof} of a \kl{graph} $G$ is a derivation $G \steps{} \emptyset$.
\end{definition}

\section{Illative atomicity}\labsec{atomicity}

\subsection{Insertions and omissions}

A remarkable feat of Peirce's rules, on which he insisted very much, is that
they are only expressed in terms of \emph{insertions} and \emph{omissions} of
\kl{graphs} on $\SA$. He thought that those were the \emph{smallest} steps in which
reasoning could be dissected, making his system extremely appropriate for
\emph{analytical} purposes. This is summarized in the following excerpt
\sidecite[][p.~533]{peirce_prolegomena_1906}:

\begin{quote}
  In the first place, the most perfectly analytical system of representing
propositions must enable us to separate \kl{illative transformations} into
indecomposable parts. Hence, an \kl{illative transformation} from any proposition, A,
to any other, B, must in such a system consist in first transforming A into AB,
followed by the transformation of AB into B. For an omission and an insertion
appear to be indecomposable transformations and the only indecomposable
transformations.
\end{quote}

We already considered this question of decomposing logical inferences into their
most elementary operations, when reflecting on the graphical presentation of
\kl{BJ} at the end of \refsec{bubbles-graphical-rules}. In this setting, the
most basic insertions and omissions we could find were not logically
\emph{sound}, whereas in \kl{Alpha} they are. This is quite promising, and
prompts us to reevalute our conception of \intro{illative atomicity}, understood
precisely as the definition of what it means for an inference step to be (the
most) \emph{elementary}.

Note that this should be distinguished from the notion of
\emph{\kl{analyticity}}, as popularized by Gentzen with the \emph{\kl{subformula property}} in \kl{sequent calculus}: the latter is concerned with the analysis
of \emph{propositions} into their constituents through \kl{inference rules},
while here we are interested in the analysis of the \kl{inference rules}
themselves\sidenote{We will give a positive answer to the question of
\kl{analyticity} in \kl{Alpha} at the end of \refsec{eg-completeness}.}. However
there is a conceptual bottleneck, because \kl{inference rules} are usually
conceived \emph{by definition} as the smallest constituents of proofs in a given
\kl{proof system}; and it is very hard to formulate objective criteria for
comparing rules in different \kl{proof systems}.

Apart from Peirce, the only other logician we know of who attempted to give a
non-trivial account of \kl{illative atomicity} is J.-Y. Girard. In fact it can
be argued that it is the main motivation behind most of his works starting from
linear logic, which became explicit in ludics with his slogan ``From the rules
of logic to the logic of rules'' \sidecite{girard_locus_2001}.

\subsection{Computational aspects}

\paragraph{Linearity}

There is an intriguing remark by Peirce in
\cite[pp.~536--537]{peirce_prolegomena_1906} about the atomicity of the rules
presented thus far, that seems to have gone unnoticed in the literature on
\kl{EGs}. Indeed, Peirce argues that the principle of \kl{Double{-}cut} ``cannot
be assumed as an undeduced Permission'' --- i.e. a primitive rule of the system
--- because when the double-\kl{cut} is removed, the area inside the inner
\kl{cut} becomes identified with the area outside the outer \kl{cut}, a
transformation that ``is not strictly an Insertion or a Deletion''.

Another way to interpret this, is that \kl{Double{-}cut} is the only
\emph{linear} rule of the system, in the sense that the premiss and conclusion
contain exactly the same atomic \kl{graphs}. Contrast this with linear logic,
which instead takes linearity as the criterion for \kl{illative atomicity}, as
exemplified by the linear decomposition of implication $A \limp B \defeq \oc A
\multimap B$. This might be the consequence of an opposite treatment given to
\emph{negation}: while in \kl{EGs} it is the only primitive constructor of the
system --- remember that the only way to increase the \kl{depth} of a \kl{graph}
is with a \kl{cut}, in \sys{LL} negation is the only defined notion through De
Morgan dualities. Thus \kl{EGs} are closer (at least syntactically) to
\emph{\kl{type theories}}, which also take a negative operation (the arrow or
dependent product \kl{type}) as their sole primitive construct.

\paragraph{Interactivity and Locativity}

Peirce then suggests a ``more scientific way'', where the principle of
\kl{Double{-}cut} is subsumed by a restricted variant of the principles of
\kl{Iteration} and \kl{Deiteration}. His description of this ``more scientific''
\kl[Iteration]{(De)iteration} principle is based on a relation of \emph{local
justification} (our terminology) between two areas of a \kl{graph}, that
captures the fact that the deeper occurrence of $G$ in the \kl{Iter} and
\kl{Deit} rules (\reffig{alpha}) is justified by the other occurrence by virtue
of their respective \emph{locations}. Later in the text, Peirce emphasizes the
importance of this locative aspect of argumentation
\cite[pp.~544--545]{peirce_prolegomena_1906}:

\begin{quote}%
[...] when an Argument is brought before us, there is brought to our notice a%
process whereby the Premisses bring forth the Conclusion, not informing the%
Interpreter of its Truth, but appealing to him to assent thereto. This Process%
of Transformation, which is evidently the kernel of the matter, is no more built%
out of Propositions than a motion is built out of position.%
\end{quote}

Once again, game-theoretical ideas and the concept of space
(\refremark{eg-polarity}) are prominent in this excerpt: Truth is not primitive,
but rather a side effect of the interaction between an Interpreter (Opponent)
being lead to agree with the Graphist (Player), whenever the latter performs a
transformation on the \kl{graph} under discussion. The soundness of such a
transformation guarantees that this will work for \emph{any}
Interpreter/Opponent, leading to what is known as a \emph{winning strategy} in
game semantics. Since \kl{illative transformations} only consist of insertions
and omissions, whose validity depends solely on the positions where they occur
in the \kl{graph}, it ensues that the components of an argumentation can be
reduced to ``motions'' (moves) that relate pure locations.

It is then interesting to notice that the quest for \kl{illative atomicity}, who led
Peirce to discover these interactive and locative aspects of logic, also led
Girard to identify these properties as fundamental, in his recent works on
ludics \cite{girard_locus_2001} and transcendental syntax
\cite{eng_exegesis_2023}. We tend to share the vision put forth by Boris Eng in
his thesis \cite[\S 24.4]{eng_exegesis_2023}, that logic is mostly about
\emph{space} and the \emph{shape} of objects, while \emph{time} and
\emph{dynamics} pertain more to the realm of computation. In this view, Peirce's
systems of \kl{EGs} are a logico-computational complex where each aspect can clearly
be identified: the Process of Illative Transformation is an interactive
computation among the Graphist and Interpreter, whose logical nature is
determined by the spatial constraints of the Permissions, that are expressible
thanks to the topology induced by \kl{cuts} on $\SA$\sidenote{Both Peirce and Girard
also shared the ambition to develop a comprehensive philosophical foundation for
logic, as part of a more general theory of \emph{meaning}: for Peirce it was his
\emph{semeiotic}, stemming from his overarching doctrine of \emph{pragmaticism}
\cite{sep-peirce}; for Girard it was the theory of programming languages and
their semantics.}.

\subsection{The more scientific way}\labsubsec{scientific-way}

Let us now go back to the ``more scientific'' \kl[Iteration]{(De)iteration} principle
proposed by Peirce. With our formalization of \kl{graphs} as multisets, we can
give a more rigorous formulation than the original natural language description
given by Peirce\sidenote{Although we limit ourselves to propositional logic in
\kl{Alpha}, while in \cite{peirce_prolegomena_1906} Peirce also accounts for the
\kl{lines of identity} of \kl{Beta} handling \kl{predicate logic}, to be
introduced in \refsec{beta}.}. In this setting, a location in a \kl{graph} is
represented by the \kl{hole} of a \kl{context}, thus the relation of local
justification between two areas is defined on \kl{contexts}:

\begin{marginfigure}
  \input{figures/eg-local-justification.tex}
  \caption{Graphical representation of the four conditions of local
  justification}
  \labfig{eg-local-justification}
\end{marginfigure}

\begin{definition}[Local justification]\labdef{eg-local-justification}

  Given two \kl{contexts} $G\hole$ and $H\hole$, we say that $H\hole$ is
  \emph{locally justified} by $G\hole$, written $G\hole \ljustif H\hole$, if and
  only if one of the following conditions holds for some $K\hole$, $G_0\hole$,
  $H_0\hole$ such that $G\hole = \cfill{K}{G_0\hole}$ and $H\hole =
  \cfill{K}{H_0\hole}$:
  \begin{enumerate}
    \item $G_0\hole = H_0\hole = K_0, \hole$ for some $K_0$;
    \item $G_0\hole = K_0, \acut{K_1}, \hole$ and $H_0\hole = K_0, \acut{K_1, \hole}$ for
    some $K_0, K_1$;
    \item $G_0\hole = K_0, \acut{K_1, \acut{K_2}}, \hole$ and $H_0\hole = K_0, [K_1, [K_2,
    \hole]]$ for some $K_0, K_1, K_2$;
    \item $G_0\hole = K_0, \acut{\acut{K_1, \hole}}$ and $H_0\hole = K_0, \hole, \acut{\acut{K_1}}$
    for some $K_0, K_1$.
  \end{enumerate}
\end{definition}

\begin{figure}
  \input{figures/alpha-a.tex}
  \caption{The illatively atomic system \kl{Alphaa}}
  \labfig{eg-atomic-alpha}
\end{figure}

\AP
These four conditions are exactly the formal counterpart of those given by
Peirce in \cite{peirce_prolegomena_1906}. They might be more easily understood
by looking at their graphical representation in \reffig{eg-local-justification}:
the red and blue dots denote respectively the locations of the \kl{holes} in the
justified \kl{context} $H\hole$ and justifying \kl{context} $G\hole$, as
suggested by the arrow between them. In particular, it becomes clear that it is
Condition 4 that will account for the principle of \kl{Double{-}cut}.

Then the new rules of iteration \kl{Iter{+}}, \kl{Iter{-}} and deiteration
\kl{Deit{+}}, \kl{Deit{-}} are given in a so-called \emph{atomic} variant of
\kl{Alpha}, that we name \intro{Alphaa} in \reffig{eg-atomic-alpha}. As
promised, \kl{Alphaa} only comprises rules that truly are insertions and
omissions of \emph{arbitrary} \kl{graphs}\sidenote{The terminology ``atomic''
might be a bit confusing: here we think of \emph{illative} atomicity in Peirce's
sense, not the fact that the \kl{graphs} manipulated by rules are atomic, which
might be termed \emph{structural} atomicity. There seems to be a symmetric
tradeoff when comparing \kl{EGs} to the \kl{calculus of structures}: in the
former, one maximizes \kl{illative atomicity} by minimizing linearity and
structural atomicity; while in the latter, one maximizes structural atomicity
and linearity by minimizing \kl{illative atomicity}. This will become explicit
in \refsec{eg-completeness}, when simulating the \kl{calculus of structures}
\kl{SKS} in \kl{Alpha}.}. The atomic (de)iteration rules are a restriction of
the original ones in two respects:

\begin{description}
  \item[Locality] Following \refdef{eg-local-justification}, the \kl{depth} of the
  justified \kl{context} $H_T\hole$ can be at most $2$ in the atomic rules, while it
  is unbounded in the original rules. This does not hinder their expressivity
  however: global (de)iterations can be simulated by successive applications of
  local ones, by erasing intermediate copies with the \kl{Ins} and \kl{Del}
  rules. This is because the \emph{global justification} relation $\gjustif$
  associated with the original rules coincides with the transitive closure of
  the local relation $\ljustif$, modulo the 4\textsuperscript{th} condition for
  double-\kl{cuts}\sidenote{Our notation for justification relations actually comes
  from \cite{minghui_graphical_2019}, where the authors define the same notion
  informally for an \kl{intuitionistic} variant of \kl{EGs}.}.

  \item[Polarity] In the atomic iteration (resp. deiteration) rules, the
  justified \kl{context} must be \kl{positive} (resp. \kl{negative}), while it
  can have an arbitrary \kl{polarity} in the original rules. This is expressed
  by splitting each of the latter into two rules, one where the outer
  \kl{context} $K\hole$ is \kl{positive} ($\kl{Iter{+}},\kl{Deit{+}}$), and one
  where it is \kl{negative} ($\kl{Iter{-}},\kl{Deit{-}}$). Again, this does not
  alter their deductive power: every iteration (resp. deiteration) in a
  \kl{negative} (resp. \kl{positive}) \kl{context} can be trivially performed by
  an instance of the \kl{Ins} (resp. \kl{Del}) rule. Thus atomic rules eliminate
  a redundancy of \kl{Alpha}, where many insertions/omissions could be
  interpreted as instances of either \kl{Iter}/\kl{Deit} or \kl{Ins}/\kl{Del}.
  They also eliminate the possibility for a conclusion to justify a hypothesis,
  as remarked in \refsec{illative} when commenting on the principle of
  \kl{Iteration}, making them closer to the rules of \kl{sequent
  calculus}\sidenote{We suspect however that the more general (de)iteration
  rules are still relevant from a computational point of view.}.
\end{description}

In the rest of this chapter, we settle on the more standard system \kl{Alpha},
and leave a more detailed and rigorous study of \kl{Alphaa} for future work.
But the above informal arguments should convince the reader that there is little
doubt that \kl{Alphaa} is both sound and complete if and only if \kl{Alpha}
is, which is the object of the following two sections.

\section{Soundness}\labsec{eg-soundness}

\AP
We are now going to prove formally the \emph{soundness} of each rule of
\kl{Alpha}, by showing that if $G \step{} H$ and $H$ is \emph{true}, then so is
$G$. In \kl{classical} propositional logic, one can easily evaluate the truth of
any formula $A$, given a \intro[valuation]{truth-valuation} $v : \atoms \to
\{0,1\}$ for the atoms of $A$. The same applies to \kl{graphs}:

\begin{definition}[Evaluation]

  Given a \kl{valuation} $v : \atoms \to \{0,1\}$ and a \kl{graph} $G$, the
  \emph{evaluation} $v(G)$ of G is defined by mutual recursion as follows:
  \begin{align*}
    v(G) &= \begin{cases}
      1 &\text{if $G = \emptyset$} \\
      \min_{g \in G}{v(g)} &\text{otherwise}
    \end{cases} \\
    v(\acut{G}) &= 1 - v(G)
  \end{align*}
\end{definition}

This follows the standard way to evaluate conjunctions and negations.

To factorize the proof of soundness, we first prove a few lemmas for the
\emph{\kl{invertible}} rules of \kl{Alpha}, that is those who satisfy $v(G) = v(H)$
for every \kl{valuation} $v$ if $G \step{} H$.

\begin{lemma}[Iteration]\lablemma{eg-iteration}
  For every \kl{graph} $G$, \kl{context} $H\hole$ and \kl{valuation} $v$, we have
  $$v\left(G, H\select{\phantom{G}}\right) = v\left(G, H\select{G}\right)$$
\end{lemma}
\begin{proof}
  By induction on $\sdepth{H\hole}$.

  \def\arraystretch{1.5}
  \begin{itemize}
    \item[\bcase]
      Suppose $H\hole = H', \hole$. Then we have
      $$
      \begin{array}{rcll}
        v(G, H')
        &=& \min_{g \in G \cup H'}{v(g)} & \\
        &=& \min_{g \in G \cup H' \cup G}{v(g)} & \\
        &=& v(G, H', G) & \\
      \end{array}
      $$
    \item[\rcase]
      Suppose $H = H', \acut{H_0\hole}$. Then we have
      $$
      \begin{array}{rcll}
        v\left(G, H', \acut{H_0\select{\phantom{G}}}\right)
        &=& \min\left(v(H'), v\left(G, \acut{H_0\select{\phantom{G}}}\right)\right) & \\
        &=& \min\left(v(H'), v\left(G, \acut{H_0\select{G}}\right)\right) &\text{(IH)} \\
        &=& v\left(G, H', \acut{H_0\select{G}}\right) & \\
      \end{array}
      $$
  \end{itemize}
\end{proof}

\begin{lemma}[Double-cut]\lablemma{eg-doublecut}
  For every \kl{graph} $G$ and \kl{valuation} $v$, we have
  $$v(\acut{\acut{G}}) = v(G)$$
\end{lemma}
\begin{proof}
  $$
  v(\acut{\acut{G}}) = 1 - (1 - v(G)) = \begin{cases}
    1 - (1 - 0) = 0 &\text{if $v(G) = 0$} \\
    1 - (1 - 1) = 1 &\text{if $v(G) = 1$}
  \end{cases}
  $$
  In both cases we have $v(\acut{\acut{G}}) = v(G)$.
\end{proof}

Since all rules apply in an arbitrary deep \kl{context} $K\hole$, we will benefit
from the following \emph{functoriality} lemmas:

% \begin{lemma}\lablemma{minleq}
%   For every $a, b, c \in \{0,1\}$, $a \leq b$ implies $\min(a, c) \leq \min(b,
%   c)$.
% \end{lemma}
% \begin{proof}
%   If $a \leq c$ and $b \leq c$, then $\min(a, c) = a \leq b = \min(b, c)$.
%   Otherwise either $a > c$ or $b > c$. In both cases it follows that $c = 0$,
%   and thus $\min(a, c) = \min(b, c) = 0$.
% \end{proof}

\begin{lemma}[Variance]\lablemma{eg-variance}
  
  For every \kl{context} $K\hole$, \kl{graphs} $G, H$ and \kl{valuation} $v$ such that $v(G)
  \leq v(H)$, we have:
  \begin{enumerate}
    \item $v\left(K\select{G}\right) \leq v\left(K\select{H}\right)$ if $K\hole$
    is \kl{positive};
    \item $v\left(K\select{H}\right) \leq v\left(K\select{G}\right)$ if $K\hole$
    is \kl{negative}.
  \end{enumerate}
\end{lemma}
\begin{proof}
  By induction on $\sdepth{K\hole}$.

  \def\arraystretch{1.5}
  \begin{itemize}
    \item[\bcase~($\sdepth{K\hole} = 0$)]\sbr
      \begin{enumerate}
        \item Suppose $K\hole = K', \hole$. Then we have
        $$
        \begin{array}{rcll}
          v(K', G)
          &=& \min(v(K'), v(G)) & \\
          &\leq& \min(v(K'), v(H)) &\text{(Hypothesis)} \\
          &=& v(K', H) &
        \end{array}
        $$

        \item We have $\sdepth{K\hole} > 0$ since $K\hole$ is \kl{negative}.
        Contradiction.
      \end{enumerate}
    \item[\rcase~($\sdepth{K\hole} > 0$)]\sbr
      \begin{enumerate}
        \item Suppose $K\hole = K', \acut{K_0^-\hole}$. Then by IH we have
        $v\left(K_0^-\select{H}\right) \leq v\left(K_0^-\select{G}\right)$, and thus
        $1 - v\left(K_0^-\select{G}\right) \leq 1 - v\left(K_0^-\select{H}\right)$. Therefore
        $$
        \begin{array}{rcll}
          v\left(K', \acut{K_0^-\select{G}}\right)
          &=& \min\left(v\left(K'\right), 1 - v\left(K_0^-\select{G}\right)\right) & \\
          &\leq& \min\left(v\left(K'\right), 1 - v\left(K_0^-\select{H}\right)\right) & \\
          &=& v\left(K', \acut{K_0^-\select{H}}\right) &
        \end{array}
        $$

        \item Suppose $K\hole = K', \acut{K_0^+\hole}$. Then by IH we have
        $v\left(K_0^+\select{G}\right) \leq v\left(K_0^+\select{H}\right)$, and thus
        $1 - v\left(K_0^+\select{H}\right) \leq 1 - v\left(K_0^+\select{G}\right)$. Therefore
        $$
        \begin{array}{rcll}
          v\left(K', \acut{K_0^+\select{H}}\right)
          &=& \min\left(v\left(K'\right), 1 - v\left(K_0^+\select{H}\right)\right) & \\
          &\leq& \min\left(v\left(K'\right), 1 - v\left(K_0^+\select{G}\right)\right) & \\
          &=& v\left(K', \acut{K_0^+\select{G}}\right) &
        \end{array}
        $$
      \end{enumerate}
  \end{itemize}
\end{proof}

\begin{corollary}[Functoriality]\labcor{eg-functoriality}
  If $v(G) = v(H)$ then $v\left(K\select{G}\right) = v\left(K\select{H}\right)$.
\end{corollary}

\begin{theorem}[Soundness]
  If $G \step{} H$, then $v(H) \leq v(G)$ for every \kl{valuation} $v$.
\end{theorem}
\begin{proof}
  By inspection of each rule.
  \begin{itemize}
    \item[\kl{Iter}, \kl{Deit}] We have $v\left(K\select{G,
    H\select{\phantom{G}}}\right) = v\left(K\select{G, H\select{G}}\right)$ by
    \reflemma{eg-iteration} and functoriality.

    \item[\kl{Dcut{\da}}, \kl{Dcut{\ua}}] We have
    $v\left(K\select{G}\right) = v\left(K\select{\acut{\acut{G}}}\right)$ by
    \reflemma{eg-doublecut} and functoriality.

    \item[\kl{Ins}, \kl{Del}] This follows from the fact that $v(G) \leq 1 =
    v(\emptyset)$ and \reflemma{eg-variance}.
  \end{itemize}
\end{proof}

\section{Completeness}\labsec{eg-completeness}

\AP
To show the completeness of \kl{Alpha}, it is standard in the literature on
\kl{EGs} to simulate an existing \kl{proof system} for \kl{classical}
propositional logic, that is itself known to be complete. For instance in
\cite{Roberts+1973}, completeness is shown by simulating the Hilbert-style
system \sys{P} of Church, which only comprises 3 \kl{axioms} for the
(functionally complete) fragment $\{\limp, \neg\}$. We propose in this section a
proof by simulation of a \emph{\kl{deep inference}} system, more specifically
the \kl{calculus of structures} \intro{SKS} first introduced in
\sidecite{brunnler_local_2001}. This should provide a good overview of the
similarities and differences between the two systems, and in particular of how
they exemplify two distinct approaches to \emph{symmetry} in a \kl{deep
inference} setting.

\begin{kaonote}
  This section was written without being aware of the work of Ma and Pietarinen
  in \sidecite{ma_proof_2017}, where they give a simulation of the calculus of
  structures \intro{SKSg} in \kl{Alpha}, and also conversely a simulation of
  \kl{Alpha} in \kl{SKSg}. While they do a similar comparison of features
  between the two systems, in particular concerning their treatment of symmetry
  and \kl{polarity}, our work differs mainly in two respects:
  \begin{description}
    \item[Locality] \kl{SKS} is the \emph{local} version of \kl{SKSg}, thus we
    briefly comment on locality in \kl{SKS} and \kl{Alpha} and what our
    simulation says about it;
    \item[Analyticity] crucially, our objective is to show at the end of this
    section that \kl{Alpha} is \emph{\kl{analytic}}. Ma and Pietarinen discuss
    this question very quickly in their paper, by affirming that \kl{Alpha} is
    analytic both in the sense of Gentzen because it can simulate the \kl{cut}
    rule, and in the sense of \emph{\kl{illative atomicity}} discussed in
    \refsec{atomicity}. We disagree with the first claim, and use our simulation
    to show \kl{analyticity} in the proper sense of satisfying a form of
    \kl{\kl{subformula property}}.
  \end{description}
\end{kaonote}

\subsection{Calculus of structures}

As the name indicates, the objects manipulated by \kl{inference rules} in \kl{calculi of
structures} are so-called \emph{\kl{structures}}. In the case of \kl{SKS}, they
correspond to formulas in \emph{negation normal form} built from atoms and units
$\{\top, \bot\}$, i.e. where connectives are restricted to the fragment
$\{\land, \lor\}$, and negation is pushed to atoms by relying on De Morgan's
laws.

\begin{definition}[Structure]
  The \intro{structures} of \kl{SKS} are generated by the following grammar:
  $$S, T, U, V, W \Coloneq a \mid \dmdual{a} \mid \top \mid \bot \mid S \land T
  \mid S \lor T$$
\end{definition}

\begin{definition}[De Morgan dual]
  The \emph{De Morgan dual} of a \kl{structure} $S$ is defined recursively as
  follows:
  \begin{align*}
    \dmdual{a} &= \dmdual{a} & \dmdual{\dmdual{a}} &= a \\
    \dmdual{\top} &= \bot & \dmdual{\bot} &= \top \\
    \dmdual{S \land T} &= \dmdual{S} \lor \dmdual{T} & \dmdual{S \lor T} &= \dmdual{S} \land \dmdual{T}
  \end{align*}
\end{definition}

It is customary in \kl{CoS} to further quotient the set of \kl{structures} with additional
equations between them, which account for various algebraic properties of
connectives such as associativity, commutativity and unitality. Here we will
rely instead on the formulation of \kl{SKS} given by Tubella and Straßburger in
\cite{tubella:hal-02390267}, where all equations are incorporated in the system
as explicit rules.

\begin{figure*}
  \input{figures/sks.tex}
  \caption{\kl{Inference rules} of \kl{SKS}}
  \labfig{sks}
\end{figure*}

The full set of rules of \kl{SKS} is given in \reffig{sks}. All rules are
implicitly applicable in any \emph{\kl(sks){context}} $W\hole$ of arbitrary \kl{depth}, with
the usual notion of \intro(sks){context} as a \kl{structure} with a \kl{hole}.

\begin{definition}[Depth]
  The \reintro{depth} $\sdepth{S\hole}$ of a \kl(sks){context} $S\hole$ is
  defined recursively as follows:
  \begin{align*}
    \sdepth{\hole} &= 0 \\
    \sdepth{S\hole \land T} = \sdepth{T \land S\hole} = \sdepth{S\hole \lor T} = \sdepth{T \lor S\hole} &= \sdepth{S\hole} + 1
  \end{align*}
\end{definition}

\begin{remark}\labremark{sks-ctx-pol}
  \kl(sks){Contexts} for \kl{structures} are always \emph{\kl{positive}}, since negation is pushed
down to atoms. This is the opposite situation from that of \kl{Alpha}, where
negation is the \emph{only} construct that can increase the \kl{depth} of a \kl{graph}.
This explains why some rules in \kl{Alpha} need explicit indications for the
\kl{polarity} of the \kl{context} in which they apply.
\end{remark}

The rules of \kl{SKS} satisfy two notable properties:
\begin{description}
  \item[Symmetry] 
    Every rule $\mathsf{r}$ has a \emph{dual} rule $\dmdual{\mathsf{r}}$, that
    is $U\select{S} \xstep{\mathsf{r}} U\select{T}$ if and only if
    $U\select{\dmdual{T}} \xstep{\dmdual{\mathsf{r}}} U\select{\dmdual{S}}$. For
    rules whose name ends with $\da$, the dual is the rule with the same name
    ending with $\ua$. This corresponds to all rules except the \emph{switch}
    rule \kl{s} and the \emph{medial} rule \kl{m} which are self-dual, i.e.
    $\dmdual{\kl{s}} = \kl{s}$ and $\dmdual{\kl{m}} = \kl{m}$. In \kl{Alpha},
    duality is captured in the \emph{\kl{polarity}} of \kl{contexts} rather than
    through De Morgan's laws:
    
    \begin{fact}[Duality]\labfact{eg-duality}
      $K^+\select{G} \xstep{\mathsf{r}} K^+\select{H}$
      if and only if $K^-\select{H} \xstep{\mathsf{\dmdual{r}}} K^-\select{G}$, where
      \begin{align*}
        \dmdual{\kl{Iter}} &= \kl{Deit} & \dmdual{\kl{Deit}} &= \kl{Iter} \\
        \dmdual{\kl{Ins}} &= \kl{Del} & \dmdual{\kl{Del}} &= \kl{Ins} \\
        \dmdual{\kl{Dcut{\da}}} &= \kl{Dcut{\ua}} & \dmdual{\kl{Dcut{\ua}}} &= \kl{Dcut{\da}}
      \end{align*}
    \end{fact}

    \begin{remark}\labremark{eg-intuitionism}
      
    Contrary to De Morgan duality, the notion of \kl{polarity} of a \kl{context}
    also exists in \kl{intuitionistic} logic, and is in fact used in the same
    way to obtain dual rules in the \kl{intuitionistic} \kl{calculus of
    structures} \sys{SISa} of Tiu \cite{tiu_local_2006}. This constitutes one
    more argument in favor of the view defended by Ma and Pietarinen in
    \sidecite{minghui_graphical_2019}, that Peirce had a
    \emph{pre-intuitionistic} conception of negation. It also echoes our
    observation in \refremark{eg-entitative}, that the choice of negation and
    conjunction as primitives is to be connected with the eminently constructive
    works of Girard and Melliès, in particular the non-involutive tensorial
    negation of the latter. The issue of finding seeds of intuitionism in
    Peirce's work will be discussed more in depth in \refsec{Flowers}.
    \end{remark}

  \item[Locality]
    Every rule is \emph{local}, in the sense that it does not require the
    inspection of expressions of arbitrary size (Definition 2.1.1 in
    \cite{tubella:hal-02390267}). This is almost the opposite in \kl{Alpha}: to
    the exception of the rules \kl{Dcut{\da}} and \kl{Dcut{\ua}} (which are best
    seen as a structural equivalence like those of \kl{CoS}), all rules depend
    heavily on the creation, duplication or deletion of \kl{subgraphs} of
    arbitrary size. This is exemplified by the derivation of Peirce's law in
    \reffig{eg-peirce-law}, where not a single rule is instantiated on atoms. In
    fact it is quite hard to see how to build a derivation of Peirce's law that
    performs only local transformations.
    
    In light of this, it becomes surprising that we \emph{will} be able to
    simulate \kl{SKS} in \kl{Alpha}. Indeed, it means that there is a set
    \sys{Alpha_{SKS}} of perfectly local rules on \kl{graphs}, corresponding to the
    translation of the rules of \kl{SKS}, and which is entirely derivable in
    \kl{Alpha}. Thus by restricting oneself to the rules of \sys{Alpha_{SKS}}
    (and forgetting that they are derived with non-local rules), one gets a
    fully local subsystem of \kl{Alpha}!
\end{description}

\subsection{Simulation}

To formulate the simulation, we need to translate the \kl{structures} of \kl{SKS}
into equivalent \kl{graphs}. This is easily done by exploiting the
functional completeness of $\{\land, \neg\}$ (see \refsec{alpha}):

\begin{definition}[Structure translation]
  The \emph{translation} $\intro*\strans{S}$ of a \kl{structure} $S$ as a \kl{graph} is
  defined recursively as follows:
  \begin{align*}
    \strans{a} &= a & \strans{\dmdual{a}} &= \acut{a} \\
    \strans{\top} &= \emptyset & \strans{\bot} &= \acut{} \\
    \strans{(S \land T)} &= \strans{S}, \strans{T} & \strans{(S \lor T)} &= \acut{\acut{\strans{S}}, \acut{\strans{T}}}
  \end{align*}
\end{definition}

As per \refremark{sks-ctx-pol}, the translation of a \kl{structure} \kl(sks){context} (where
the \kl{hole} is translated as itself) will always be \kl{positive}:

\begin{fact}\labfact{strans-pos}
  For every \kl(sks){context} $S\hole$, $\strans{S\hole}$ is \kl{positive}.
\end{fact}
\begin{proof}
  By a straightforward induction on $\sdepth{S\hole}$.
\end{proof}

It is easy to show that a \kl{structure} and its translation as a \kl{graph} are
semantically equivalent, i.e. $v(S) = v(\strans{S})$ for any \kl{valuation} $v$. Thus
to get the completeness of \kl{Alpha}, it is sufficient to simulate the
translation of each rule of \kl{SKS}. But first, we need to ensure that
\kl{Alpha} satisfies a property of \emph{contextual closure}: this will allow
us to ignore the implicit \kl(sks){context} $W\hole$ in the rules of \reffig{sks}.

\begin{lemma}[Positive closure]\lablemma{eg-posclos}
  If $G \step{} H$, then $K^+\select{G} \step{} K^+\select{H}$.
\end{lemma}
\begin{proof}
  Since all rules of \kl{Alpha} apply in a \kl{context} of unbounded \kl{depth}, we know
  that there are some \kl{graphs} $G_0, H_0$ and \kl{context} $K'\hole$ such that $G =
  K'\select{G_0}$ and $H = K'\select{H_0}$. Then either $K'\hole$ is \kl{positive},
  and $\inv(K^+\select{K'\hole}) = \inv(K^+\hole) + \inv(K'\hole)$ is even since
  it is the sum of two even numbers; or $K'\hole$ is \kl{negative}, and
  $\inv(K^+\select{K'\hole})$ is odd since it is the sum of an even and an odd
  number. In both cases $K^+\select{K'\hole}$ has the same \kl{polarity} as
  $K'\hole$, and thus the same rule can be applied.
\end{proof}

\begin{theorem}[Completeness]
  If $U\select{S} \step{} U\select{T}$, then $\strans{U}\select{\strans{S}} \steps{}
  \strans{U}\select{\strans{T}}$.
\end{theorem}
\begin{proof}
  We show that $\strans{S} \steps{} \strans{T}$ by simulating each rule of
  \reffig{sks}. The closure with $\strans{U\hole}$ follows from
  \reffact{strans-pos} and \reflemma{eg-posclos}.
  
  To make the notation lighter, we implicitly apply the translation
  $\strans{(-)}$ on substructures. We also add some coloring to put clearly in
  evidence the \kl{subgraphs} manipulated by rules. Assuming that the rules are read
  from bottom to top:
  \begin{description}
    \item[(De)iteration] In the rules \kl{Iter} and \kl{Deit}, the justifying
    occurrence is squared in \itsrc{\text{blue}}. In the \kl{Iter} (resp.
    \kl{Deit}) rule, the erased (resp. space for the inserted) copy is
    highlighted in \itdst{\text{blue}}.
    \item[Insertion/Deletion] In the rule \kl{Ins} (resp. \kl{Del}), the erased
    (resp. space for the inserted) \kl{subgraph} is highlighted in \ins{\text{red}}.
    \item[Double-cut] In the rule \kl{Dcut{\da}} (resp.
    \kl{Dcut{\ua}}), the space around which the double-\kl{cut} is erased
    (resp. inserted) is highlighted in \dcut{\text{gray}}.
  \end{description}
  \newcommand{\vsp}{\vspace{2em}}

  We start with the identity rules $\{\kl{i{\da}},\kl{i{\ua}}\}$:
  $$
  \begin{array}{r@{\mt}l@{\qquad\qquad}r@{\mt}l@{\vsp}}
    \R[\kl{ai{\da}}]
      {\top}
      {a \lor \dmdual{a}}
    &
    \R[\kl{Dcut{\da}}]
    {\R[\kl{Iter}]
    {\R[\kl{Ins}]
    {\R[\kl{Dcut{\da}}]
    {}
    {\acut{\acut{\dcut{\phantom{\!!\!}}}}}}
    {\acut{\acut{}, \ins{a}}}}
    {\acut{\acut{\itdst{a}}, \itsrc{a}}}}
    {\acut{\acut{a}, \acut{\acut{\dcut{a}}}}}
    &
    \R[\kl{ai{\ua}}]
      {a \land \dmdual{a}}
      {\bot}
    &
    \R[\kl{Del}]
    {\R[\kl{Deit}]
    {a, \acut{a}}
    {\itsrc{a}, \acut{\itdst{\phantom{a}}}}}
    {\ins{\phantom{a}}\acut{}}
    \\
  \end{array}
  $$

  Then onto \kl{weakening} $\{\kl{aw{\da}},\kl{aw{\ua}}\}$ and
  \kl{contraction}
  $\{\kl{ac{\da}},\kl{ac{\ua}},\kl{nm{\da}},\kl{nm{\ua}}\}$:
  $$
  \begin{array}{r@{\mt}l@{\qquad\qquad}r@{\mt}l@{\vsp}}
    \R[\kl{aw{\da}}]
      {\bot}
      {a}
    &
    \R[\kl{Dcut{\ua}}]
    {\R[\kl{Ins}]
    {\acut{}}
    {\acut{\ins{\acut{a}}}}}
    {\dcut{a}}
    &
    \R[\kl{aw{\ua}}]
      {a}
      {\top}
    &
    \R[\kl{Del}]
    {a}
    {\ins{\phantom{a}}}
    \\
    \R[\kl{ac{\da}}]
      {a \lor a}
      {a}
    &
    \R[\kl{Dcut{\ua}}]
    {\R[\kl{Deit}]
    {\acut{\acut{a},\acut{a}}}
    {\acut{\itsrc{\acut{a}}\itdst{\phantom{\acut{a}}}}}}
    {\dcut{a}}
    &
    \R[\kl{ac{\ua}}]
      {a}
      {a \land a}
    &
    \R[\kl{Iter}]
    {a}
    {\itsrc{a}, \itdst{a}}
    \\
    \R[\kl{nm{\da}}]
      {\bot}
      {\bot \land \bot}
    &
    \R[\kl{Iter}]
    {\acut{}}
    {\itsrc{\acut{}},\itdst{\acut{}}}
    &
    \R[\kl{nm{\ua}}]
      {\top \lor \top}
      {\top}
    &
    \R[\kl{Dcut{\ua}}]
    {\R[\kl{Deit}]
    {\acut{\acut{},\acut{}}}
    {\acut{\itsrc{\acut{}}\itdst{\phantom{\acut{}}}}}}
    {\dcut{\phantom{\!!\!}}}
    \\
  \end{array}
  $$
  
  For the \kl{switch rule} \kl{s}, we give two dual derivations: the first uses the
  rules \kl{Deit} and \kl{Ins} to move $U$ \emph{into} the \kl{cuts} enclosing $T$,
  while the second uses the rules \kl{Del} and \kl{Iter} to move $S$ \emph{out
  of} the \kl{cuts} of $T$.

  \begin{mathpar}
    \R[\kl{Dcut{\ua}}]
    {\R[\kl{Deit}]
    {\R[\kl{Ins}]
    {\R[\kl{Dcut{\da}}]
    {S, \acut{\acut{T}, \acut{U}}}
    {\acut{\acut{\dcut{S, \acut{\acut{T}, \acut{U}}}}}}}
    {\acut{\acut{S, \acut{\acut{T}, \acut{U}}}, \ins{\acut{U}}}}}
    {\acut{\acut{S, \acut{\acut{T}\itdst{\phantom{\acut{U}}}}}, \itsrc{\acut{U}}}}}
    {\acut{\acut{S, \dcut{T}}, \acut{U}}}
    \and
    \R[\kl{Del}]
    {\R[\kl{Iter}]
    {S, \acut{\acut{T}, \acut{U}}}
    {\itsrc{S}, \acut{\acut{\itdst{S}, T}, \acut{U}}}}
    {\ins{\phantom{S}}\acut{\acut{S, T}, \acut{U}}}
  \end{mathpar}

  Similarly for the medial rule \kl{m}, which is the other self-dual rule of
  \kl{SKS}, we have two dual derivations:

  \begin{equation*}
    \R[\kl{Deit}]
    {\R[\kl{Deit}]
    {\R[\kl{Ins}]
    {\R[\kl{Ins}]
    {\R[\kl{Ins}]
    {\R[\kl{Dcut{\da}}]
    {\R[\kl{Dcut{\da}}]
    {\acut{\acut{S,T},\acut{U,V}}}
    {\acut{\acut{S,T},\acut{U,\acut{\acut{\dcut{V}}}}}}}
    {\acut{\acut{S,\acut{\acut{\dcut{T}}}},\acut{U,\acut{\acut{V}}}}}}
    {\acut{\acut{S,\acut{\acut{T}}},\acut{U,\acut{\ins{\acut{T}},\acut{V}}}}}}
    {\acut{\acut{S,\acut{\acut{T},\ins{\acut{V}}}},\acut{U,\acut{\acut{T},\acut{V}}}}}}
    {\acut{\acut{S,\acut{\acut{T},\acut{V}}},\acut{U,\acut{\acut{T},\acut{V}}}},\ins{\acut{\acut{T},\acut{V}}}}}
    {\acut{\acut{S,\acut{\acut{T},\acut{V}}},\acut{U\itdst{\phantom{\acut{\acut{T},\acut{V}}}}}},\itsrc{\acut{\acut{T},\acut{V}}}}}
    {\acut{\acut{S\itdst{\phantom{\acut{\acut{T},\acut{V}}}}},\acut{U}},\itsrc{\acut{\acut{T},\acut{V}}}}
    \quad\quad
    \R[\kl{Del}]
    {\R[\kl{Del}]
    {\R[\kl{Del}]
    {\R[\kl{Del}]
    {\R[\kl{Iter}]
    {\acut{\acut{S,T},\acut{U,V}}}
    {\itsrc{\acut{\acut{S,T},\acut{U,V}}},\itdst{\acut{\acut{S,T},\acut{U,V}}}}}
    {\acut{\acut{S,T},\acut{U,V}},\acut{\acut{S,T},\acut{\ins{\phantom{U}}V}}}}
    {\acut{\acut{S,T},\acut{U,V}},\acut{\acut{\ins{\phantom{S}}T},\acut{V}}}}
    {\acut{\acut{S,T},\acut{U\ins{\phantom{V}}}},\acut{\acut{T},\acut{V}}}}
    {\acut{\acut{S\ins{\phantom{T}}},\acut{U}},\acut{\acut{T},\acut{V}}}
  \end{equation*}

  The other rules correspond to equations on \kl{structures}: $\alpha$ for
  \emph{associativity}, $\sigma$ for \emph{commutativity}, and $\mathsf{f}$ and
  $\mathsf{t}$ for unitality. Note that all rules involving $\land$ and $\top$
  are trivially simulated by the \kl{isotropy} of $\SA$. Simulating the other
  rules only requires the double-\kl{cut} rules, substantiating our claim (based
  on Peirce's own view, see \refsec{atomicity}) that the latter should be seen
  as expressing a structural equivalence, rather than as \textit{bona fide}
  \kl{inference rules}.

  $$
  \begin{array}{r@{\mt}l@{\qquad\qquad}r@{\mt}l@{\vsp}}
    \R[\alpha{\da}]
      {S \lor (T \lor U)}
      {(S \lor T) \lor U}
    &
    \R[\kl{Dcut{\da}}]
    {\R[\kl{Dcut{\ua}}]
    {\acut{\acut{S},\acut{\acut{\acut{T},\acut{U}}}}}
    {\acut{\acut{S},\dcut{\acut{T},\acut{U}}}}}
    {\acut{\acut{\acut{\dcut{\acut{S},\acut{T}}}},\acut{U}}}
    &
    \R[\alpha{\ua}]
      {S \land (T \land U)}
      {(S \land T) \land U}
    &
    S, T, U
    \\
    \R[\sigma{\da}]
      {S \lor T}
      {T \lor S}
    &
    \acut{\acut{S},\acut{T}}
    &
    \R[\sigma{\ua}]
      {S \land T}
      {T \land S}
    &
    S, T
    \\
    \R[\kl{f{\da}}]
      {S}
      {S \lor \bot}
    &
    \R[\kl{Dcut{\da}}]
    {\R[\kl{Dcut{\da}}]
    {S}
    {\acut{\acut{\dcut{S}}}}}
    {\acut{\acut{S},\acut{\acut{\dcut{\phantom{\!!\!}}}}}}
    &
    \R[\kl{f{\ua}}]
      {\top \land S}
      {S}
    &
    S
    \\
    \R[\kl{t{\da}}]
      {S}
      {S \land \top}
    &
    S
    &
    \R[\kl{t{\ua}}]
      {\bot \lor S}
      {S}
    &
    \R[\kl{Dcut{\ua}}]
    {\R[\kl{Dcut{\ua}}]
    {\acut{\acut{\acut{}},\acut{S}}}
    {\acut{\acut{\dcut{\phantom{\!!\!}}S}}}}
    {\dcut{S}}
    \\
  \end{array}
  $$
\end{proof}

\subsection{Analyticity}

\AP
A powerful result of Brünnler and Tiu \cite{brunnler_local_2001}, is that the
whole up-fragment of \kl{SKS} (all rules whose name ends with $\ua$) is
\emph{\kl{admissible}}: if a \kl{structure} $S$ has a \emph{proof} $S \steps{} \top$, then
it also has a proof $S \xsteps{\kl{KS}} \top$, with \intro{KS} defined as
\kl{SKS} without the up-fragment\sidenote{The name \kl{KS} comes from \kl{SKS},
with the first `S' standing for ``symmetric'' dropped.}. Dually, the whole
down-fragment (all rules whose name ends with $\da$) is
\emph{``co-admissible''}: if a \kl{structure} $S$ has a \emph{refutation} $\bot
\steps{} S$, then it also has a refutation $\bot \xsteps{\kl{dualKS}} S$,
with $\intro{dualKS}$ defined as \kl{SKS} without the down-fragment.

This duality reflects nicely in our simulation: we were careful to always give
derivations for up-rules that mirror closely those for down-rules, modulo the
use of the double-\kl{cut} principle. Roughly, if the simulation of $S \xstep{r} T$
has the shape $\strans{S} \xstep{r_1} \ldots \xstep{r_n} \strans{T}$, then the
simulation of $\dmdual{T} \xstep{\dmdual{r}} \dmdual{S}$ has the shape
$\strans{\dmdual{T}} \xstep{\dmdual{r_n}} \ldots \xstep{\dmdual{r_1}}
\strans{\dmdual{S}}$ (see \reffact{eg-duality}). An important consequence is
that the deletion rule \kl{Del} is never used in the simulation of \kl{KS}, if
one chooses the appropriate derivation among the two provided for the switch and
medial rules \kl{s} and \kl{m}. Thus deletion is \kl{admissible} in \kl{Alpha}, a
result that seems to be novel in the literature on \kl{EGs}.

\begin{corollary}[Admissibility of Deletion]\labcor{adm-era}
  $$
  \text{If $G \steps{} \emptyset$, then $G \xsteps{\kl{Alpha}\setminus\{\kl{Del}\}}
  \emptyset$.}
  $$
\end{corollary}

Dually, the insertion rule \kl{Ins} is never used in the simulation of
\kl{dualKS}, implying the co-admissibility of insertion. In fact there is
a curious dissymmetry, in that the rule \kl{Dcut{\da}} of \emph{double-\kl{cut}}
insertion also never appears in the simulation of \kl{dualKS}, while
\kl{Dcut{\ua}} is used multiple times in the simulation of \kl{KS}:

\begin{corollary}[Co-admissibility of Insertion]\labcor{adm-ins}
  $$
  \text{If $\acut{} \steps{} G$, then $\acut{}
  \xsteps{\kl{Alpha}\setminus\{\kl{Ins},\kl{Dcut{\da}}\}} G$.}
  $$
\end{corollary}

 \refcor{adm-era} is what allows us to conclude that \kl{Alpha} is an
\emph{\kl{analytic}} system, in a sense very close to that of Gentzen. Because we do
not have a notion of logical connective nor tree-shaped derivations, we must
reduce the \kl{subformula property} to \emph{atomic} \kl{graphs}. Then it is easy to see
that all rules of \kl{Alpha} except \kl{Del} satisfy this property:

\begin{definition}[Subgraph]
  \AP
  A \kl{graph} $G$ is a \intro{subgraph} of a \kl{graph} $H$, written $G \intro*\subgraph
  H$, if there exists a \kl{context} $K\hole$ such that $H = K\select{G}$.
\end{definition}

\begin{fact}
  For every $a \in \atoms$, if $G \xstep{\kl{Alpha} \setminus \{\mathsf{Del}\}}
  H$ and $a \subgraph H$ then $a \subgraph G$.
\end{fact}

\begin{corollary}[Analyticity]
  If $G$ is provable in \kl{Alpha}, then it has a proof $G \step{} G_1 \step{}
  \ldots \step{} G_n \step{} \emptyset$ where $a \subgraph G$ for all $i$ and $a
  \in \atoms$ such that $a \subgraph G_i$.
\end{corollary}

\end{scope}


\section{Beta graphs}\labsec{beta}

\begin{scope}\knowledgeimport{beta}

Before working on \kl{EGs}, Peirce had already developed a deep understanding of
the logic of relations of arbitrary arity, inventing the notions of variables
and quantifiers 30 years before the standard Russell-Whitehead syntax for
\kl{predicate logic} appeared in 1910 \sidecite{sep-peirce}. This all stemmed
from his extensive study of \emph{relation algebras}, first investigated by De
Morgan in 1860. However in his system \kl{Beta} of \kl{EGs}, Peirce gives a very
different account of the logic of relations, both in the graphical
representation of relational statements, and the \kl{illative transformations}
that govern them. In the following, we illustrate informally the principles of
\kl{Beta}, and how they are able to capture what is identified nowadays in
\kl{symbolic} logic as \emph{purely relational} \kl{first-order} theories (that
is, without constant nor function symbols) equipped with a primitive equality
predicate.

\subsection{Syntax}

\paragraph{Spots}

In the propositional system \kl{Alpha}, atomic \kl(alpha){graphs} represent
sentences that can be asserted or denied (or equivalently, assigned a truth
value), but they do not exhibit any internal structure syntactically: they might
as well be depicted just as (distinguished) points on $\SA$. As most logicians
of his time, Peirce was directly influenced by the \emph{term logic} of
Aristotle, where assertions are decomposed into a \emph{subject} to which
applies some \emph{predicate}. However, while Aristotle's notion of predicate
has a metaphysical flavor, Peirce's notion is purely grammatical. For instance,
Aristotle rejected the sentence ``The person sitting down is Socrates'' as a
genuine predication, because Socrates is an \emph{individual}, and in his view
predicates could only be so-called \emph{universals} like ``humans'' or
``mortals'' \sidecite{smith_aristotles_2022}. In \kl{Beta} there is no such
restriction, and the previous sentence can be represented by the following
\kl{graph}:
\begin{equation}
  \tikzfig{1}{0.5}{sitting-socrates} \label{eq:sitting-socrates}
\end{equation}
\AP
Here both ``The person sitting down'' and ``Socrates'' are modelled as unary
predicates. The little diamonds, called \intro{hooks} by Peirce, represent
placeholders for the arguments (subjects) of each predicate, and the data of a
predicate together with its \kl{hooks} is called a \intro{spot}\sidenote{The use
of little diamonds to depict \kl{hooks} is our own addition, Peirce never drew
them explicitly.}. Any predicate of arity $n$ can then be represented by a
\kl{spot} with $n$ \kl{hooks} disposed freely around its periphery. For
instance, the sentence ``Paris is between London and Berlin'' can be expressed
by the \kl{graph}
\begin{equation*}
  \tikzfig{1}{0.5}{paris-between-london-berlin} \label{eq:paris-between-london-berlin}
\end{equation*}
where ``between'' is modelled as a ternary predicate.

\paragraph{Lines of Identity}

\AP To assert that there exists an individual who is both the person sitting
down and Socrates in \kl{graph} (\ref{eq:sitting-socrates}), we connect the two
\kl{hooks} with a so-called \intro{line of identity} (\reintro{LoI}). In
Peirce's view, each point in a \kl{LoI} denotes an individual of the universe
represented by $\SA$. Since scribing anything on $\SA$ means asserting its truth
in the universe, then it suffices to reduce the truth of an individual to its
existence, in order to interpret the marking of \kl{LoIs} as having existential
force. This is actually the origin of the ``existential'' qualificative in the
denomination ``existential graph''. Note that no information is given but the
individual's existence: in particular, two distinct points on $\SA$ may or may
not denote two distinct individuals, just as two distinct variables $x$ and $y$
in predicate calculus may or may not refer to the same object. It is the
\emph{continuity} of a \kl{LoI} that signifies, in an \kl{iconic} way, the
identity of every point/individual constituting, and connected by the line.
Hence the following
\kl{graph}
$$\tikzfig{1}{0.5}{sitting-socrates-disconnected}$$ expresses that both the
person sitting down and Socrates exist, but we do not know whether they are the
same individual.

\paragraph{First-order logic} ``The person sitting down is Socrates'' might be
equivalently expressed in a \kl{first-order} language as the formula
\begin{equation}
  \exists x. \mathsf{PersonSittingDown}(x) \wedge \mathsf{Socrates}(x) \label{eq:sitting-socrates-fo}
\end{equation}
\kl{LoIs} can then be seen as encoding the concept of \emph{existential
quantification} over a single \emph{variable} $x$, where the occurrences of $x$
correspond to the extremities of the line connected to the \kl{hooks}. As in
\kl{Alpha}, the conjunctive aspect of the sentence is accounted for by the fact
that the two \kl{spots} are juxtaposed in the same area on $\SA$.

Now if one considers a \kl{first-order} theory with a predicate symbol $=$
satisfying the usual \kl{axioms} for equality, then our sentence can
alternatively be expressed by the following formula:
\begin{equation}
  \exists x. \exists y. \mathsf{PersonSittingDown}(x) \wedge
  \mathsf{Socrates}(y) \wedge x = y \label{eq:sitting-socrates-fo-eq}
\end{equation}

\subsection{Deconstructing identity}

In Peirce's original notation, there is no way to distinguish between the two
formulations (\ref{eq:sitting-socrates-fo}) and
(\ref{eq:sitting-socrates-fo-eq}), as they would both be represented by
\kl{graph} (\ref{eq:sitting-socrates}). However in
\sidecite{pietarinen_compositional_2020}, in order to have a rigorous
interpretation of the syntax of \kl{Beta} in category theory, the authors
propose to analyze \kl{LoIs} into essentially two distinct \kl{icons},
\emph{\kl{binders}} and \emph{\kl{teridentities}}\sidenote{This is our own
terminology, chosen mainly for historical reasons. In
\cite{pietarinen_compositional_2020}, \kl{binders} and \kl{teridentities}
correspond to the generators of the monoid-comonoid pair of a Frobenius
algebra.}, from which every \kl{LoI} can be reconstructed:

\begin{marginfigure}
  \input{figures/eg-forall.tex}
  \caption{Universal quantification $\forall x. P(x)$ in \kl{Beta}}
  \labfig{eg-forall}
\end{marginfigure}

\begin{description}
  \item[Binder] As we have just seen, one function of \kl{LoIs} is to \emph{quantify}
  over individuals. For now we have only considered \kl{LoIs} located at the top-level
  of $\SA$, i.e. in a \emph{\kl{positive}} area, where they are given
  \emph{existential} force. If we were to negate the \kl{graph}
  (\ref{eq:sitting-socrates}) by enclosing it in a \kl{cut}, we would get a \kl{graph}
  expressing the negation of formula (\ref{eq:sitting-socrates-fo}), which by De
  Morgan duality is equivalent to
  $$\forall x. \neg \mathsf{PersonSittingDown}(x) \vee \neg
  \mathsf{Socrates}(x)$$ More generally, it is well-known that in \kl{classical}
  logic, \emph{universal} quantification can be defined \kl{symbolically} by
  $\forall x. P(x) \defeq \neg \exists x. \neg P(x)$, and hence that any formula
  in the usual language of \kl{FOL} has a \kl{classically} equivalent formula in
  the fragment $\{\neg, \land, \exists\}$. This is precisely how Peirce
  expresses universal quantification in \kl{Beta}, as illustrated for a unary
  predicate $P$ by the \kl{graph} (\ref{eq:eg-forall}) of \reffig{eg-forall}.
  But in order to interpret correctly this \kl{graph}, one needs to adopt what
  Peirce calls an \intro{endoporeutic}\sidenote{From the greek \emph{endon}
  (`within') and \emph{poros} (`passage, pore') \cite{endoporeutic}, literally
  ``that lets through within''. This physically-flavored terminology is
  reminiscent of the intuition we developed within our own \kl{bubble calculus}
  and illustrated in \reffig{bubbles-flow}, even though we were not aware of the
  existence of \kl{EGs} at the time!} reading: one should start inspecting the
  \kl{graph} from the \emph{top-level} of $\SA$, and then descend (recursively)
  into its various \kl{cuts}. In particular, the location of a \kl{LoI} should
  be identified with its \emph{outermost} end, as illustrated in \kl{graph}
  \ref{eq:eg-forall-out}; if we were to associate it instead with its innermost
  end as in \kl{graph} (\ref{eq:eg-forall-in}), then we would swap the positions
  of $\exists$ and $\neg$ in the associated statement, giving the non-equivalent
  formula $\neg \neg \exists x. P(x)$.

  \AP
  From these observations, we get that the type of quantification prformed by a
  \kl{LoI} is fully captured by its location: existential in a \kl{positive}
  area, universal in a \kl{negative} area. This leads us to analyze the syntax
  of \kl{LoIs} into two components: so-called \intro{binders} that encode
  \emph{quantifiers} as distinguished heavy dots on $\SA$ like those of
  \reffig{eg-forall}\sidenote{Peirce liked to insist that \kl{LoIs} should be
  drawn as \emph{heavy} lines, to distinguish them from the normal lines used to
  depict \kl{cuts}.}; and two-ended \intro{wires} that connect \kl{binders} to the
  \kl{hooks} of predicates, encoding the \emph{identity} between a bound
  variable $x$ and an occurrence of $x$.
  
  \begin{marginfigure}
    \begin{center}
    \tikzfig{1}{0.5}{eg-forall-imp}
    \end{center}
    \caption{Implication $A \limp B$ in \kl{Alpha}}
    \labfig{eg-forall-imp}
  \end{marginfigure}
  
  \begin{remark}\labremark{eg-forall-type-theory}
    
    The \kl{graph} of universal quantification (\ref{eq:eg-forall-out}) bears a
    striking similarity to that of implication, as illustrated in
    \reffig{eg-forall-imp}. While the similarity also exists for the
    \kl{symbolic} encoding of these connectives in the fragment $\{\neg, \land,
    \exists\}$ captured by \kl{Beta}, the graphical representation makes this
    fact more apparent. In \kl[Martin-Löf type theory]{constructive type
    theories}, both universal quantification $\forall x. B$ and implication $A
    \limp B$ are seen as instances of a more general construct, the
    \emph{dependent product \kl{type}} $\Pi x : A. B$. Thus retrospectively, one
    might interpret the above observation as another manifestation of Peirce's
    pre-\kl{intuitionistic} conception of logic.
  \end{remark}

  \item[Teridentity]
  
  \begin{marginfigure}
    \begin{gather}
    \tikzfig{0.8}{0.5}{sitting-socrates-teridentity} \label{eq:sitting-socrates-teridentity} \\[1em]
    \tikzfig{0.8}{0.5}{sitting-socrates-eq-teridentity} \label{eq:sitting-socrates-eq-teridentity}
    \end{gather}
    \caption{Decomposing \kl{lines of identity}}
    \labfig{sitting-socrates-teridentity}
  \end{marginfigure}

  \AP
  \kl{Binders} and \kl{wires} are sufficient to express unary predicates applied to
  distinct bound variables, but they cannot identify multiple occurrences of the
  \emph{same} variable, and thus cannot account for \kl{graph}
  (\ref{eq:sitting-socrates}). To palliate this, Peirce used a construct that he
  called \intro{teridentity}, that we propose to represent as a black triangle
  $\tikz{\ter{t}{0,0}}$. The three vertices of the triangle should be seen as
  three \emph{plugs} on which one can connect \kl{wires}, so that the \kl{graph}
  $$\tikzfig{1}{0.5}{teridentity}$$
  can be interpreted as the formula $\exists x. \exists y. \exists z. x = y
  \wedge y = z$. Rather than a way to express equality between variables, we
  think it is more useful however to have an \emph{operational} understanding of
  \kl{teridentities}: their real purpose is to \emph{duplicate} \kl{wires} (e.g. by
  splitting them), or dually to \emph{merge} two \kl{wires} into a single one. This
  gives a constructive way to explain the notion of \emph{occurrence} of a
  variable\sidenote{A very similar \kl{metaphor} is brought up by Girard in
  \cite{girard:hal-01322183}, where the fact that a variable can have infinitely
  many occurrences is seen precisely as a consequence of the possibility to
  split or ``debit'' indefinitely a wire into smaller wires.}. Then formulas
  (\ref{eq:sitting-socrates-fo}) and (\ref{eq:sitting-socrates-fo-eq}) can be
  represented faithfully and respectively by the \kl{graphs}
  (\ref{eq:sitting-socrates-teridentity}) and
  (\ref{eq:sitting-socrates-eq-teridentity}) of
  \reffig{sitting-socrates-teridentity}.
\end{description}

\kl{LoIs} (or their decomposition into the above constructs) constitute the only \kl{icons}
introduced in \kl{Beta} compared to \kl{Alpha}.
% We will see shortly that they are sufficiently powerful to express and combine
% all features of \kl{first-order logic} in an elegant and intuitive way, particulary
% so at the \emph{illative} level.

\paragraph{Iconic atomicity}
The fact that Peirce took \kl{LoIs} to be a primitive, unanalyzed \kl{icon} can be seen as
a consequence (or a cause?) of his view that only \emph{closed} sentences should
be considered. Indeed in an early exposition of \kl{EGs} (Ms 493), he proposed a way
to show, if necessary, that ``a complete assertion is not intended''
\cite[p.~49]{Roberts+1973}. That is, he devised a syntax equivalent in purpose
to that of \emph{free variables} in predicate calculus. Our analysis into \kl{wires}
that connect \kl{hooks}, \kl{binders} and \kl{teridentities} allows this, and more generally
makes the syntax of \kl{graphs} closer to predicate calculus. But in later
expositions, Peirce always required every \kl{hook} to be filled with a \kl{LoI}, that is
every variable to be quantified.

\begin{marginfigure}
  \begin{center}
  \tikzfig{1.25}{0.5}{wire-teridentity}
  \end{center}
  \caption{Building a two-ended \kl{wire} from two \kl{teridentities}}
  \labfig{wire-teridentity}
\end{marginfigure}

Now, it is unclear which of those presentations is the more ``analytical'' or
primitive, when restricting oneself to closed (or in Peirce's terminology,
complete) assertions. Indeed, consider that we give the force of quantification
to \kl{wires}, as Peirce does with \kl{LoIs}: then one can replace every \kl{binder} by a
dangling \kl{wire}, rendering \kl{binders} useless in the syntax. Also since empty \kl{hooks}
and \kl{teridentities} are forbidden, they will always be filled with \kl{wires}. Then
couldn't we just reduce the full syntax of \kl{LoIs} to \kl{wires}?

A remarkable insight of Peirce, is that one cannot build the concept of
\kl{teridentity} from two-ended \kl{wires}, but that the converse \emph{is}
possible, as illustrated in \reffig{wire-teridentity}. This is most clearly (and
speculatively) understood by seeing \kl{LoIs} as made out of \emph{pipes} rather
than \kl{wires}. Indeed, just gluing one extremity of a two-ended pipe
$\mathtt{P}$ to the exterior of another pipe $\mathtt{Q}$ will not make
$\mathtt{P}$ and $\mathtt{Q}$ communicate; and there is no reason to interpret
the joining, or (infinitesimally) close \kl{juxtaposition} of two lines on $\SA$
at one point, as doing more than the gluing of two pipes.

\begin{digression}
  If one wants to keep the \kl{wire} \kl{metaphor}, joining two \kl{LoIs} can be
interpreted as just putting in contact two \kl{wires}. Then electrical current
can flow from one \kl{wire} to the other, but the problem resides elsewhere, at
the \emph{illative} level: indeed nothing prevents us from separating back the
two \kl{wires}, or connecting two initially disjoint \kl{wires}. But in a
\kl{negative} (resp. \kl{positive}) \kl{context}, the former (resp. latter)
\kl{action} corresponds to forgetting a possibly necessary equality hypothesis
between two variables (resp. identifying two possibly distinct individuals),
which is not valid logically speaking. To prevent this, one needs to do more
than just put the two \kl{wires} in contact with eachother (\kl{juxtaposition}),
i.e. solder them together (\kl{teridentity}).
\end{digression}

One solution would be to first drill a hole in $\mathtt{Q}$, before glueing
$\mathtt{P}$ on it. Another is to have \emph{branching} pipes as the basic
building blocks for our plumbing, i.e. \kl{teridentities}. This is how we
interpret and justify \kl{metaphorically} the following enigmatic quote from
Peirce \cite[p.~116]{Roberts+1973}:

\begin{quote}
  Teridentity is not mere identity. It is identity and identity, but this `and'
is a distinct concept [from that denoted by the \kl{juxtaposition} of \kl{graphs} on
$\SA$], and is precisely that of teridentity.
\end{quote}

Then every \kl{LoI} can be built out of \kl{teridentities}, which Peirce expressed like
so \cite[p.~117]{Roberts+1973}:
% \sidenote{This is again reminiscent of Girard's metaphor, where the fact that
% a cable is just a ``bundle of very small wires'' corresponds to the
% decomposition of LoI into microscopic points of teridentities.}
\begin{quote}
  Every line of identity ought to be considered as bristling with microscopic
points of teridentity; so that
$\tikz{\draw (0,0) -- (2,0);}$
when magnified shall be seen to be
$\tikz{\draw (0,0) -- (2,0);
       \draw[decorate,decoration=zigzag] (0.1,0) -- (1.9,0);}$.
\end{quote}
  
\subsection{Rules}

A remarkable fact about \kl{Beta} is that it does not need any new illative
principle compared to \kl{Alpha}. Rather, it simply generalizes those of
\kl{Alpha} to account for \kl{LoIs}\sidenote{Thus in a sense, the
\kl{first-order} individuals denoted by \kl{LoIs} behave in the exact same way
as the propositions denoted by the \kl{graphs} of \kl{Alpha}. This can be
interpreted as a manifestation of Peirce's \emph{psycho-physical monism}
\cite{sep-peirce}, that blurs the distinction between the psychological level of
propositions (concepts) and the physical level of individuals (objects) enforced
in the language of \kl{predicate logic}, and inherited from Aristotle's
metaphysical conception of predication. Independently, Girard has recently been
pushing the idea further in his transcendental syntax programme, by proposing to
see individuals as particular kinds of \emph{linear} propositions (see \cite[\S
84.3]{eng_exegesis_2023}). And what is more linear than a line?}.

\begin{description}
  \item[Iteration/Deiteration] When a \kl{spot} is iterated, every \kl{hook} of the new
  copy must be connected to the same \kl{hook} of the original copy with a \kl{LoI}.
  Conversely, when a \kl{spot} is deiterated, every \kl{LoI} of the deleted copy must be
  retracted to the corresponding \kl{hook} of the original copy. This applies in
  particular to any \kl{binder} seen as a unary \kl{spot}, which allows to extend (resp.
  retract) any \kl{LoI} inside (resp. outside) a \kl{cut}:
  $$\tikzfig{1}{0.5}{eg-iter-loi-left} ~~~\leftrightarrow~~~ \tikzfig{1}{0.5}{eg-iter-loi-right}$$

  \item[Insertion/Deletion] Every pair of \kl{binders} residing in the same \kl{negative}
  area can be connected and replaced by a \kl{wire} (\kl{Insertion}):
  $$\nsheet{\tikzfig{1}{0.5}{eg-delins-loi-left}~~~\step{}~~~\tikzfig{1}{0.5}{eg-delins-loi-right}}$$
  Dually, every \kl{wire} in a \kl{positive} area can be severed in two, capping off the
  newly created ends with two \kl{binders} (\kl{Deletion}):
  $$\tikzfig{1}{0.5}{eg-delins-loi-right}~~~\step{}~~~\tikzfig{1}{0.5}{eg-delins-loi-left}$$
  By reading the rules from right to left --- i.e. in \emph{proof search} mode,
  \kl{Insertion} and \kl{Deletion} on \kl{LoIs} can be understood as capturing
  respectively the operations of \emph{anti-unification} and \emph{unification}
  on two variables. That is:
  \begin{description}
    \item[Unification] adding a \kl{wire} between two disconnected \kl{binders} $x$
  and $y$ is equivalent in purpose to substituting $x$ for $y$ (resp. $y$ for
  $x$) in every \kl{spot}/predicate connected by a \kl{LoI} to $y$ (resp. $x$);
    \item[Anti-unification] while severing a \kl{wire} connected to a \kl{binder}
  $z$ in two parts capped by \kl{binders} $x$ and $y$ amounts to partitioning the set
  of \kl{spots} connected to $z$ in two sets $\{P_i\}$ and $\{Q_j\}$, and
  substituting $x$ for $z$ in every $P_i$, and $y$ for $z$ in every $Q_j$.
  \end{description}
  Unification and anti-unification are the heart of many (semi-)decision
  procedures implemented in automated and \kl{interactive theorem provers},
  including the \kl{unification} of subformulas in our own approach to \kl{SFL}
  (\refsubsec{identity}); thus it is remarkable that they constitute a core
  illative principle of \kl{Beta}.

\end{description}

\begin{remark}
In the original \kl{Beta} system, Peirce enforces the usual \kl{model-theoretic}
assumption that the universe of discourse must be non-empty --- i.e. contain at
least one individual, through an \emph{\kl{axiom}} permitting to ``scribe a heavy dot
or unattached line on $\SA$'' \cite[p.~47]{Roberts+1973}. In fact together with
the \kl{axiom} allowing to assert the blank $\SA$, which was implicit in our notion
of proof for \kl{Alpha} (\refdef{eg-proof}), these are the only \kl{axioms} in all
systems of \kl{EGs}. This is yet another striking similarity with Girard's philosophy,
who attempted to get rid of \kl{axioms} in logic starting with ludics --- although in
many of his writings, he actively criticizes the non-empty model assumption.
\end{remark}

\begin{figure*}
  \input{figures/eg-socrates-mortal.tex}
  \caption{A proof of a famous syllogism in \kl{Beta}}
  \labfig{eg-socrates-mortal}
\end{figure*}

\reffig{eg-socrates-mortal} gives a proof of the famous syllogism from Aristotle
in \kl{Beta}, by reducing the \kl{graph} associated to the formula
\begin{equation}
  \begin{array}{l}
    \forall x. \socrates(x) \land \human(x) \land (\forall y. \human(y) \limp \mortal(y)) \limp \\
    \exists z. \socrates(z) \land \mortal(z) \label{eq:socrates-mortal-fo}
  \end{array}
\end{equation}
to the empty $\SA$. Again, we invert the direction of arrows in inference steps,
to follow the proof search reading of rules. Note that in many steps, we add or
remove some \kl{teridentities} and \kl{binders} without further justification: these
correspond to splits, merges and rewirings of \kl{LoIs}, and a more rigorous set of
equations describing these operations can be found in \cite[Section 3: ``The
algebra of lines of identity'']{pietarinen_compositional_2020}.

The essence of the syllogism lies in the instantiation of the universally
quantified variable $y$ by $x$ in formula (\ref{eq:socrates-mortal-fo}),
captured by the \kl{Deletion} step in \reffig{eg-socrates-mortal}. Thus
contrary to \kl{Alpha}, it seems that \kl{Deletion} is not \kl{admissible} in
\kl{Beta} anymore. Because of the \emph{subterm property} of \kl{first-order logic}
\sidecite{DBLP:books/el/RV01/DegtyarevV01}, this should not break \kl{analyticity}:
indeed we should only need \kl{Deletion} on \kl{LoIs}, which connects already
existing \kl{binders}. However one still needs to find out the \kl{binders} that must be
connected, which we conjecture to be a major factor in the undecidability of
\kl{first-order logic}.

\section{Gardens}\labsec{gardens}

\paragraph{Ergonomy of LoIs}

Overall, the example of \reffig{eg-socrates-mortal} demonstrates how \kl{Beta}
is particularly well-suited to analyze the fine structure of relational
reasoning: be it at the level of \emph{statements}, with the complex circuits
resulting from the composition of \kl{LoIs}; or at the level of \emph{proofs}, with a
decomposition of such a simple syllogism into 8 distinct inferential steps.
While this is satisfying from the standpoint of meta-logical investigation
originally pursued by Peirce, this syntax seems to be too cumbersome to form the
basis for a practical theorem proving interface, where the user would perform
\kl{illative transformations} through \kl{direct manipulation} of \kl{graphs}. In the words of
Peirce himself \cite[p.~544]{peirce_prolegomena_1906}:
\begin{quote}
  There are a number of deduced liberties of transformation, by which even much
  more complicated inferences than a syllogism can be performed at a stroke. For
  that sort of problem, however, which consists in drawing a conclusion or
  assuring oneself of its correctness, this System is not particularly adapted.
\end{quote}

\paragraph{Variables}

\begin{marginfigure}
  \hspace{-3em}
  \tikzfig{0.8}{0.5}{eg-socrates-mortal-var}
  \caption{Using variables in \kl{EGs}}
  \labfig{eg-socrates-mortal-var}
\end{marginfigure}

Still, the complexity of \kl{Beta} from a \kl{UX} perspective stems mostly from the
tedious management of \kl{LoIs}. Our analysis into \kl{binders} and \kl{teridentities} gives us
a hint towards the solution: since we now have \kl{binders}, why not just replace
the complex circuits of \kl{teridentities} by \emph{variables}? The process is
simple:
\begin{enumerate}
  \item Take any complex \kl{LoI} connected to $n$ \kl{binders} and $m$ \kl{hooks};
  \item Among the $n$ \kl{binders}, choose one that occurs in the outermost area of
  $\SA$, and give it a fresh name $x$;
  \item Replace the $m$ \kl{wires} connected to the \kl{hooks} by $m$ occurrences of $x$;
  \item Erase all remaining \kl{wires} and \kl{teridentities}.
\end{enumerate}
Thus for instance, both the first and second \kl{graphs} in
\reffig{eg-socrates-mortal} would be represented by the \kl{graph} of
\reffig{eg-socrates-mortal-var}. In fact, Peirce already had the idea to use a
name-based syntactic device similar to, and arguably more primitive than
variables, which he called \emph{selectives}, in order to avoid the ambiguity of
\kl{LoIs} crossing \kl{cuts} \cite[p.~531]{peirce_prolegomena_1906}:
\begin{quote}
A Ligature crossing a Cut is to be interpreted as unchanged in meaning by
erasing the part that crosses to the Cut and attaching to the two Loose Ends so
produced two Instances of a Proper Name nowhere else used; such a Proper name
(for which a capital letter will serve) being termed a \emph{Selective}.
\end{quote}
The idea of connecting two locations by marking them with the same \kl{symbol} is
quite natural, and is implemented for instance in \emph{footnotes} --- or in
this thesis, sidenotes --- with the help of \emph{numbers} rather than capital
letters. Footnotes are a good example, because contrary to variables, they share
with selectives a \emph{linearity} property, that exactly \emph{two} occurrences
of the \kl{symbol} must be present. This is necessary to simulate accurately a
two-ended \kl{wire}, and it is not surprising that the same device has been used
recently (under the name of \emph{ports}) to give an algebraic syntax to
\emph{interaction nets} \sidecite{10.1007/978-3-642-40184-8_15}, a model of
computation inspired by \emph{linear} logic and its graphical, string-diagram
like syntax of \emph{proof nets} \cite{girard-linear-1987}.

\paragraph{Bridges} 

\begin{marginfigure}
  \includegraphics{bridge.png}
  \caption{A depiction of Peirce's Bridge for \kl{lines of identity}}
  \footnotesize Source:\hspace{3pt}\url{https://commons.wikimedia.org/wiki/File:4CT_Inadequacy_Explanation.svg}
  \labfig{eg-bridge}
\end{marginfigure}

After introducing selectives, Peirce further remarks on the next page:
\begin{quote}
  In order to avoid the intersection of Lines of Identity, either a Selective
may be employed, or a \emph{Bridge}, which is imagined to be a bit of paper
ribbon.
\end{quote}
Thus he already identified the problem of readability stemming from having too
many \kl{wires} crossing eachother, a well-known concern in the design of graphical
programming languages\sidenote{This is to be opposed to critics of the syntax of
EGs such as Quine, who devised a notation similar to \kl{LoIs}, and deemed it ``too
cumbersome for practical use'' \cite[p.~125]{Roberts+1973}.}. The proposed
alternative solution of having so-called \emph{bridges} is quite interesting, in
that it makes the syntax of \kl{EGs} \emph{three-dimensional}, in order to preserve
the continuity of lines. A nice illustration of the bridge is given in
\reffig{eg-bridge}. We found this picture in the Wikipedia article of the
\emph{four color theorem} \cite{noauthor_four_2023}, which is no coincidence
according to Burch \cite{sep-peirce}:
\begin{quote}
  Peirce began to research the four-color map conjecture, to work on the
graphical mathematics of de Morgan's associate A. B. Kempe, and to develop
extensive connections between logic, algebra, and topology, especially
topological graph theory. Ultimately these researches bore fruit in his
existential graphs [...]
\end{quote}

\paragraph{Multisets}

The Wikipedia article on Alfred Kempe also mentions the following interesting
fact \cite{noauthor_alfred_2023}:
\begin{quote}
  Kempe (1886) revealed a rather marked philosophical bent, and much influenced
Charles Sanders Peirce. Kempe also discovered what are now called multisets,
although this fact was not noted until long after his death
\sidecite{kempe_i_1997,kempe-search}.
\end{quote}
As it turns out, we can also give a multiset formalization of the syntax of
\kl{graphs} in \kl{Beta} extending that of \refsec{multisets}, and based on the
previous idea of replacing \kl{teridentities} by variables. Every area will now be
equipped with a set of \kl{binders}, in addition to the multiset of nodes (i.e. atoms
and \kl{cuts}). Anticipating our flower \kl{metaphor} of \refch{flowers}, we call
sets of \kl{binders} \intro{sprinklers}, which can be imagined as irrigating \kl{spots} on
their \kl{hooks} by sending water through the (now invisible) \kl{LoIs}, seen as hoses.
Naturally, the pair formed by a sprinkler and a multiset of nodes is called a
\emph{\kl{garden}}.

\newpage
\begin{definition}[Graph]\labdef{beta-graph}

  \AP Given a denumerable set of variables $\intro*\vars$ and a denumerable set
  of predicate symbols $\intro*\psymbs$ together with their arities
  $\intro*\arity : \psymbs \to \nats$, the sets of \intro{nodes}
  $\intro*\bnodes$, \intro{gardens} $\intro*\bgardens$ and \intro{graphs}
  $\intro*\bgraphs$ are defined mutually inductively as follows:
  \begin{itemize}
    \itemAP \textbf{(Spot)} If $p \in \psymbs$ with $\arity(p) = n$, then $p(x_1,
    \ldots, x_n) \in \bnodes$;
    \itemAP \textbf{(Graph)} If $G \subset \bnodes$ is a finite multiset, then $G
    \in \bgraphs$.
    \itemAP \textbf{(Garden)} If $\bx \subset \vars$ is a finite set and $G
    \subset \bnodes$ a finite multiset, then $\intro*\bgarden{\bx}{G} \in
    \bgardens$;
    \itemAP \textbf{(Enclosure)} If $\gamma \in \bgardens$, then
    $\intro*\bcut{\gamma} \in \bnodes$.
  \end{itemize}
\end{definition}

\begin{example}
  The \kl{graph} of \reffig{eg-socrates-mortal-var} can be written in textual
  notation as the following expression:
  \begin{align*}
    \bcut{&x \cdot \socrates(x),\human(x), \\
     &\bcut{y \cdot \human(y),\bcut{{} \cdot \mortal(y)}}, \\
     &\bcut{z \cdot \socrates(z),\mortal(z)}}
  \end{align*}
  Note that the `$\cdot$' operator for constructing \kl{gardens} has lower precedence
  than the `$,$' operator for \kl{juxtaposition} of \kl{graphs}. Thus the expression $x
  \cdot \socrates(x),\human(x)$ is to be read as $x \cdot
  (\socrates(x),\human(x))$, and not $(x \cdot \socrates(x)),\human(x)$ (which
  would be ill-typed anyway).
\end{example}

\paragraph{Inference rules}

\begin{figure}
  \input{figures/beta.tex}
  \caption{Inductive presentation of the rules of \kl{Beta}}
  \labfig{beta}
\end{figure}

\begin{marginfigure}
  \input{figures/eg-socrates-mortal-garden.tex}
  \caption{A proof in the inductive syntax of \kl{Beta}}
  \labfig{eg-socrates-mortal-garden}
\end{marginfigure}

As already suggested earlier, the \kl{garden} syntax for \kl{graphs} quotients the
\kl{LoI} syntax: the first \kl{Deiteration} step in \reffig{eg-socrates-mortal}
cannot be performed in the \kl{garden} syntax, because its premiss and conclusion are
represented by the same \kl{graph}. In fact, both \kl{Iteration} and
\kl{Deiteration} are automatically handled by the notion of \emph{scope} for
\kl{binders}, that results from the \kl{endoporeutic} reading of \kl{graphs}.
Then it only remains to account for \kl{Insertion} and \kl{Deletion} on
\kl{LoIs}, which is done by capturing our intuition relating these principles to
\emph{unification} in the rules \kl{Unif{\da}} and \kl{Unif{\ua}} of
\reffig{beta}, respectively. Thus the inductive version of \kl{Beta} is obtained
by simply adding these two rules to those of \kl{Alpha} introduced in
\reffig{alpha}.

\reffig{eg-socrates-mortal-garden} gives a derivation of Aristotle's syllogism
in this system. Even though we do not apply the \kl{Deit} rule anymore compared
to the graphical proof of \reffig{eg-socrates-mortal}, we need two additional
instances of \kl{Unif{\ua}} on $z$ and $x$, that would correspond to two
instances of \kl{Del} on the associated \kl{LoI}.

\begin{remark}
  As in \kl{Alpha}, \kl{graphs} $G, H, K$ and their one-holed \kl{contexts}
  $G\hole, H\hole, K\hole$ are multisets of \kl{nodes}. But now they do not
  correspond anymore to \emph{areas} in the graphical notation, which are
  instead captured by \kl{gardens} $\gamma, \delta, \chi$. In particular, this
  entails a subtle difference from Peirce's formulation of \kl{Beta}: because
  rules apply to \kl{graphs} and not \kl{gardens}, one cannot have \kl{binders} at the
  top-level of $\SA$, they must be enclosed in at least one \kl{cut}. Thus to be able
  to reason on (the \kl{garden} version of) the \kl{graph} (\ref{eq:sitting-socrates}), we
  must first enclose it in a double-\kl{cut}, giving the \kl{graph}
  $$\bcut{\bgarden{{}}{\bcut{\bgarden{x}{\mathsf{TheSittingPerson}(x),\mathsf{Socrates}(x)}}}}$$
  While this choice might seem confusing, it makes the formulation of rules more
  uniform with that of \kl{Alpha}, and will ease the transition to the flower
  calculus in \refch{flowers}.
\end{remark}

Note that the rules \kl{Unif{\da}} and \kl{Unif{\ua}} rely on the usual notion
of capture-avoiding substitution:

\begin{definition}[Substitution]
  The \emph{capture-avoiding substitution} of a variable $y$ for a variable $x$
  in a \kl{graph} $G$, written $\subst{G}{y}{x}$, is defined by mutual
  recursion as follows:
  \begin{align*}
    \subst{p(x_1,\ldots,x_n)}{y}{x} &= p(z_1, \ldots, z_n) \text{ with } z_i = \begin{cases}
      y &\text{if $x_i = x$} \\
      x_i &\text{otherwise}
    \end{cases} \\
    \subst{g_1, \ldots, g_n}{y}{x} &= \subst{g_1}{y}{x}, \ldots, \subst{g_n}{y}{x} \\
    \subst{(\bgarden{\bz}{G})}{y}{x} &= \begin{cases}
      \bgarden{\bz}{\subst{G}{y}{x}} &\text{if $x \not\in \bz$} \\
      \bgarden{\bz}{G} &\text{otherwise}
    \end{cases} \\
    \subst{\bcut{\gamma}}{y}{x} &= \bcut{\subst{\gamma}{y}{x}}
  \end{align*}
\end{definition}

Also, this system supports free variables, and there is no need to forbid them
like Peirce did in his original \kl{LoI}-based syntax.


\end{scope}
\end{scope}
