% \setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

% \paragraph{Formal manipulations}

Proof assistants --- also called \emph{interactive theorem provers} --- are
software systems that allow to both create, and check the correctness of
mathematical proofs. They are based on the idea that mathematical knowledge can
be represented unambiguously inside \emph{proof formalisms}, where the truth of
a statement can be reduced to the mechanical application of symbolic
manipulation rules. For instance, consider the equation
$$4x + 6x = (12 - 2)x$$ 
While any mathematician would immediately recognize it as true, a middle school
student learning algebra would have to carry manually some computations to
convince herself (and her teacher) of its validity. A first step might consist
in applying the distributivity of multiplication over addition on the left-hand
side of the equation, yielding the new equation
$$(4 + 6)x = (12 - 2)x$$
Then, computing the sum on the left-hand side and the difference on the
right-hand side gives the final equation
$$10x = 10x$$
which is trivially true. This is a very simple example, but it already shows the
two main aspects of proof formalisms: on the one hand, they allow to represent
mathematical statements in a formal language, here that of equations between
linear univariate polynomials; and on the other hand, they allow to manipulate
this representation in order to prove the statements, here through term
rewriting rules that transform a valid equation into another valid equation.

Algebra lends itself particularly well to formalization, as it is arguably the
very study of the rules governing symbolic manipulations in mathematics. It also
heavily relies on computations, which explains why it was the target of the
first, and to this day most popular application of computers to mathematics:
computer algebra systems.

However in this thesis, we are interested in improving the usability of proof
assistants, which have a much broader scope than computer algebra systems: their
ambition is to enable the formalization on computers of virtually \emph{any}
kind of mathematics. Ultimately, the dream is to provide a platform that helps
humans in creating \emph{new} mathematics: both novel solutions and proofs to
existing problems, and brand new theories involving new types of mathematical
objects. This seemingly disproportionate ambition is not entirely utopic: it is
based on the great discoveries of 19\textsuperscript{th} and
20\textsuperscript{th} century mathematicians and logicians, in the broad
research area now known as \emph{mathematical logic}.

\subsection{Mathematical logic}

\paragraph{Universal language}

At the dawn of the 20\textsuperscript{th} century, some mathematicians started
to realize that it might be possible to formalize not only specific branches of
mathematics like algebra with its own specific language, but the \emph{whole} of
mathematics in a single, universal language. This idea was first intuited by
Leibniz with his dream of a \textit{characteristica universalis}, an ideal
language in which all propositions --- mathematical propositions, but also
scientific propositions about the real world, and even metaphysical propositions
--- could be expressed and understood unambiguously by every human. Also,
Leibniz introduced the concept of a \textit{calculus ratiocinator}, a systematic
method for determining the truth of any proposition expressed in the
\textit{characteristica universalis}, providing a definitive and objective way
to settle any argument through simple calculations\sidenote{Leibniz himself
might have been inspired by his predecessor Galileo, who famously declared that
``the universe [...] is written in the language of mathematics''
\cite{assayer}.}.

\paragraph{Predicate logic and set theory}

The possibility of a universal language for mathematics really became credible
at the dusk of the 19\textsuperscript{th} century, thanks to the works of
logicians like Boole \sidecite{Boole1854-BOOTLO-4}, Frege \sidecite{frege79} and
Peirce \sidecite{peirce_algebra_1885} on one hand, and mathematicians like
Cantor and Dedekind on the other hand
\sidecite{07557982-f50c-352d-bd73-3e2bc6403d4f}. The first laid the groundwork
for a formal account of deduction that greatly improved on Aristotle's
syllogistic, by inventing notations and rules that can express reasoning about
not only \emph{properties} of individuals, but also \emph{relations} between
them. The second invented \emph{set theory}, which provided the first setting
where a general notion of \emph{function} or mapping could be rigorously
defined, a notion that became increasingly central in modern mathematics.

\paragraph{Foundations}

This formed the basis for a unification of many branches of mathematics on the
same \emph{foundation}: it was realized that with enough effort, every
mathematical structure could be encoded with the sets of Cantor, and all the
laws governing sets could be expressed with a finite number of \emph{axioms}
expressed in \emph{predicate} logic, i.e. the language and calculus of relations
devised by 19\textsuperscript{th} century logicians. This crystallized into two
famous axiomatic systems for set theory: the \textit{Principia Mathematica} of
Russell and Whitehead \sidecite{russell25}; and Zermelo-Fraenkel set theory
(\sys{ZF}, or \sys{ZFC} with the axiom of choice), which is the most popular
foundation nowadays because of its greater simplicity.

\subsection{Proof theory}

\paragraph{Truth and proofs}

An axiomatic system specifies the formal language in which statements about
mathematical objects are expressed, as well as a collection of such statements
--- the axioms --- that are taken to be true from the outset, without further
justification. In fact, one does not even need to speak about \emph{truth} to
define the system: although it can be a guiding intuition when designing the
system, the fact that axioms denote true properties of abstract objects in some
``mathematical universe'' is a particular philosophical stance (platonism),
which has nothing to do with concrete reasoning on the formal representation.

Traditionally, the branch of mathematical logic that tries to model the
``semantic'' content of axioms is called \emph{model theory}. In this thesis, we
are concerned with the construction of formal proofs that derive the
consequences of axioms by pure ``syntactic'' manipulation, through the
application of so-called \emph{inference rules}. Accordingly, the branch of
mathematical logic studying this activity is called \emph{proof theory}. We will
still do a bit of model theory in a few places (\refsec{bubbles-soundness},
\refsec{Completeness}), but only as a means to justify the properties of our
syntax. Thus to avoid any unnecessary philosophical commitment, we will only
consider axioms of a given system as ordinary \emph{assumptions} that can be
used in the course of reasoning, without according any particular status with
regard to their truth. This is very much in line with the \emph{formalist}
school of thought in philosophy of mathematics, represented by the great
mathematician and main instigator of proof theory David Hilbert.

\begin{framed}
\begin{quote}
The real focus throughout this thesis will be on the \emph{inference rules} used
to build (correct) proofs from axioms/assumptions. Those form the theoretical
basis for both the \emph{interactive creation}, and the \emph{automatic
checking} of formal proofs in proof assistants.
\end{quote}
\end{framed}

\paragraph{Axiomatic systems} 

In the very beginnings of proof theory in the 1920s, under the influence of
Hilbert, the axiomatic method was predominant, and thus proof systems of this
era --- now called Hilbert systems --- featured very few inference rules. Almost
all logical reasoning principles were encoded as \emph{axiom schemas} involving
generic \emph{propositional variables}. For instance, the famous \emph{law of
excluded middle}, that expresses that every proposition is either true or false,
is expressed formally by the schema
$$A \lor \neg A$$
where $\lor$ and $\neg$ are symbols denoting the logical connectives of
\emph{disjunction} and \emph{negation}, and $A$ is a propositional variable that
can be substituted with any concrete formula built from \emph{atomic
propositions} and other logical connectives. An atomic proposition is typically
a property of a mathematical object, that does not involve any logical
connective. An example of instance of this schema would be the proposition ``$n$
is prime or $n$ is not prime'' with $n$ some natural number, which can be
written formally as
$$\mathrm{prime}(n) \lor \neg\mathrm{prime}(n)$$
Another related principle is the \emph{law of non-contradiction}, which states
that no proposition can be both true and false at the same time. It is expressed
by the schema
$$\neg (A \land \neg A)$$
where $\land$ is the symbol denoting \emph{conjunction}.

\paragraph{Inference rules}

In Hilbert systems, the only inference rule is that of \emph{modus ponens},
which is expressed formally by the following figure:
$$\R[\rsf{mp}]{A}{A \limp B}{B}$$ It can be read from top to bottom as follows:
for any propositions $A$ and $B$, if we have a proof of $A$ and a proof of $A
\limp B$, i.e. a proof that $A$ implies $B$, then we can immediately derive a
proof of $B$ by virtue of the rule, here designated by the abbreviated name
\rsf{mp}. This reading of the rule corresponds to a form of \emph{forward}
reasoning: starting from the known \emph{premises} that $A$ and $A \limp B$ are
true, it \emph{necessarily} follows that the \emph{conclusion} $B$ is true.
Conversely, one can also have a bottom-up reading of the rule: to build a proof
of any proposition $B$, one way to proceed is to come up with another
proposition $A$ such that both $A$ and $A \limp B$ are provable. This reading
corresponds to a form of \emph{backward} reasoning: we start from the conclusion
$B$ that we want to reach, also called the \emph{goal}, and try to find
\emph{subgoals} $A$ and $A \limp B$ that are provable, and hopefully simpler to
prove; then the rule guarantees that proving these subgoals is \emph{sufficient}
to ensure the truth of the original goal.

Forward reasoning is typically the way in which mathematicians write (informal)
proofs on paper, i.e. for the \emph{presentation} of their proofs to other
mathematicians. Indeed, it is more natural for humans to follow an argument by
starting from its premises, because the latter will always contain all the
information required to deduce the conclusion, the argument only serving as a
means to explicate how this information is combined. On the other hand, backward
reasoning is more natural during the \emph{construction} phase of a proof,
because the information required to reach the conclusion (e.g. the proposition
$A$ in the \rsf{mp} rule) is not yet known.

\paragraph{Natural deduction}

Axiomatic systems can be quite concise, in that many logics can be expressed in
them with a small number of axioms. In return, they produce very long and
verbose formal proofs that are hard for humans to follow, and almost impossible
to come up with in most cases. In a series of seminars started in 1926, the
Polish logician Łukasiewicz became one of the first to advocate for a more
\emph{natural} approach in proof theory, that models more closely the way
mathematicians actually reason \sidecite{Jaskowski1934-JAKOTR-4}. A few years
later, in a dissertation delivered to the faculty of mathematical sciences of
the University of Göttingen \sidecite{gentzen_untersuchungen_1935}, the German
logician Gerhard Gentzen proposed independently his famous calculus of
\emph{natural deduction}.

This formalism follows the opposite approach to Hilbert systems: it features as
few axioms as possible, favoring the use of \emph{inference rules} to model the
forms of reasoning found in mathematical practice. Those are divided in two
categories: \emph{introduction} rules \emph{define} the meaning of logical
connectives, by prescribing how to prove complex formulas from proofs of their
components. Dually, \emph{elimination rules} explain how to \emph{use} complex
formulas, by giving a canonical way to derive new conclusions from them. The
most simple example can be found in the rules for the conjunction connective
$\land$, written as follows:
\begin{mathpar}
  \R[\rsf{\land i}]
  {A}{B}
  {A \land B}
  \and
  \R[\rsf{\land e_1}]
    {A \land B}{A}
  \and
  \R[\rsf{\land e_2}]
    {A \land B}{B}
\end{mathpar}
The introduction rule \rsf{\land i} allows to build a proof of $A \land B$ by
combining a proof of $A$ and a proof of $B$, while the elimination rules
\rsf{\land e_1} and \rsf{\land e_2} allow to derive proofs of $A$ and $B$ from a
proof of $A \land B$. The complete set of natural deduction rules for other
connectives in intuitionistic logic is given in \reffig{calculi-NJ}.

\paragraph{Sequent calculus}



\paragraph{Deep inference}

The calculus of structures, as a rewriting system, is closer to the equational
reasoning that mathematicians are accustomed to in algebra. The main difference
is that most rules can only be applied in a \emph{single} direction, because the
premiss and conclusion are not \emph{equivalent}. A better analogy would be with
the substitutional rules that are sometimes used to reason about
\emph{inequalities}.

\subsection{Proof assistants}

\paragraph{Statement languages}

\begin{itemize}
  \item[\textbf{Mathematical notations}]
  \item[\textbf{Logical primitives}]
\end{itemize}

\paragraph{Proof languages}

\begin{itemize}
  \item[\textbf{Imperative}]
  \item[\textbf{Declarative}]
\end{itemize}

\paragraph{Intuitionistic type theory}

\subsection{This thesis}

\paragraph{Direct manipulation}

\paragraph{Graphical deep inference}

\paragraph{Iconic manipulations}

The dominant paradigm for proof formalisms is \emph{linguistic} in nature:
mathematical objects and assertions about them are represented in a formal
language of propositions, also called \emph{formulas}, understood as a (usually
infinite) set of strings of symbols.

Contrary to a common misconception from researchers in logic, Leibniz did not
conceive of his \textit{characteristica universalis} as a symbolic language, but
rather as an \emph{iconic} one \cite[Chpt.~3]{logique_leibniz}:
\begin{quote}
    The true ``real characteristic'' [...] would express the composition of
    concepts by the combination of signs representing their simple elements,
    such that the correspondence between composite ideas and their symbols would
    be natural and no longer conventional. [...] This shows that the real
    characteristic was for him an ideography, that is, a system of signs that
    directly represent things (or, rather, ideas) and not words.
\end{quote}
This is to be compared to Frege's \textit{Begriffsschrift}, a graphical,
two-dimensional language and calculus of ``pure thought'', whose name has
repeatedly been translated as \emph{ideography}
\sidecite{Frege1952-FRETFT}\sidecite{Frege1999-FRELUL}.

\paragraph{Contributions}

\subsection{Related works}

\paragraph{Window inference}

Other researchers have stressed the importance of being able to reason
\emph{deep} inside formulas, in order to provide intuitive proof steps. The
first and biggest line of research supporting this idea is probably that of
\emph{window inference}, which started in 1993 with the seminal article of P.J.
Robinson and J. Staples \sidecite{robinson-formalizing-1993}, and slowly became
out of fashion during the 2000s. This is well expressed in the following quote
from one of its main contributors, Serge Autexier
\sidecite[][p.~184--187]{autexier_phd}:

\begin{quote}
We believe it is an essential feature of a calculus for intuitive reasoning to
support the transformation of parts of a formula without actually being forced
to decompose the formula. In that respect the inference rules of Schütte's proof
theory are a clear contribution. [...] One motivation for the development of the
CORE proof theory was to overcome the need for formula decomposition as enforced
by sequent and natural deduction calculi in order to support an intuitive
reasoning style.
\end{quote}

Thus we are not the first to attempt to design new proof-theoretical frameworks
based on deep inference principles, with the explicit objective of supporting a
more intuitive reasoning style in interactive theorem provers\sidenote{Note that
during most of the time when window inference was developed, the terminology of
``deep inference'' had not been introduced yet. Indeed, the first article on the
subject appeared in 1999 \cite{Guglielmi1999ACO}, with very different
motivations in mind: namely, the development of a proof-theoretical approach
unifying concurrent and sequential computation, resulting in the calculus of
structures for the logic \sys{BV}. But some proof systems based on deep
inference principles already existed and inspired researchers in window
inference, as witnessed by the reference to Schütte's proof theory
\cite{schutte_proof_1977} in the above quote.}. However, we believe our approach
is unique in that it emphasizes two aspects:
\begin{itemize}
  \item the use of \emph{direct manipulation} on goals to perform proof steps
  (although some pointing interactions were already at work in window
  inference-based systems);
  \item in the second part of this thesis, the use of \emph{iconic}
  representations for the proof state, that stray away from traditional symbolic
  formulas.
\end{itemize}

\paragraph{Ayers' thesis}

More recently, Ayers described in his thesis a new tool for producing verifiable
and explainable (formal) proofs, including both theoretical discussions of novel
concepts and designs for components of proof assistants, and practical
implementations of software evaluated through user studies
\sidecite{ayers_thesis}. Notable contributions from our point of view are:
\begin{itemize}
  \item his \texttt{Box} development calculus, which introduces a unified
\texttt{Box} data structure representing at the same time goals and partial
proofs, with the aim to offer more ``human-like'' interfaces for both the
\emph{construction} and the \emph{presentation} of proofs;
  \item and his \texttt{ProofWidgets} framework, that allows to extend the Lean
proof assistant with new interactive and domain-specific notations for
mathematical objects, thus offering a form of \emph{end-user} programming.
\end{itemize}
The \texttt{Box} data structure easily lends itself to visualisation in a
two-dimensional graphical notation, while \texttt{ProofWidgets} promises great
capabilities for proofs by both direct and iconic manipulations.

However, the work of Ayers focuses mainly on designing a general framework that
can integrate modern interfaces for proofs in the Lean proof assistant, while we
focus on exploring various proof calculi that provide the foundations for such
interfaces at the purely logical level, mostly independently of any particular
proof assistant. Thus we believe that our work is quite complementary with that
of Ayers: it puts emphasis on different aspects, while sharing a common vision
for the future of proof assistants, where modern graphical interfaces play a
crucial role in improving the interaction between the user and the computer.