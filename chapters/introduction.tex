\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\epigraph{The ultimate meaning of logic is this ability to manipulate.}
{\textbf{Jean-Yves Girard}, \textit{The blind spot}, 2011}

% \paragraph{Formal manipulations}

\AP \intro{Proof assistants} (\reintro{PAs}) --- also called
\reintro{interactive theorem provers} (\intro{ITPs}) --- are software systems
that allow to both create and check the correctness of mathematical proofs. They
are based on the idea that mathematical knowledge can be represented
unambiguously inside \intro{proof formalisms} --- also called \reintro{proof
systems}, where the truth of a statement can be reduced to the mechanical
application of \kl{symbolic} manipulation rules. For instance, consider the
equation
$$4x + 6x = (12 - 2)x$$ 
While any mathematician would immediately recognize it as true, a middle school
student learning algebra would have to carry manually some computations to
convince herself (and her teacher) of its validity. A first step might consist
in applying the distributivity of multiplication over addition on the left-hand
side of the equation, yielding the new equation
$$(4 + 6)x = (12 - 2)x$$
Then, computing the sum on the left-hand side and the difference on the
right-hand side gives the final equation
$$10x = 10x$$
\AP which is trivially true. This is a very simple example, but it already shows
the two main aspects of \kl{proof formalisms}: on the one hand, they allow to
\emph{represent} mathematical statements in a formal language, here that of
equations between linear univariate polynomials; on the other hand, they allow
to \emph{manipulate} this representation in order to prove the statements, here
through \intro{rewriting rules} that transform a valid equation into another
valid equation.

Algebra lends itself particularly well to formalization, as it is arguably the
very study of the rules governing \kl{symbolic} manipulations in mathematics. It also
heavily relies on computations, which explains why it was the target of the
first, and to this day most popular application of computers to mathematics:
computer algebra systems.

However in this thesis, we are interested in improving the usability of
\kl{proof assistants}, which have a much broader scope than computer algebra
systems: their ambition is to enable the formalization on computers of virtually
\emph{any} kind of mathematics. Ultimately, the dream is to provide a platform
that helps humans in creating \emph{new} mathematics: both novel solutions and
proofs to existing problems, and brand new theories involving new types of
mathematical objects. This seemingly disproportionate ambition is not entirely
utopic: \AP it is based on the great discoveries of 19\textsuperscript{th} and
20\textsuperscript{th} century mathematicians and logicians, in the broad
research area now known as \intro{mathematical logic}.

\section{Proof theory}

\subsection{Mathematical logic}

\paragraph{Universal language}

\AP
At the dawn of the 20\textsuperscript{th} century, some mathematicians started
to realize that it may be possible to formalize not only specific branches of
mathematics like algebra with their own language, but the \emph{whole} of
mathematics in a single, universal language. This idea was first intuited in the
17\textsuperscript{th} century by Leibniz with his dream of a
\intro{characteristica universalis}, an ideal language in which all propositions
--- mathematical propositions, but also scientific propositions about the real
world, and even metaphysical propositions --- could be expressed and understood
unambiguously by every human. Also, Leibniz introduced the concept of a
\intro{calculus ratiocinator}, a systematic method for determining the truth of
any proposition expressed in the \kl{characteristica universalis}, providing a
definitive and objective way to settle any argument through simple
calculations\sidenote{Leibniz himself might have been inspired by his
predecessor Galileo, who famously declared that ``the universe [...] is written
in the language of mathematics'' \cite{assayer}.}.

\paragraph{Predicate logic and set theory}

\AP
The possibility of a universal language for mathematics became credible at the
dusk of the 19\textsuperscript{th} century, thanks to the works of logicians
like Boole, Frege and Peirce on one hand
\sidecite{Boole1854-BOOTLO-4,frege79,peirce_algebra_1885}, and mathematicians
like Cantor and Dedekind on the other hand
\sidecite{07557982-f50c-352d-bd73-3e2bc6403d4f}. The former laid the groundwork
for a formal account of deduction that greatly improved on Aristotle's
syllogistic, by inventing notations and rules that can express reasoning about
not only \emph{properties} of individuals, but also \emph{relations} between
them. The latter invented \intro{set theory}, which provided the first setting
where a general notion of \emph{function} or mapping could be rigorously
defined, a notion that became increasingly central in modern mathematics.

\paragraph{Foundations}

\AP
This formed the basis for a unification of many branches of mathematics on the
same \emph{foundation}: it was realized that with enough effort, every
mathematical structure could be encoded with the sets of Cantor, and all the
laws governing sets could be expressed with a finite number of \kl{axioms}
expressed in \intro{predicate logic}, i.e. the language and calculus of
relations devised by 19\textsuperscript{th} century logicians. This crystallized
into two famous axiomatic systems for \kl{set theory}: the \textit{Principia
Mathematica} of Russell and Whitehead \sidecite{russell25}; and Zermelo-Fraenkel
\kl{set theory} (\intro{ZF}, or \intro{ZFC} with the axiom of choice), which is
the most popular foundation nowadays because of its greater simplicity.

\paragraph{Truth and proofs}

\AP
An \intro{axiomatic system} specifies the formal language in which statements
about mathematical objects are expressed, as well as a collection of such
statements --- the \intro{axioms} --- that are taken to be true from the outset,
without further justification. One does not even need to speak about
\emph{truth} to define the system: although it can be a guiding intuition when
designing the system, the fact that \kl{axioms} denote true properties of
abstract objects in some ``mathematical universe'' is a particular philosophical
stance (platonism), which has nothing to do with concrete reasoning on the
formal representation.

\AP
Traditionally, the branch of \kl{mathematical logic} that tries to model the
``semantic'' content of \kl{axioms} through the notion of truth is called
\intro{model theory}. In this thesis, we are concerned with the construction of
formal proofs that derive the consequences of \kl{axioms} by pure ``syntactic''
manipulation, through the application of so-called \intro{inference rules}.
Accordingly, the branch of \kl{mathematical logic} studying this activity is
called \intro{proof theory}. We will still do a bit of \kl{model theory} in a few
places (\refsec{bubbles-soundness}, \refsec{Completeness}), but only as a means
to justify the properties of our syntax. Thus to avoid any unnecessary
philosophical commitment, we will only consider \kl{axioms} of a given system as
ordinary \emph{assumptions} that can be used in the course of reasoning, without
according any particular status to their truth. This is very much in line with
the \emph{formalist} school of thought in philosophy of mathematics, represented
by the great mathematician and main instigator of \kl{proof theory} David Hilbert.

\begin{emphpar}
\AP The real focus throughout this thesis is on the \emph{\kl{inference rules}}
used to build (correct) proofs from \kl{axioms}/assumptions. Those form the
theoretical basis for both the \emph{interactive creation}, and the
\emph{automatic checking} of formal proofs in \kl{proof assistants}. The branch
of \kl{proof theory} concerned with the study of \kl{inference rules} is called
\intro{structural proof theory}.
\end{emphpar}

\paragraph{Axiomatic systems} 

\AP
In the very beginnings of \kl{proof theory} in the 1920s, under the influence of
Hilbert, the axiomatic method was predominant, and thus \kl{proof systems} of
this era --- now called \reintro{Hilbert systems} --- featured very few
\kl{inference rules}. Almost all logical reasoning principles were encoded as
\emph{axiom schemas} involving generic \emph{propositional variables}. For
instance, the famous \intro{law of excluded middle} (\intro{LEM}), that states
that every proposition is either true or false, is expressed formally by the
schema
$$A \lor \neg A$$
Here, $\lor$ and $\neg$ are \kl{symbols} denoting the logical connectives of
\emph{disjunction} (``or'') and \emph{negation} (``not''), and $A$ is a
propositional variable that can be substituted with any concrete \emph{formula}
built from \emph{atomic propositions} and logical connectives. An atomic
proposition is typically a property of a mathematical object, that does not
involve any logical connective. An example of \emph{instance} of this schema
would be the proposition ``$n$ is prime or $n$ is not prime'' with $n$ some
natural number, which can be written formally as
$$\mathrm{prime}(n) \lor \neg\mathrm{prime}(n)$$
\AP Another related principle is the \intro{law of non-contradiction}, which states
that no proposition can be both true and false at the same time. It is expressed
by the schema
$$\neg (A \land \neg A)$$
where $\land$ is the \kl{symbol} denoting \emph{conjunction} (``and'').

\paragraph{Intuitionistic logic}

One motivating factor in the development of a new foundation for mathematics was
the discovery of strange theorems that defy intuition, like the existence of the
Weierstra{\ss} function which is continuous everywhere but differentiable
nowhere \sidecite{weirstrass_function}, or the Banach-Tarski paradox which
asserts that a ball can be decomposed and reassembled into two exact copies of
itself \sidecite{banach_sur_1924}. Some mathematicians like Brouwer and Weyl
rejected the truth of such theorems, on the basis that their proofs rely on
reasoning principles that are not \emph{constructive}\sidenote{This is true for
the Banach-Tarski paradox, which relies crucially on non-constructive
definitions of the concepts of partitions and equivalence classes \cite{175699}.
But for the Weierstra{\ss} function, it is possible to give it a constructive
definition with some efforts \cite{439047}.}. In particular, these principles
allow to prove the existence of objects satisfying certain properties without
ever providing a \emph{witness}, i.e. a concrete object that satisfies the
properties in question. This marked the birth of \emph{constructivism} in
philosophy of mathematics, whose most famous incarnation is Brouwer's
\emph{intuitionism}.

\AP
The original intuitionism of Brouwer was strongly opposed to any attempt at
formalizing mathematics, standing against both Frege and Russell's logicism that
saw mathematics as a mere branch of logic, and Hilbert's formalism that reduced
mathematics to a game of \kl{symbol} manipulation. However, this did not prevent
Heyting, one of Brouwer's students, from developing an axiomatic system in the
style of Hilbert and Frege, in an attempt to capture formally the objections of
Brouwer towards \intro{classical} logic --- i.e. the logic developed by
19\textsuperscript{th} century logicians that was at the heart of the new
\kl{set-theoretical} foundations. Heyting's system captures what is now called
\intro{intuitionistic} logic, which can be succinctly summarized as being
exactly \kl{classical} logic, but \emph{without} the \kl{law of excluded
middle}. Thus \kl{intuitionistic} logic is a generalization of \kl{classical}
logic, where propositions cannot be assigned a truth value \emph{a priori}: they
are only considered true if they can be proved with \emph{direct}, constructive
evidence.

To this day, there is no consensus among mathematicians as to which logic ---
\kl{intuitionistic} or \kl{classical} --- is the right one to found mathematics upon.
Since \kl{intuitionistic} logic is more restrictive than \kl{classical} logic, some
fundamental theorems of \kl{classical} mathematics do not hold anymore, requiring in
the worst cases to recreate entire branches of mathematics from scratch, like in
\emph{constructive analysis}. This explains why a large majority of
mathematicians still work in \kl{classical} logic, and are often even unaware of the
existence of constructive mathematics.

\begin{emphpar}
To account for this diversity, in this thesis we design \kl{proof systems} that
support \emph{both} \kl{classical} and \kl{intuitionistic} reasoning. Because every
theorem of \kl{intuitionistic} logic is also a theorem of \kl{classical} logic (but not
the converse), we will often focus first on the \kl{intuitionistic} ``kernel'' of our
systems, designing the \kl{classical} part as an extension of the former.
\end{emphpar}

\subsection{Structural proof theory}\labsubsec{structural-proof-theory}

\paragraph{Inference rules}

\AP
In \kl{Hilbert systems}, the only \kl{inference rule} is that of \intro{modus
ponens}, which is expressed formally with the following figure:
$$\R[\intro{mp}]{A}{A \limp B}{B}$$
Like \kl{axioms}, it is a \emph{schema} that involves generic propositional variables
$A$ and $B$, which may be \emph{instantiated} with arbitrary formulas. It can be
read from top to bottom as follows: for any propositions $A$ and $B$, if we have
a proof of $A$ and a proof of $A \limp B$, i.e. a proof that $A$ implies $B$,
then we can immediately derive a proof of $B$ by virtue of the rule, here
designated by the abbreviated name \kl{mp}. This reading of the rule
corresponds to a form of \intro{forward} reasoning: starting from the known
\emph{premises} that $A$ and $A \limp B$ are true, it \emph{necessarily} follows
that the \emph{conclusion} $B$ is true.

\AP
Conversely, one can also have a bottom-up reading of the rule: to build a proof
of any proposition $B$, one way to proceed is to come up with another
proposition $A$ such that both $A$ and $A \limp B$ are provable. This reading
corresponds to a form of \intro{backward} reasoning: we start from the conclusion
$B$ that we want to reach, also called the \intro{goal}, and try to find
\intro{subgoals} $A$ and $A \limp B$ that are provable, and hopefully simpler to
prove; then the rule guarantees that proving these \kl{subgoals} is \emph{sufficient}
to ensure the truth of the original \kl{goal}.

\kl{Forward} reasoning is typically how mathematicians write (informal) proofs on
paper, for the \emph{presentation} of their proofs to other mathematicians.
Indeed, it is more natural for humans to follow an argument by starting from its
premises, because the latter will always contain all the information required to
deduce the conclusion, the argument only serving as a means to explicate how
this information is combined. On the other hand, \kl{backward} reasoning is more
natural during the \emph{construction} phase of a proof, because the information
required to reach the conclusion (e.g. the proposition $A$ in the \kl{mp} rule)
is not yet known.

\paragraph{Natural deduction}

\AP
\kl{Axiomatic systems} can be relatively concise, in that many logics can be
expressed in them with a small number of \kl{axioms}. In return, they produce very
long and verbose formal proofs that are hard for humans to follow, and almost
impossible to come up with in most cases. In a series of seminars started in
1926, the Polish logician Łukasiewicz became one of the first to advocate for a
more \emph{natural} approach in \kl{proof theory}, that models more closely the way
mathematicians actually reason \sidecite{Jaskowski1934-JAKOTR-4}. A few years
later, in a dissertation delivered to the faculty of mathematical sciences of
the University of Göttingen \sidecite{gentzen_untersuchungen_1935}, the German
logician Gerhard Gentzen proposed independently his famous calculus of
\intro{natural deduction}.

\AP
This formalism follows the opposite approach to \kl{Hilbert systems}: it
features as few \kl{axioms} as possible, favoring the use of \emph{\kl{inference
rules}} to model the forms of reasoning found in mathematical practice. Those
are divided into two categories: \intro(natded){introduction rules}
\emph{define} the meaning of logical connectives, by prescribing how to prove
complex formulas from proofs of their components. Dually, \intro{elimination
rules} explain how to \emph{use} complex formulas, by giving a canonical way to
derive new conclusions from them. \reffig{calculi-NJ} shows the complete set of
\kl{natural deduction} rules for all connectives and quantifiers in
\kl{intuitionistic} logic, that was introduced by Gentzen under the name
\intro{NJ}\sidenote{Gentzen simultaneously introduced a \kl{natural deduction}
calculus named \intro{NK} for \kl{classical} logic, which is just \kl{NJ} with
an additional rule modelling the principle of \emph{indirect proof} --- i.e. the
possibility to prove any proposition $A$ by deriving a contradiction from its
negation $\neg A$ (\reffig{NK-ip}). Indeed, this principle can be expressed by
the formula $\neg\neg A \limp A$ which is strictly equivalent to the \kl{law of
excluded middle}, in the sense that $\neg\neg A \limp A$ is
(\kl{intuitionistically}) provable if and only if $A \lor \neg A$ is.}
\cite{gentzen_untersuchungen_1935}. The most simple example can be found in the
rules for the conjunction connective $\land$: the \kl(natded){introduction rule}
\kl{\land i} allows to build a proof of $A \land B$ by combining a proof of $A$
and a proof of $B$; while the \kl{elimination rules} \kl{\land e_1} and
\kl{\land e_2} allow to derive proofs of $A$ and $B$ from a proof of $A \land
B$.

\begin{figure*}
  \input{figures/nj.tex}
  \caption{\kl{Natural deduction} calculus \kl{NJ} for \kl{intuitionistic} logic}
  \labfig{calculi-NJ}
\end{figure*}

\begin{marginfigure}
  $$
  \prftree[r][l]{\intro{ip}}{\small[\prfref<H1>]}
  {\summ
    {\Gamma, \prfboundedassumption<H1>{\neg A}}
    {\bot}}
  {A}    
  $$
  \caption{Rule of indirect proof in \kl{natural deduction}}
  \labfig{NK-ip}
\end{marginfigure}

\begin{remark}
  Note that the rules for negation $\neg$ are not present in
  \reffig{calculi-NJ}: indeed, it is customary in \kl{intuitionistic} logic to define
  negation by $\neg A \defeq A \limp \bot$, identifying the negation of any
  proposition $A$ with its implying of a contradiction. Thus the rules for
  negation are subsumed by those for implication $\limp$ and absurdity $\bot$.
\end{remark}

\begin{marginfigure}
  $$
  \prftree[r][l]{\kl{{\limp}i}}{\small[\prfref<H1>]}
    {\R[\kl{{\limp}e}]
      {\R[\kl{\land e_1}]
        {\prfboundedassumption<H1>{A \land \neg A}}
        {A}}
      {\R[\kl{\land e_2}]
        {\prfboundedassumption<H1>{A \land \neg A}}
        {\neg A}}
      {\bot}}
    {\neg (A \land \neg A)}
  $$
  \caption{Proof of the \kl{law of non-contradiction} in \kl{natural deduction}}
  \labfig{PNC-NJ}
\end{marginfigure}

All logical reasoning principles that were \emph{axiomatized} in \kl{Hilbert
systems} can be \emph{derived} in \kl{natural deduction}. For example,
\reffig{PNC-NJ} shows a proof of the \kl{law of non-contradiction}, built
by \emph{composing} instances of rules from \reffig{calculi-NJ}. The composition
of a rule instance \rsf{r1} with another rule instance \rsf{r2} simply consists
in using the conclusion of \rsf{r1} as one of the premisses of \rsf{r2}.

\begin{figure*}
  \input{figures/lj.tex}
  \caption{\kl{Sequent calculus} \kl{LJ} for \kl{intuitionistic} logic}
  \labfig{calculi-LJ}
\end{figure*}

\paragraph{Sequent calculus}

\AP
In addition to the constructivists' objections, some doubts were raised by the
discovery of fatal flaws in early attempts at defining foundational
\kl{axiomatic systems}, the most famous one being the \emph{antinomy} of
Russell's paradox caused by the unrestricted axiom of comprehension in naive
\kl{set theory}. In order to restore absolute trust in the foundations of
(\kl{classical}) mathematics, Hilbert proposed in the early 1920s to prove
mathematically the \intro{consistency} of the \kl{axiomatic system} for arithmetic
introduced in 1889 by Peano\sidenote{A more involved \kl{axiomatic system} was
proposed one year earlier by Dedekind \cite{dedekind_nat}. Less known is that
Peirce had already published in 1881 an equivalent axiomatization of natural
numbers \cite{peirce_logic_1881}.}, i.e. that no contradiction can be derived
from Peano's axioms. Indeed, he believed that every mathematical truth could be
derived from the principles of arithmetic, thus reducing the problem of the
\kl{consistency} of mathematics to that of arithmetic. Moreover, Hilbert's program
was to be carried by \emph{finitist} means, without resorting to any reasoning
principle involving infinite collections --- which were at the heart of the
controversy started by constructivists. This was the initial impulse for
developing \kl{proof theory}, since it provided a mathematical definition of
``mathematical proofs'' as \emph{finite} sequences of \kl{symbols} satisfying
certain properties.

\AP
Gentzen's work on \kl{natural deduction} was an integral part of this program,
as an attempt to render the \emph{metamathematics} of \kl{proof theory} more
structured and elegant. However, he could not devise any argument for
\kl{consistency} in this framework. He thus set out to devise a new formalism
that would be a reformulation of \kl{natural deduction} with better mathematical
properties, such as \emph{symmetries}. This gave us \intro{sequent calculus},
which is widely regarded as the cornerstone for most developments in \kl{proof
theory} to this day. Gentzen introduced simultaneously \emph{two} \kl{sequent
calculi}: one for \kl{classical} logic called \intro{LK}, and one for
\kl{intuitionistic} logic called \intro{LJ}. Here we focus on the
\kl{intuitionistic} system \kl{LJ}, whose rules are shown in
\reffig{calculi-LJ}.

\AP
\kl{Sequent calculus} is based on the observation that some rules in \kl{natural
deduction} depend crucially on the use of \emph{hypotheses} that appear
``higher'' or earlier in the proof. The prototypical example is the
\kl(natded){introduction rule} \kl{{\limp}i} for implication: to prove $A \limp
B$, it suffices to prove $B$ under the assumption that $A$ is true. Then the
hypothesis $A$ is \emph{discharged} by the rule (bracket notation in
\reffig{calculi-NJ}), meaning that the conclusion $A \limp B$ holds
\emph{unconditionally}, without the assumption. In \kl{sequent calculus}, this
relation of provability of a conclusion $C$ under a
collection/\intro(sequent){context} of hypotheses $\Gamma$ is captured by the
expression $\Gamma \intro*\seq C$, called a \intro{sequent}. The
\kl(natded){introduction rule} \kl{{\limp}i} is then expressed by the so-called
\intro{right introduction rule} \kl{{\limp}R}, which keeps track of the full
\kl(sequent){context} of hypotheses by having \kl{sequents} as premiss and
conclusion, instead of just formulas. \kl{Right introduction rules} for other
connectives are also obtained straightforwardly from the corresponding
\kl(natded){introduction rules} in \kl{natural deduction}, by simply making the
\kl(sequent){contexts} $\Gamma$ and $\Delta$ always explicit.

\AP
Following the original presentation of Gentzen, \kl(sequent){contexts} are taken
to be \emph{lists} of formulas (``Sequenz'' in German), i.e. ordered collections
where repetitions are allowed. Still, we really want to see them as \emph{sets}
of formulas, since it is implicit in mathematical practice that:
\begin{enumerate}
\item the \emph{order} in which hypotheses are listed does not matter;
\item hypotheses may be used \emph{more than once} in a proof (as in
\reffig{PNC-NJ}).
\end{enumerate}
These two conventions are respectively captured by the \intro{structural rules}
\kl{xL} of \intro{exchange} and \kl{cL} of \intro{contraction} in
\reffig{calculi-LJ}. A third \kl{structural rule}, \kl{wL} for
\intro{weakening}, accounts for the presence of unused assumptions in some
proofs, by allowing the introduction of new hypotheses at will (with a top-down
reading of rules).

\begin{marginfigure}
  $$
  \R[\kl{{\limp}R}]
  {\R[\kl{cL}]
  {\R[\kl{\land L_1}]
  {\R[\kl{\land L_2}]
  {\R[\kl{{\limp}L}]
    {\R[\kl{ax}]
    {A \seq A}}
    {\R[\kl{ax}]
    {\bot \seq \bot}}
    {A, \neg A \seq \bot}}
  {A, A \land \neg A \seq \bot}}
  {A \land \neg A, A \land \neg A \seq \bot}}
  {A \land \neg A \seq \bot}}
  {\seq \neg (A \land \neg A)}
  $$
  \caption{Proof of the \kl{law of non-contradiction} in \kl{sequent calculus}}
  \labfig{PNC-LJ}
\end{marginfigure}

\AP The main difference between \kl{sequent calculus} and \kl{natural deduction}
lies in its splitting of \kl{elimination rules} into two parts: \intro{left
introduction rules}, and the \intro(rule){cut} rule \kl{cut}. As their name
indicates, \kl{left introduction rules} serve a purpose symmetric to \kl{right
introduction rules}: while the latter define how to introduce a connective in
the conclusion of a sequent, the former define how to introduce a connective in
one of its hypotheses. Then, the only way to \emph{use} such an hypothesis $A$
is through the \kl{cut} rule, which erases $A$ from the \kl{context} in the
conclusion of the rule, by justifying it with the proof of $A$ given as premiss.
The \kl{cut} rule can also be seen as a generalization of the \kl{modus ponens}
rule of \kl{Hilbert systems}, replacing the logical connective $\limp$ of
implication by the ``structural connective'' $\seq$ of sequents. The remaining
\intro(rule){axiom} rule \kl{ax} is in a sense dual to the \kl{cut} rule: while
the latter allows justifying a hypothesis by an identical conclusion, the
\kl{ax} rule allows justifying a conclusion by an identical
hypothesis\sidenote{It is not clear where the terminology ``axiom rule'' comes
from. It might be because hypotheses can be considered as \kl{axioms} in the
sense we introduced earlier, i.e. statements that we \emph{assume} to be true.
In any case, it would be a misattribution of category to consider the \kl{ax}
rule itself as an \kl{axiom}, and some authors like Girard call it the
\intro(sequent){identity rule} or \rsf{id} rule to avoid the confusion --- which
also coincides with the concept of \emph{identity morphism} in \kl{category
theory}.}. \reffig{PNC-LJ} shows a proof of the \kl{law of non-contradiction} in
\kl{LJ}.

% Both the \kl{cut} and \kl{ax} rules seem completely trivial. Yet surprisingly,
Gentzen managed to prove a powerful result called alternatively
\textit{Haupstatz}, \emph{fundamental theorem} (of \kl{proof theory}), or
\intro{cut-elimination}: every provable formula in \kl{sequent calculus} has a
\emph{cut-free} proof, i.e. a proof that does not make use of the \kl{cut} rule.
Intuitively, it can be understood as a formal justification for the possibility
to \emph{inline} proofs of lemmas, by seeing an instance of \kl{cut} on $A$ as a
way to invoke the lemma $A$ without duplicating its proof. Moreover, Gentzen's
proof of the Haupstatz is itself constructive: it describes an \emph{algorithm}
for transforming every \kl{sequent calculus} proof into a cut-free one. Thus the
\kl{cut} rule is said to be \intro{admissible}, in the sense that any provable
\kl{sequent} can be proved without it.

\AP An important consequence of \kl{cut-elimination}, which was the original
motivation of Gentzen, is the consistency of the logic (\kl{intuitionistic}
\kl{predicate logic} in the case of \kl{LJ}). This stems from the fact that all
rules apart from the \kl{cut} rule satisfy the \intro{subformula property}:
every formula $A$ appearing in the premisses is a \emph{subformula} of some
formula $B$ in the conclusion, i.e. $A$ already occurs inside $B$. Thus there
cannot exist a proof of the absurd \kl{sequent} $\seq \bot$, since the only formula
that is a subformula of $\bot$ is $\bot$ itself, and there is no rule instance
with $\seq \bot$ as conclusion that only contains $\bot$ in its premisses. The
\kl{subformula property} is the first occurrence of the concept of
\intro{analyticity} in \kl{proof theory}, and can be seen as a technical
realization of the philosophical notion of analyticity first applied to
propositions by Kant, and later to proofs in mathematics by Bolzano
\sidecite{bolzano}.

Unfortunately, the proof of \kl{cut-elimination} for the \kl{sequent calculus}
incorporating Peano's axioms, found by Gentzen a few years after proving
\kl{cut-elimination} for \kl{LJ} \sidecite{gentzen_widerspruchsfreiheit_1936}, is
not finitist: it makes use of a transfinite induction up to the ordinal
$\epsilon_0$. But the very ideas of \kl{cut-elimination} and \kl{analyticity} will have
far-reaching applications in \kl{proof theory} and beyond, including many of the
results presented in this thesis.

\paragraph{Deep inference}

\AP Many years after Gentzen's seminal work, at the advent of the
21\textsuperscript{th} century, Alessio Guglielmi introduced a new methodology
for designing \kl{proof formalisms} called \intro{deep inference}
\sidecite{Guglielmi1999ACO}. The idea was to overcome some limitations of
Gentzen formalisms while preserving their good properties, by allowing
\kl{inference rules} to be applied \emph{anywhere} inside formulas, instead of
only at the top-level of sequents\sidenote{In fact, Schütte had already proposed
a \kl{deep inference} system as early as 1977 \cite{schutte_proof_1977}, as did
Peirce one century before him with his \emph{\kl{entitative graphs}} (as we will
see in \refch{eg}). But the idea did not generate much interest at the times.}.
The first \kl{deep inference} system was the \intro{calculus of structures}
(\intro{CoS}), which can be succinctly described as a \intro{rewriting system}
on formulas. For instance, the following \intro{switch rule}, when read
bottom-up (i.e. in \kl{backward} mode), indicates that the formula $A \lor (B
\land C)$ may be rewritten into $(A \lor B) \land C$:
$$\R[\kl{s}]{S\select{(A \lor B) \land C}}{S\select{A \lor (B \land C)}}$$
\AP Importantly, the rule can be applied in any \emph{context} $S\hole$. A
\intro{context} $S\hole$ is simply a formula containing a single occurrence of a
special subformula $\hole$ called its \intro{hole}, which can be \emph{filled}
(i.e. substituted) with any formula $A$ to give a new formula $S\select{A}$.
This notion of \kl{context} serves two purposes:
\begin{itemize}
  \item it formalizes the ability of \kl{rewriting rules} to be applied at an
  arbitrary \emph{depth} inside expressions, while retaining all the information
  available in the surrounding \kl{context};
  \item it generalizes the \kl(sequent){contexts} $\Gamma, \Delta$ of
  \kl{sequent calculus}, by giving them the full structure of formulas instead
  of just flat lists of formulas. Indeed, a \kl{sequent} $\Gamma \seq C$ can be
  interpreted as the formula $\bigwedge \Gamma \limp C$, where $\bigwedge
  \Gamma$ denotes the conjunction of all the formulas in $\Gamma$.
\end{itemize}
Then, a proof of a formula $A$ in the \kl{calculus of structures} is not a
\emph{tree} of rule instances as in \kl{natural deduction} and \kl{sequent
calculus}, but a \emph{sequence} of rewritings $A \steps{} \top$ that reduces
$A$ to the trivially true \kl{goal} $\top$.

\AP
The \kl{calculus of structures}, as a \kl{rewriting system}, is closer to the
equational reasoning that mathematicians are accustomed to in algebra. The main
difference is that most rules (including the \kl{switch rule} \kl{s}) can only
be applied in a \emph{single} direction, because the premiss and conclusion are
not \emph{equivalent}\sidenote{In the standard system \kl{SKS} from where the
\kl{s} rule originates, every context is \emph{\kl(sfl){positive}}, in the sense that the
hole $\hole$ never occurs under a negation $\neg$ or left-hand side of an
implication $\limp$; otherwise the rule would need to be restricted to \kl{positive}
contexts to stay valid. See \refsec{eg-completeness} for more details on
\kl{SKS}.}. When the premiss and conclusion of a rule are equivalent, we say
that the rule is \intro{invertible}.

\begin{emphpar}
\emph{All} the \kl{proof formalisms} designed in this thesis are \kl{deep inference}
\kl{rewriting systems} in the style of the \kl{calculus of structures}.
\end{emphpar}

\section{Proof assistants}

\kl{Proof assistants} are a direct application of \kl{proof theory}, exploiting the
ability of programming languages to represent and manipulate arbitrary data
structures to give a concrete implementation of \kl{proof formalisms}. Crucially,
they open the possibility to \emph{automate} the construction and verification
of formal proofs, by acting on two fronts:
\begin{itemize}
  \item on the \textbf{human} side, the design of \emph{high-level interfaces}
  for representing and manipulating statements and proofs can bridge the gap
  between the low-level and very detailed proofs of formal logic, and the
  informal proofs of mathematicians;
  \item on the \textbf{machine} side, the design of \emph{algorithms} that both
  find proofs of given statements and ensure their correctness can --- to some
  extent\sidenote{For instance, it is well-known that the problem of provability
  in \kl{predicate logic} is \emph{undecidable}.} --- relieve mathematicians from the
  burdens of proof-writing and proof-checking.
\end{itemize}
Thus the advent of computers gave a new purpose to \kl{proof theory}, going beyond
its foundational role with the hope to support and change the everyday practice
of mathematicians.

\begin{emphpar}
In this thesis, we are concerned mostly with the \emph{human} side of the
equation: we aim to provide smoother means for the user to communicate her
intent to the \kl{proof assistant}, and conversely for the \kl{proof assistant}
to communicate its results and suggestions on how to solve problems.
\end{emphpar}

\subsection{Logical frameworks}

\paragraph{Type theory}

\AP
The ancestor of all \kl{proof assistants} was the \intro{Automath} project,
initiated by Nicolaas Govert de Bruijn as soon as 1967. Citing Geuvers
\sidecite{geuvers_proof_2009}:
\begin{quote}
[One] aim of the project was to develop a mathematical language in which all of
mathematics can be expressed accurately, in the sense that linguistic
correctness implies mathematical correctness. This language should be computer
checkable and it should be helpful in improving the reliability of mathematical
results.
\end{quote}
\AP
Thus the design of \kl{Automath} was focused on the automatic
\emph{verification} of proofs through \emph{linguistic} means. It introduced
many fundamental ideas that are still at work in many modern \kl{proof
assistants}, the most prominent being the use of a \intro{type theory} to encode
formal proofs.

\AP Contrary to \kl{predicate logic} in traditional \kl{proof theory}, \kl{type
theories} break the syntactic hierarchy imposed upon mathematical objects,
propositions and proofs, by giving them a uniform representation as so-called
\intro{terms} that can be assigned a \intro{type}. The assertion that a
\kl{term} $t$ has \kl{type} $T$ is usually written with the expression $t : T$,
which has come to be called a typing \intro{judgment} after Martin-Löf. For
instance, the \kl{judgment} $3 : \nats$ states that the \kl{term} $3$ has the
\kl{type} $\nats$ of natural numbers, and $1 + 1 = 2 : \Prop$ states that the
\kl{term} $1 + 1 = 2$ has the \kl{type} $\Prop$ of propositions.

\paragraph{First-order vs. higher-order}

\AP Almost all \kl{type theories} are \intro{higher-order}: they give a
first-class status to functions and predicates, by allowing them to take other
functions and predicates as arguments. This is because they are based on the
\intro{$\lambda$-calculus} of Alonzo Church
\sidecite{15897363-af72-3dac-82e6-fde144ad66c0}, an intensional theory of
\kl{higher-order} functions that is now considered to be the first functional
programming language in history. For instance in \emph{simply-typed}
\kl{$\lambda$-calculus} \sidecite{e4d9a073-5bba-3a5b-90d3-81832cd433e5}, one may
\kl{type} the \emph{sum operator} over sequences of natural numbers with the
\kl{judgment} $\lambda u. \lambda n. \sum_{i = 1}^{n}{u~i} : (\nats \to \nats) \to
\nats \to \nats$, where $\lambda u. \lambda n. \sum_{i = 1}^{n}{u~i}$ is a
\intro{$\lambda$-term} encoding the \kl{higher-order} function that takes a
sequence represented as a function $u : \nats \to \nats$ and a bound $n :
\nats$, and returns the sum of each of $u$'s values at index $1 \leq i \leq n$
encoded as the function application $u~i$.

\AP By contrast, the \kl{predicate logic} developed in the
19\textsuperscript{th} century and studied in traditional \kl{proof theory} is
\intro{first-order}: functions and predicates can only take so-called
\emph{first-order individuals} as arguments, which usually model
``non-functional'' mathematical objects like numbers and sets.

\begin{emphpar}
In this thesis, we exclusively study \kl{proof formalisms} for
\emph{\kl{first-order}} \kl{predicate logic} (``\intro{FOL}'' hereafter).
\end{emphpar}

We identified a few reasons for working in \kl{FOL}:
\begin{itemize}
  \item it is a standard and well-understood setting that has received a lot of
  attention, allowing us to exploit various existing works from the \kl{structural
  proof theory} literature, and even from some overlooked theories of
  19\textsuperscript{th} century logicians;
  \item by contrast, \kl{type theory} is a quite recent subject\sidenote{Almost 100
  years younger than \kl{FOL} if we ignore Russell's theory of types (1902), that was
  based on a \kl{set theory} encoded in \kl{FOL}.}, which explains why there is still a
  great diversity of \kl{type theories} that differ in subtle and often incompatible
  ways;
  \item \kl{FOL} is a common kernel of virtually every \kl{type theory}, making our work
  directly applicable to all present and future \kl{proof assistants};
  \item it is also a simpler setting, that is powerful enough to study the
  essential features of logical reasoning, without the idiosyncracies of \kl{type
  theories} aimed at capturing the full complexity of mathematics.
\end{itemize}

\paragraph{Curry-Howard correspondence}

\AP
De Bruijn came up with the revolutionary idea that propositions could themselves
be seen as \kl{types}, by having \kl{judgments} such as $~t : 1 + 1 = 2~$ where $t$
is a \intro{proof term} representing a proof of the proposition $1 + 1 = 2$.
This \intro{propositions-as-types} principle was rediscovered independently by
Howard in 1978 \sidecite{Howard1980-HOWTFN-2}, and developed further into a
\intro{proofs-as-programs} correspondence --- also called \reintro{Curry-Howard
correspondence} or \reintro{Curry-Howard isomorphism}\sidenote{In fact, Curry
had already noticed in 1958 a similar connection between the \kl{types} of
combinators in his \emph{combinatory logic}, and the \kl{axioms} of \kl{Hilbert
systems} for implication \cite{Curry1959-CURCLV} --- hence the mention of
Curry.} --- where \kl{$\lambda$-terms} in the simply-typed
\kl{$\lambda$-calculus} are put in one-to-one correspondence with proofs in the
implicational fragment of the \kl{natural deduction} system \kl{NJ} of Gentzen.

\AP Thus the core of \kl{type theory} is \emph{\kl{intuitionistic}} in nature,
and the \kl{Curry-Howard correspondence} has fostered many fruitful interactions
between computer science, logic and constructive mathematics, with \kl{proof
assistants} acting as a crucial tool and source of investigations. One
influential development in this direction has been the \intro{intuitionistic
type theory} of Martin-Löf (\intro{MLTT}), which formed the basis for the
implementation of many \kl{proof assistants} like \intro{NuPrl}, \intro{Alf},
and most recently \intro{Agda} \cite{geuvers_proof_2009}. A system closely
related to \kl{MLTT}, the \intro{calculus of inductive constructions}
(\intro{CoIC}), is also implemented in two leading \kl{proof assistants}:
\intro{Coq} \sidecite{the_coq_development_team_2022_7313584} and \intro{Lean}
\sidecite{10.1007/978-3-030-79876-5_37}. Following the \kl{proofs-as-programs}
correspondence, these systems support the creation of both proofs and programs
that manipulate and compute mathematical objects, by compiling everything down
to \kl{typed} \kl{terms}.

\subsection{Interfaces}

\paragraph{Elaboration}

\AP
\kl{Type theories} are the logical foundation for the \intro(PA){kernel} of
\kl{proof assistants}, i.e. the part of the system that is responsible for
checking formal proofs expressed in a \emph{terse}, \emph{machine-oriented}
format. But it quickly became clear that this was not enough to make \kl{proof
assistants} a viable alternative to paper proofs: de Bruijn estimates that it
takes a time factor of 20 to translate a paper proof into a formalized proof in
\kl{Automath} \sidecite{7a66996d195e4eba8c3fe5dc5bab8fc5}. This factor has been
estimated to be shrinkable to 4 in the \intro{Mizar} \kl{proof assistant}
\sidecite{debruijn_factor}, thanks to the design of high-level \emph{languages}
for representing mathematical statements and proofs, that sit on top of the core
logical theory\sidenote{In the case of \kl{Mizar}, a typed \kl{set theory} based
on \kl{FOL}.}. The process of compiling a high-level proof text into a low-level
\kl{proof term} is called \intro{elaboration}.

\paragraph{Statement languages}

Any \kl{proof assistant} must provide the two following features in its statement
language:

\begin{description}
  \item[Logical primitives] Naturally, one needs a way to write
  propositions formed with logical connectives and quantifiers. This is usually
  done in \kl{symbolic} form, either with a custom ASCII notation, or with
  Unicode characters corresponding to the standard \kl{symbols} in more modern
  \kl{proof assistants}. 
  \item[Mathematical notations] In addition to the logical primitives,
  one needs to be able to express mathematical objects and operations in the
  domain of interest, e.g. numbers and arithmetic operators. Contrary to the
  logical language that can be hardcoded once and for all in the \kl{proof
  assistant}, the mathematical language needs to be \emph{extensible} by the
  user, so that custom notations can be defined for new mathematical objects.
\end{description}

\begin{emphpar}
  In this thesis, we focus exclusively on the \emph{logical primitives}, because
  they are found universally in all types of mathematical reasoning. 
\end{emphpar}

\AP
We leave aside the question of providing domain-specific languages for
particular branches of mathematics, which is nonetheless as much important. It
has been tackled extensively in Ayers' thesis \sidecite{ayers_thesis}, and more
specifically in his framework \intro{ProofWidgets} for user-extensible,
interactive graphical notations in the \kl{Lean} \kl{proof
assistant}\sidenote{For a similar approach to \kl{ProofWidgets} in the context
of functional programming, see \cite{omar-filling-2021}.}
\sidecite{ayers_graphical_2021}. De Moura and Ullrich have also designed a
powerful macro system for \kl{Lean} 4, that supports the \kl{elaboration} of
abstract notations into \kl{terms} of the underlying \kl{type theory}
\sidecite{ullrich_beyond_2022}.

\paragraph{Proof languages}

Once one disposes of a convenient way to state mathematical propositions, comes
the question of how to efficiently write \emph{proofs} of these propositions.
There have been broadly two approaches in the design of high-level proof
languages:

\begin{description}[labelsep=0pt]
  \itemAP[\intro{Imperative}~]proof languages, like imperative programming languages,
  offer a set of \emph{commands} or instructions than can be given to the
  computer to modify some state stored in memory. The latter is called the
  \intro{proof state}, and corresponds to the \emph{partial} proof that is built
  by the system incrementally through the execution of commands. In the dominant
  paradigm, these commands are provided by the user in text form; since Robin
  Milner and the LCF theorem prover~\sidecite{doi:10.1098/rsta.1984.0067}, they
  are called \intro{tactics}. Proof files are literally \intro{proof scripts},
  that is the sequence of \kl{tactics} typed in by the user.
  
  \AP
  Contrary to imperative programming languages, the main execution paradigm for
  \kl{proof scripts} is \emph{interactive}: the user triggers commands one at a time,
  so that she can visualize the intermediate \kl{proof states} and determine the next
  steps to take. In most \kl{tactics}-based \kl{ITPs},
  % \sidenote{One notable exception is the \kl{Agda} proof assistant, where the proof
  % state is assimilated with the whole file being edited, which is itself a
  % partial \emph{program} in a dependently-typed programming language,
  % following the Curry-Howard correspondence.}
  only the \emph{statement} part of the \kl{proof state} is shown, in a so-called
  \intro{proof view} or \reintro{goal view}. This corresponds to the \kl{goals} that the
  user needs to prove, and each \kl{goal} is presented in the form of a \kl{sequent} $\Gamma \seq C$, where $C$ is the conclusion that must be reached under the
  assumptions in $\Gamma$. \kl{Tactics} generally apply to one \kl{goal} at a time, and
  the user can choose which \kl{goal} to \emph{focus} at any point during the
  interaction. When the set of \kl{goals} becomes empty, we say that the initial goal
  or conjecture has been \emph{solved}.

  \begin{remark}\labremark{tactics-rules}
    The transformations performed by \kl{tactics} can be more or less
  sophisticated. But, fundamentally, one finds elementary commands that
  correspond roughly to the \kl{inference rules}, generally of \kl{natural
  deduction} or \kl{sequent calculus}. For instance, a \kl{goal} $\Gamma\seq
  A\lor B$ (resp. $\Gamma\seq A\land B$) can be turned into either a \kl{goal}
  $\Gamma\seq A$ or a \kl{goal} $\Gamma\seq B$ (resp. into two \kl{goals}
  $\Gamma\seq A$ and $\Gamma\seq B$), corresponding to the rules \kl{\lor R_1}
  and \kl{\lor R_2} (resp. \kl{\land R}) of \kl{LJ} in their \kl{backward}
  reading (\reffig{calculi-LJ}).
  \end{remark}

  \kl{Coq} and \kl{Lean} are examples of state-of-the-art \kl{proof assistants} in the
  \kl{imperative} paradigm.
  
  \itemAP[\intro{Declarative}~]proof languages follow a different approach, by
  aiming to provide an \emph{explicit}, \emph{self-contained} description of the
  proof. Although a \kl{goal view} is still available to guide the user during the
  proof construction process, the finished proof must be readable
  \emph{statically} by a human, without relying on the \kl{ITP} to compute and
  display intermediate \kl{proof states}. Consequently, \kl{declarative} proofs are more
  verbose and take longer to type than their \kl{imperative} homologues. But since
  they contain more information, they have the advantage of being more
  \emph{robust} to slight changes to definitions or to the statement of the
  theorem being proved, and are generally easier to \emph{debug}. They can also
  be put in correspondence with some \kl{proof-theoretical} formalisms, usually
  Fitch-style \kl{natural deduction} \cite{geuvers_proof_2009}.
  
  \AP
  \kl{Agda}, \kl{Mizar} and \intro{Isabelle} (with its \intro{Isar} proof
  language \sidecite{isar}) are examples of state-of-the-art \kl{proof
  assistants} in the \kl{declarative} paradigm.
\end{description}

\begin{emphpar}
  In this thesis, we focus mostly on exploring new modalities of interaction in
  the \emph{\kl{imperative}} paradigm. Only in \refsubsec{flowers-unified} do we
  sketch a possible escape from the \kl{imperative}/\kl{declarative} dichotomy.
\end{emphpar}

\begin{remark}
In some rare cases, an additional \emph{natural language} layer is added on top
of the proof language, to be as close as possible to informal proofs. With the
recent development of large language models like GPT, such natural language
translations are becoming increasingly convincing (see e.g. Patrick Massot's
work in \kl{Lean} \cite{LeanIPAM}).
\end{remark}


\section{This thesis}

\subsection{Research goals}

% \paragraph{Interoperability}

% Existing \kl{proof assistants} can (almost) never \emph{interoperate}: a proof or
% theorem written in one system cannot be imported into a different system. This
% is unsatisfactory for many reasons, including fragmentation in the formal
% methods community, and duplication of effort instead of reuse. There have been
% many attempts in recent years to address this issue in the \emph{backend}:
% various pairs of systems are made to exchange formal results through
% translations between their input (high-level) and output (low-level) languages,
% or in some cases by translations to and from more universal languages (see e.g.
% \sidecite{proofcert,dedukti}).

\paragraph{Universal user interface}

% The interoperability problem can also potentially be approached from the
% \emph{front-end}.
Many kinds of logical manipulations such as discharging assumptions,
instantiating quantifiers, and composing lemmas, are conceptually universal;
yet, a user wishing to carry out such manipulations in a particular \kl{proof
assistant} must express the wish in terms of the specific proof language of the
system. A \emph{universal user interface} would instead allow performing such
manipulations directly on the \emph{\kl{goal}} itself, using physical,
reversible, and incremental \intro{actions} with immediate feedback. A sequence
of user \kl{actions} should then be representable in terms of any particular proof
language.

\paragraph{Direct manipulation}

\AP
This approach to user interfaces has been termed \intro{direct manipulation} by
Shneiderman in the 1980s \sidecite{shneiderman_direct_1983}. It is now at the
heart of virtually every modern user interface and is arguably the major factor
in the \emph{personal computing revolution}. Indeed, it has opened the use of
computing devices to a much wider audience, by allowing users to interact with
them in an intuitive way that resembles interactions with physical objects in
the real world. According to Shneiderman, this contrasts with the
\emph{command-line} paradigm, where users need to memorize a complex command
language that often varies from one software to another within the \emph{same}
domain. This induces an unnecessary cognitive burden for newcomers, especially
those unfamiliar with textual interfaces, which constitute the majority of users
of computing devices nowadays; to the point that the hurdle is too great to
overcome for most potential users.

Unfortunately, the user interfaces of state-of-the-art \kl{proof assistants} are
largely stuck in the pre-80s era of command-line interfaces, making them
reserved to an audience of highly motivated, computer-savvy individuals. One
could argue that like programming languages, this is due to their inherent
\emph{abstraction} capabilities, that can only be captured through the
\kl{symbolic} power of language; hence that this state of fact is unavoidable,
and can only be solved through the addition (or improvement) of computer science
curricula in primary and secondary education. We do not agree with this
conception: we believe that the current state of user interfaces in \kl{ITPs} is
one of the major obstacles to their wider adoption in the mathematical
community, both by professional researchers, teachers, and novice students
alike.

\begin{emphpar}
  Our first main working hypothesis is that the \kl{direct manipulation}
  paradigm is not only possible, but also \emph{crucial} for building formal
  proofs in \kl{ITPs}, if they are to become a viable alternative to paper
  proofs in mathematics.
\end{emphpar}

In fact, following the \kl{proofs-as-programs} correspondence, we also believe
that this applies (to some extent) to \emph{programming}, and that it is only a
matter of time before direct manipulation becomes viable for building
(general-purpose) programs, in the spirit of \emph{visual programming
languages}. We do not explore this direction in this thesis, but it is one of
our hopes that some of the techniques we develop will apply to programming as
well; and one of the reasons why we chose to focus so much on
\emph{intuitionistic} logic.

\paragraph{Graphical deep inference}

\AP
A first attempt to design \kl{direct manipulation} principles for interactive
proof building was made in the 90s by the team of Gilles Kahn at Inria, where
they coined the \intro{Proof-by-Pointing} (\intro{PbP}) paradigm
\sidecite{PbP}. The idea was to synthesize complex \kl{tactics} from the simple
act of \emph{pointing} at specific locations inside expressions occurring in the
\kl{goal}, typically with a mouse cursor. More recently
\sidecite{Chaudhuri2013}, Kaustuv Chaudhuri proposed a variation on this idea
called \intro{subformula linking} (\intro{SFL}) or \intro{Proof-by-Linking}
(\intro{PbL}), where instead of selecting expressions in isolation, one can
\emph{link} two of them together to make them interact.

In both cases, the expressions considered were logical formulas and the
associated actions chains of inferences in \kl{FOL}. Importantly, both paradigms rely
on the use of \emph{\kl{deep inference}}, since the user can point at subformulas
that occur at an arbitrary depth inside the \kl{goal}. This is in contrast with the
basic commands found in the proof languages of \kl{ITPs}, where the user can only
designate formulas appearing at the top-level of sequents\sidenote{A notable
exception is the \texttt{rewrite} \kl{tactic} of \kl{imperative} proof languages, where
the user can specify \emph{patterns} to designate particular occurrences of a
term $t$ that are to be rewritten into an equal term $u$, usually thanks to an
assumed equation $t = u$. But coming up with such patterns is a lot slower than
directly \emph{pointing} at the locations of interest onscreen.}. While the
semantics of \kl{PbP} \kl{actions} is still based on the \emph{shallow} \kl{inference rules} of
\kl{sequent calculus}, \kl{PbL} fully embraces the \kl{deep inference} paradigm by relying on
\kl{CoS}-style \kl{rewriting rules}.

\begin{emphpar}
  \AP All the works presented in this thesis can be seen as a direct
  continuation of the research programme initiated by \kl{PbP} and \kl{PbL}. The
  aim is to \emph{replace} textual proof languages with \emph{gestural
  \kl{actions}} performed directly upon \emph{\kl{goals}} in a \intro{graphical
  user interface} (\intro{GUI}). To be as general as possible, we call such a
  paradigm \intro{Proof-by-Action} (\intro{PbA}).
\end{emphpar}

\paragraph{Proof exploration}

Note that we believe such a replacement to be useful mostly during the
\emph{construction} or \emph{writing} phase of proofs. Quoting Shneiderman
\cite{shneiderman_direct_1983}:
\begin{quote}
  The pleasure in using these systems stems from the capacity to manipulate the
object of interest directly and to generate multiple alternatives rapidly.
\end{quote}
Thus in the context of \kl{ITPs}, the major advantage of a (well-designed) \kl{GUI} in the
\kl{PbA} paradigm would be to enable the \emph{rapid exploration} of multiple paths
towards the construction of a complete proof. But once a proof has been found,
the \emph{dynamic} sequence of \kl{actions} that led to it could be ``compiled'' into
a \emph{static}, textual representation of the proof in the favorite proof
language of the user, to facilitate the \emph{reading} phase.

As for the \emph{modification} phase of proofs, the \kl{PbA} paradigm requires a
way to navigate and edit directly a recorded sequence of \kl{actions}, and possibly a
mechanism for mapping parts of a static proof object to the \kl{actions} that
generated them.

\begin{emphpar}
  In this thesis, we leave the question of \kl{proof evolution} in \kl{PbA} ---
  i.e. the design of interfaces for the reading and modification phases, that
  support a smooth interaction with the writing phase --- for future work. A
  more detailed discussion can be found in \refsubsec{proof-evolution}.
\end{emphpar}

\paragraph{Iconicity}

% The dominant paradigm for proof formalisms is \emph{linguistic} in nature:
% mathematical objects and assertions about them are represented in a formal
% language of propositions, also called \emph{formulas}, understood as a (usually
% infinite) set of strings of symbols.

Contrary to a common misconception among logicians, Leibniz did not conceive of
his \kl{characteristica universalis} as a \kl{symbolic} language, but rather as an
\emph{\kl{iconic}} one \sidecite[][Chpt.~3]{logique_leibniz}:
\begin{quote}
    The true ``real characteristic'' [...] would express the composition of
    concepts by the combination of signs representing their simple elements,
    such that the correspondence between composite ideas and their symbols would
    be natural and no longer conventional. [...] This shows that the real
    characteristic was for him an ideography, that is, a system of signs that
    directly represent things (or, rather, ideas) and not words.
\end{quote}
This is to be compared to Frege's \textit{Begriffsschrift}, a graphical,
two dimensional language and calculus of ``pure thought'', whose name has
repeatedly been translated as \emph{ideography}
\sidecite{Frege1952-FRETFT,Frege1999-FRELUL}.

\AP
Following the seminal work of Charles Sanders Peirce in
\intro{semiotics}\sidenote{\kl{Semiotics} is the systematic study of sign
processes and the communication of meaning.}, we define an \intro{iconic}
language as one whose signs are mainly \reintro{icons}, i.e. signs that
\emph{resemble} or share qualities with the objects they denote. This is to be
contrasted with \intro{symbolic} languages where most signs are just
\reintro{symbols}, i.e. signs that \emph{conventionally} denote their objects.
In his systematic usage of \emph{triads} of concepts, Peirce identified a third
kind of sign, \emph{indexes} \sidecite{atkin_peirces_2023}:
\begin{quote}
  if the constraints of successful signification require that the sign reflect
  qualitative features of the object, then the sign is an icon. If the
  constraints of successful signification require that the sign utilize some
  existential or physical connection between it and its object, then the sign is
  an index. And finally, if successful signification of the object requires that
  the sign utilize some convention, habit, or social rule or law that connects
  it with its object, then the sign is a symbol.
\end{quote}
\AP He even went further by analyzing \kl{icons} into another
trichotomy\sidenote{Actually he only applied this trichotomy to pure \kl{icons}
devoid of any indexical elements, that he called \emph{hypo-icons}.}:
\emph{images} that depend on simple quality; \intro{diagrams}, who share
\emph{structural} relations among their constituents that are analogous to that
of their object; and \intro{metaphors}, that denote features of their object by
relating them to features of another object \sidecite{legg_problem_2008}.

Interestingly, Peirce held that mathematics relies mostly on
\emph{\kl{diagrammatic}} thinking --- observation of, and experimentation on,
\kl{diagrams} \sidecite[][Chpt.~6]{Hookway1985-HOOP-2}. This agrees with the
contemporary practice of mathematics: indeed, there is an increasing number of
areas in mathematics --- the most prominent one being \intro{category theory}
--- where the heart of a proof lies in the dynamical construction of a
\emph{\kl{diagram}} capturing the structure of interest in given mathematical
objects. The natural language proof text is often just a means to explicit the
meaning or intuition behind the \kl{diagrammatic} manipulations, or simply a
retranscription of the commentary that the mathematician would give when
unfolding the construction on a blackboard.

\AP
If one views logic as one particular type of mathematical reasoning --- albeit
one that is omnipresent in all branches of mathematics, then it is only natural
to expect that some \kl{diagrammatic} system should exist for it, that can
express in the most natural way most (if not all) logical arguments. This is the
\emph{\kl{iconicity}} thesis of Peirce, which motivated his inquiry into what is
arguably the first \kl{diagrammatic} \kl{proof system} in history: the
\intro{existential graphs} (\intro{EGs}). This will be the subject of
\refch{eg}, and the basis for the development of a \emph{\kl{metaphorical}}
\kl{proof system} in \refch{flowers}.

\begin{emphpar}
  Our second main working hypothesis exploited in the second part of this
  thesis, is that \kl{iconic} representations of logical \emph{statements}, and the
  proofs that result from their manipulation, can play a crucial role in the
  design of intuitive proof building interfaces.
\end{emphpar}

\subsection{Contributions}

This thesis proposes several contributions toward the research goals highlighted
above.

\paragraph{Symbolic manipulations}
  
In the first part of this thesis, we substantiate the \kl{PbA} paradigm in the
context of traditional representations of \kl{goals}, by presenting a number of
techniques based on the \kl{direct manipulation} of \emph{\kl{symbolic}} formulas in
sequents.

\AP We start in \refch{pba} with an introduction to \kl{PbP} and \kl{PbL}, by
describing how to reason with logical connectives, quantifiers and equality
through \emph{click} and \intro{drag-and-drop} (\intro{DnD}) \kl{actions} in a
prototype of \kl{GUI} called \intro{Actema}. In particular, \kl{DnD}
\kl{actions} can be seen as a generalization of both the \texttt{apply} and
\texttt{rewrite} \kl{tactics} of \kl{imperative} proof languages.

In \refch{sfl}, we ground the semantics of \kl{DnD} \kl{actions} in \kl{deep
inference} \kl{proof theory}, by designing an \kl{intuitionistic} variant of the
\kl{CoS} for \kl{subformula linking} introduced by Chaudhuri in \cite{Chaudhuri2013}.
Our approach differs from Chaudhuri's mainly through our notion of
\emph{\kl(sfl){valid} \kl{linkage}}, which filters out unproductive \kl{DnD}
\kl{actions} by restricting them to \emph{\kl{unifiable}} subformulas.

In \refch{advanced}, we present more advanced techniques in the \kl{PbA}
paradigm, that handle pervasive forms of reasoning in mathematical practice such
as the use of \emph{definitions}, reasoning by \emph{induction}, and the
\emph{simplification} of expressions through automatic computation. This is
illustrated through a few case studies of basic logical and mathematical
problems.

In \refch{sfl-classical}, we investigate an extension of \kl{PbA} to \kl{sequents}
with \emph{multiple} alternative conclusions, as opposed to the
\emph{single}-conclusion \kl{sequents} found in the interface of almost every
\kl{ITP}. We argue that the use of \kl{direct manipulation} greatly facilitates
the management of multiple conclusions, and introduce a so-called
\emph{\kl(dnd){parallel}} \kl{interaction operator} to model reasoning in \kl{classical}
logic that involves the interaction of two conclusions.

\AP
Lastly in \refch{plugin}, we present \intro{coq-actema}, a plugin that
integrates the \kl{Actema} web application as an interactive \kl{proof view} in
\kl{Coq}. We focus on the architecture and interaction protocols that connect
the different components of the system, and give an overview of the
\kl{elaboration}/compilation strategy that turns graphical \kl{actions} performed in
\kl{Actema} into \kl{Coq} \kl{proof terms}. We also discuss current shortcomings
of our approach and future directions for improvement, in particular concerning
the question of \kl{proof evolution}.

\paragraph{Iconic manipulations}
  
In the second part of this thesis, we explore a series of \kl{deep inference}
\kl{proof systems} that give more structure to the notion of logical
\emph{\kl{goal}}. These systems share the ability to represent \kl{goals} in two
alternative ways: either \emph{textually} through a standard inductive syntax,
or \emph{graphically} through a \kl{metaphorical} notation well-suited to
\kl{direct manipulation}. The first can be used as a machine representation in
the backend of an \kl{ITP}, and the latter as the substrate for \kl{GUIs} in the
frontend.

\begin{description}
  \itemAP[Bubble calculi] In the first two chapters, we introduce a family of
  systems called \intro{bubble calculi}. They are an extension of the theory of
  \intro{nested sequents} first introduced by Brünnler
  \sidecite{brunnler_deep_2009}, that we reframe as local \kl{rewriting systems}
  with a graphical and topological interpretation. \kl{Bubble calculi} enable an
  efficient sharing of contexts between \kl{subgoals}, making them well-suited
  to the factorization of both \kl{forward} and \kl{backward} reasoning steps in
  proofs.

  \AP \refch{bubbles} presents the \emph{asymmetric} \kl{bubble calculus}
  \kl{BJ} for \kl{intuitionistic} logic, modelled after the \emph{asymmetric}
  \kl{sequents} of the \kl{intuitionistic} \kl{sequent calculus} \kl{LJ} of
  Gentzen. It introduces the \kl{metaphor} of \intro{bubbles} as a way to
  \kl{iconically} represent the separation and sharing of contexts between
  different \kl{subgoals}.

  \AP \refch{bubbles-symm} refines \kl{BJ} into a more general and symmetric
  calculus for \kl{classical} logic called \intro{system~B}, where \kl{bubbles}
  can be \emph{\kl(bubble){polarized}} in addition to formulas.
  \kl{Intuitionistic}, \kl{dual-intuitionistic} and \kl{bi-intuitionistic} logic
  can be recovered as fragments of \kl{system~B}, by forbidding certain
  \kl{inference rules} that characterize the \emph{porosity} of bubbles. We also
  devise a fully \kl{invertible} variant of \kl{system~B}, that we conjecture to
  be complete.
  
  \item[Existential graphs] In the last two chapters, we study two systems based
  on the \kl{existential graphs} of Peirce, that allow us to achieve \emph{full
  \kl{iconicity}}: every logical construction has an associated \kl{icon}, and thus there
  is no use anymore for the connectives and quantifiers of \kl{symbolic} formulas.
  Hopefully, this shall remove a first barrier in the learning of formal logic,
  which lies in the \emph{arbitrary} correspondence between \kl{symbols} and their
  meaning.

  In \refch{eg}, we give a complete review of Peirce's original systems of
  \kl{EGs} for propositional and \kl{first-order} \kl{classical} logic, which
  have been consistently neglected in the \kl{proof theory}
  literature\sidenote{One reason might be that \kl{EGs} have been invented at
  the end of the 19\textsuperscript{th} century, \emph{before} the birth of
  \kl{proof theory} as a discipline in the 1920s under the impulse of Hilbert.}.
  We propose in particular a novel inductive characterization of the syntax of
  \kl{EGs}, as well as the first identification of an \emph{\kl{analytic}}
  fragment of the system for propositional logic that is complete for
  provability.

  \AP
  Finally, we introduce in \refch{flowers} the \intro{flower calculus}, an
  \kl{intuitionistic} variant of \kl{EGs} where statements are represented
  \kl{metaphorically} as \emph{flowers}. We partition the system into a
  \emph{\kl{natural}} fragment where every rule is both \kl{analytic} and
  \kl{invertible}, and a \emph{\kl{cultural}} fragment where every rule is
  non-\kl{invertible}. We prove that the \kl{cultural} fragment is
  \kl{admissible} thanks to a completeness proof for the \kl{natural} fragment
  with respect to Kripke semantics. We exploit these meta-theoretical results to
  design the \intro{Flower\,\,Prover}, a prototype of \kl{GUI} in the \kl{PbA}
  paradigm that aims to unify the concepts of \emph{\kl{goal}} and \emph{theory}
  in a \emph{modal interface}: \kl{goals} correspond to flowers manipulated with
  \kl{natural} rules in \Proof mode; while theories correspond to the same
  flowers manipulated with \kl{cultural} rules in \Edit mode. The
  \kl{Flower\,\,Prover} is also the first \emph{mobile-friendly} interface for
  \kl{ITPs} that we know of.
\end{description}

\begin{kaonote}
Most of the content of \refch{pba} and \refch{sfl} has been previously published
in \cite{10.1145/3497775.3503692}, and the \kl{coq-actema} system described
in \refch{plugin} is under active development by us and Benjamin Werner
\cite{coq-actema}. A shortened version of \refch{eg} and \refch{flowers} has
been submitted for publication at FSCD 2024 \cite{flower-calculus}. All other
chapters present completely original and personal work.
\end{kaonote}

\subsection{How to read}

\paragraph{Reading order}

The ordering of chapters in this thesis is mostly \emph{chronological},
reflecting the order in which the ideas were developed. For readers interested
in all of the contributions, we thus advise reading all chapters in order.

Still, the investigations into \kl{iconic} manipulations in the second part started
as an offshoot of those on \kl{symbolic} manipulations in the first part, and were
carried mostly in parallel. Although we sometimes reference ideas from chapters
in the first part, the second part can thus be read mostly independently from
the first one.

\begin{emphpar}
  In all cases, the reader should start with \refch{pba}, which gives a taste of
  the \kl{PbA} paradigm explored in all other chapters.
\end{emphpar}

\reffig{chapter-deps} shows the precise graph of dependencies between chapters.
Four independent paths can be followed:
\begin{description}
  \item[The applied road (\ding{175} $\to$ \ding{177})] If
  you want to see to what extent the \kl{PbA} paradigm can currently be applied for
  practical theorem proving in real \kl{proof assistants}, this is the right path for
  you.

  \item[Proof theory of SFL (\ding{174} $\to$ \ding{176})]
  This path is for readers only interested in the \kl{proof theory} of \kl{subformula
  linking}, which is the foundation for the semantics of \kl{DnD} \kl{actions} on \kl{symbolic}
  formulas.

  \item[Bubble calculi (\ding{178} $\to$ \ding{179})]
  This path is for readers only interested in the \kl{proof theory} and potential
  applications of \kl{bubble calculi}.
  
  \item[Flower calculus (\ding{180} $\to$ \ding{181})] This path is for
  readers only interested in the \kl{proof theory} of the flower calculus, and its
  applications to automated and interactive theorem proving.
\end{description}

A last option is to read only \refch{eg}, skipping even \refch{pba}. This might
be of interest to people looking for an introduction to the \kl{existential graphs}
of Peirce.

\begin{figure*}
  \stkfig{1}{chapter-deps}
  \caption{Dependency graph between chapters}
  \labfig{chapter-deps}
\end{figure*}

\begin{digression}
  We will sometimes develop ideas loosely related to the main text in
  \emph{digression} boxes such as this one: at least on first reading, they can
  be safely ignored. We distinguish them from normal side notes, which are
  usually shorter and more relevant to the matter at hand.
\end{digression}

\paragraph{Color}

Some parts of this document make a heavy, \emph{semantic} use of colors.
Although all important concepts still have a textual, color-independent
presentation, it is recommended to print this document with a decent amount of
color levels.

\paragraph{Hyperlinks}
  
We tend to cross-reference many ideas from different chapters with the help of
\emph{hyperlinks}. In particular, we use the \kl(package){knowledge} package
from Thomas Colcombet to hyperlink occurrences of concepts and notations to the
place where they are introduced. We thus recommend the usage of a PDF reader
that supports at least hyperlink \emph{jumping}, and if possible hyperlink
\emph{preview} for a more comfortable reading experience.


\newpage
\section{Related works}\labsec{intro-rw}

\paragraph{Window inference}

\AP
Other researchers have stressed the importance of being able to reason
\emph{deep} inside formulas to provide intuitive proof steps. The first and
biggest line of research supporting this idea is probably that of \intro{window
inference}, which started in 1993 with the seminal article of P.J. Robinson and
J. Staples \sidecite{robinson-formalizing-1993}, and slowly became out of
fashion during the 2000s. This is well expressed in the following quote from one
of its main contributors, Serge Autexier \sidecite[][p.~184--187]{autexier_phd}:

\begin{quote}
We believe it is an essential feature of a calculus for intuitive reasoning to
support the transformation of parts of a formula without actually being forced
to decompose the formula. In that respect the inference rules of Schütte's proof
theory are a clear contribution. [...] One motivation for the development of the
CORE proof theory was to overcome the need for formula decomposition as enforced
by sequent and natural deduction calculi in order to support an intuitive
reasoning style.
\end{quote}

Thus we are not the first to attempt to design new \kl{proof systems} based on \kl{deep
inference} principles, with the explicit objective of improving the usability of
\kl{ITPs}.
\begin{digression}
  \AP
  Note that during most of the period when \kl{window inference} was developed,
the terminology of ``deep inference'' had not been introduced yet. Indeed, the
first article on the subject appeared in 1999 \cite{Guglielmi1999ACO}, with very
different motivations in mind: namely, the development of a
\kl{proof-theoretical} approach unifying concurrent and sequential computation,
resulting in the \kl{calculus of structures} for the logic \intro{BV}. However,
some \kl{proof systems} based on \kl{deep inference} principles already existed
and inspired researchers in \kl{window inference}, as witnessed by the reference
to Schütte's \kl{proof theory} in the above quote.
\end{digression}
However, we believe our approach is unique in that it emphasizes two aspects:
\begin{itemize}
  \item the use of \emph{\kl{direct manipulation}} on \kl{goals} to perform
  proof steps (although some pointing interactions were already at work in
  \kl{window inference}-based systems);
  \item in the second part of this thesis, the use of \emph{\kl{iconic}}
  representations for the \kl{proof state}, that stray away from traditional
  \kl{symbolic} formulas.
\end{itemize}

\paragraph{Ayers' thesis}

More recently, Ayers described in his thesis a new tool for producing verifiable
and explainable (formal) proofs, including both theoretical discussions of novel
concepts and designs for components of \kl{proof assistants}, and practical
implementations of software evaluated through user studies
\sidecite{ayers_thesis}. Notable contributions from our point of view are:
\begin{itemize}
  \item his \texttt{Box} development calculus, which introduces a unified
\texttt{Box} data structure representing at the same time \kl{goals} and partial
proofs, with the aim to offer more ``human-like'' interfaces for both the
\emph{construction} and the \emph{presentation} of proofs;
  \item and his \kl{ProofWidgets} framework, that allows to extend the
\kl{Lean} \kl{proof assistant} with new interactive and domain-specific
notations for mathematical objects, thus offering a form of \emph{end-user}
programming.
\end{itemize}
The \texttt{Box} data structure easily lends itself to visualization in a two
dimensional graphical notation, while \kl{ProofWidgets} promises great
capabilities for proofs by both direct and \kl{iconic} manipulation.

However, the work of Ayers focuses mainly on designing a general framework that
can integrate modern interfaces for proofs in the \kl{Lean} \kl{proof
assistant}, while we focus on exploring various proof calculi that provide the
foundations for such interfaces at the purely logical level, independently of
any particular \kl{proof assistant}. Thus we believe that our work is quite
complementary to Ayers': it emphasizes different aspects while sharing a common
vision for the future of \kl{proof assistants}, where modern graphical
interfaces play a crucial role in improving the interaction between the user and
the computer.