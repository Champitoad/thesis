\setchapterpreamble[u]{\margintoc}
\chapter{Subformula Linking}
\labch{sfl}

In this chapter, we engage in a thorough analysis of the logical semantics of
\kl{DnD} actions, which were introduced informally through examples in \refch{pba}.
We do this mainly from the formal perspective of \kl{deep inference} \kl{proof theory},
following the original work of K. Chaudhuri on \emph{subformula linking}
\cite{Chaudhuri2013}. But we always keep in mind the intended application to
\kl{proof assistants}, by motivating various design choices --- actual or prospective
--- as ways to improve the UX of interactive proof building.

The chapter is organized as follows: \refsec{linkages} introduces the notions of
context and polarity, and explains how \kl{DnD} actions are specified by the user
interactively through schemas called \emph{\kl{linkages}}. \refsec{validity} explains
how one can identify a subset of \kl{linkages} that guarantees a \emph{productivity}
property on \kl{DnD} actions. \refsec{action} describes the overall structure of how
\kl{linkages} translate into logical steps, and \refsec{invert} discusses some
subtleties of this translation that are related to the concept of
\emph{focusing} in automated proof search. \refsec{soundness} shows that the
logical steps are sound, and \refsec{productivity} states and proves formally
the productivity property. Finally, \refsec{dnd-completeness} shows how \kl{DnD}
actions can be turned into a complete deductive system without any need for
click actions.

\section{Linkages}\labsec{linkages}

Like most \kl{rewriting systems} on terms (that is, tree-shaped data), the rewriting
rules of \reffig{DISL} and \reffig{DISL-U} apply at any depth inside formulas.
However logically, the shape of the \emph{context} in which this rewriting
occurs can provide important information, either to ensure soundness of the
performed transformation (\refsec{soundness}, \refsec{dnd-completeness}), or to
understand the status of quantified variables (\refsubsec{identity}).

As is standard in term rewriting, our notion of context will correspond to that
of a (formula) tree with a distinguished leaf called its \emph{hole}.
% \sidenote{More precisely, our contexts correspond to what is called a
% ``structure'' in the first \kl{deep inference} formalism to date, the \emph{Calculus
% of Structures} (see \refsec{cos}).}

\begin{definition}[Formula context]\label{def:formula-context}
  A \emph{formula context}, written $A\hole$, is a proposition containing
  exactly one occurrence of a specific propositional variable $\hole$ which is
  not used elsewhere.

  Given another proposition $B$, we write $A\select{B}$ for the
  proposition obtained by replacing $\hole$ in $A\hole$ by $B$. Note
  that this replacement is not a substitution because it allows variable
  capture. For instance $\forall x.\select{P(x)}$ is the proposition
  $\forall x.P(x)$.
\end{definition}

\begin{definition}[Path]\label{def:path}
A path is a proposition where one subformula has been
selected. Formally, a path is a pair $(A\hole, B)$ formed by one
context and one proposition:
\begin{itemize}
\item $A\hole$ is called the {\em context} of the path,
\item $B$ is called the {\em selection} of the path.
\end{itemize}

The path $(A\hole, B)$ can be viewed as the proposition
$A\select{B}$.  For readability, we will generally also write
$A\select{B}$ for the path $(A\hole, B)$.
\end{definition}

\begin{definition}[Inversions]
  Given a context $A\hole$, the number of inversions in $A\hole$,
  written $\inv(A\hole)$, is the number of subterms of $A\hole$
  which are of the form $C\hole\limp D$; that is the number of
  times the hole is on the left-hand side of an implication.
  For instance:
  $$
  \begin{array}{rcl}
      \inv(D\land \hole)&=& 0\\
      \inv((D\land \hole)\limp E)&=&1\\
      \inv((\hole\limp C)\limp D) &=& 2
  \end{array}
  $$
\end{definition}

\begin{definition}[Polarity of a context]\label{def:polarity}
We will write $A^+\hole$ to specify that a context is {\em positive},
meaning that $\inv(A^+\hole)$ is even. Symmetrically, $A^-\hole$ will be
used for {\em negative} contexts, meaning that $\inv(A^-\hole)$ is odd.
\end{definition}

% In demonstrating soundness, we focused solely on the two \emph{items} involved
% in a \kl{DnD} action. But

In addition to the items involved, every \kl{DnD} action specifies the
\emph{selection} of a subterm in each item, which can be expressed formally as a
path (Definition \ref{def:path}). We call \intro{linkage} the combined data of
the two items together with the selection, since the intent is to \emph{link}
the subterms to make them interact in some way.

\begin{remark}
In this thesis we only consider \kl{linkages} between two subterms. But as noted in
\refsec{equality}, rewriting is an example of action that can benefit from
allowing multiple selections\sidenote{A restricted kind of multi-occurrence
rewrite is already available in the standalone version of Actema: one needs to
enter \emph{selection mode}, by either toggling the dedicated button (the one
with the mouse cursor in \reffig{aristote}), or holding down the \texttt{shift}
key. Then one can click successively on all occurrences of a term $t$ that are
to be rewritten, in order to add them to the selection. To perform the rewriting
to some other term $u$, the last step is to drag an equality hypothesis $t = u$
(or $u = t$) and drop it on any item holding one of the selected occurrences of
$t$.}.
\end{remark}

Each kind of \kl{DnD} action is mapped in the system to a specific form of \kl{linkage},
which is designed to hold all the information necessary for the correct
execution of the action. In this way the system can automatically search for
\kl{linkages} of a certain form, and propose to the user all well-defined actions
associated to these \kl{linkages}.

\begin{remark}
  In the future, one can imagine several \kl{DnD} actions associated to a given
  \kl{linkage}. In this case, the user could be queried to choose the action to be
  performed (typically with a pop-up menu). However with the actions considered
  in this thesis, such ambiguities never arise.
\end{remark}

On the ``items axis'', we already distinguished between \kl(dnd){backward} and \kl(dnd){forward}
\kl{linkages}, written respectively $A \back B$ and $A \forw B$. If the items are
unspecified, we will write $A \link B$.

Using the ``selection axis'', we can specify a further distinction that was
informal up to now: that of \emph{logical} action and \emph{rewrite} action.
\begin{itemize}
  \itemAP \intro{Logical linkages} link two subformulas. Thus they have the form
  $B\select{A} \link C\select{A'}$.
  \itemAP \intro{Rewrite linkages} link one side of an equality with a \kl{first-order}
  term. Using liberally the notations from Definitions \ref{def:formula-context} and
  \ref{def:path}, they thus have the form
  $$B\select{\select{t} = u} \link C
  \select{t'} \text{\ (or symmetrically $B\select{u = \select{t}} \link C
  \select{t'}$)}$$
  % \item \emph{Instanciation} actions correspond to specialization of universal
  % hypotheses, and witness choice for existential goals.
\end{itemize}

By forgetting the information on which subterms are selected, one can see any
\kl{linkage} as a formula whose topmost connective is a linking operator $\link \in
\{\back,\forw\}$. Then it is natural to view \kl{linkages} as the redexes of the
\kl{rewriting rules} of \reffig{DISL}, although from the user's standpoint \kl{linkages}
only happen at the top level\sidenote{In fact this is more of a limitation of
Actema's current interface: one cannot link two subterms that live in the same
item, because dragging actions can only be performed on \emph{entire} items. But
in the original formulation and implementation of subformula linking
\cite{Chaudhuri2013}, \kl{linkages} can be created between arbitrary subformulas. We
come back to this issue in \refsec{dnd-completeness}.}.

\section{Validity}\labsec{validity}

\begin{marginfigure}
\begin{mathpar}
  \begin{array}{r@{\quad}c@{\quad}lr}
    {A \back B}&\step{}&A \limp B &\mathsf{Brel}\\
    {A \forw B}&\step{}&A \land B &\mathsf{Frel}
  \end{array}
\end{mathpar}
\caption{Release rules}
\labfig{rellink}
\end{marginfigure}

In the original formulation of subformula linking \sidecite{Chaudhuri2013}, a
semantics is associated to every \kl{logical linkage}, even when the selected
subformulas $A$ and $A'$ are not unifiable. This is made possible by the
addition of so-called \emph{release} rules\sidenote{A terminology coming from
the line of works on \emph{focusing} in \kl{proof theory} \cite{andreoli1992}, see
also \refsec{invert}.}, which simply turn linking operators into their
associated logical connective. In our setting this would give the \kl{rewriting rules}
of \reffig{rellink}. However in this work we opt for a different approach:
instead we define a \emph{validity criterion} on \kl{linkages}, which guarantees that
they give rise to the behaviors described in the previous sections. The
criterion tackles two issues:
\begin{itemize}
  \item \textbf{Polarity:} the selected subterms must have opposite
  \emph{polarities}, so that the \emph{negative} subterm justifies the
  \emph{positive} one;
  \item \textbf{Identity:} the selected subterms must be \emph{unifiable}, so
  that after instantiating some quantifiers in their context they can interact
  through the {\rsf{id}} rule or the equality rules {\rsf{L{=}_i},
  \rsf{F{=}_i}}.
\end{itemize}
One benefit of using this criterion is that it filters out all \kl{linkages} whose
semantics relies on release rules, capturing intuitively a notion of
\emph{productivity}: instead of just moving around subformulas, we know for sure
that some simplification occurs, either a justification with the {\rsf{id}}
rule on \kl{logical linkages}, or a rewriting with the equality rules on \kl{rewrite linkages}. This will be stated more formally in \refsec{productivity}.

Validity is very useful to support the \emph{suggestion} mechanism implemented
in Actema. The idea is that when the user starts dragging an item, this
indicates to the system that she wants to perform a \kl{DnD} action involving
subterms of this item. Then the system can suggest such possible actions by
highlighting subterms in the goal which form a valid \kl{linkage} with the dragged
item. Typically in the example of \reffig{aristote}, dragging the hypothesis
$\forall x. \human(x) \limp \mortal(x)$ will have the effect of highlighting
exactly $\mortal(\socrates)$ in the conclusion and $\human(\socrates)$ in the
other hypothesis as possible drop targets. In this case this corresponds to all
subterms in the goal which are not contained in the dragged item. But if one
were to drag the $\human(\socrates)$ hypothesis, then only the subterm
$\human(x)$ in the other hypothesis would be suggested as a drop target. This
could not work with the ``release'' semantics mentioned earlier, since then all
subterms would again be highlighted, providing no useful information to the
user.

We believe that in more complex situations, this filtering can be quite helpful
to guide the user towards the right path to follow in their reasoning. Although
non-trivial arguments are often based on ``guessing'' the right value or lemma
to be used, a large part of mathematical reasoning also consists in ``connecting
the dots'' with information already at hand. Our \kl{DnD} actions capture this
metaphor quite directly, and thus shall be especially useful to beginners
unfamiliar with proving, who often show difficulties in understanding how to
build a proof from scratch. More generally, \kl{proof assistants} have the potential
to provide a well-defined and rigorous methodology in the art of crafting
proofs, in the same way that we have been teaching precise algorithms for
solving equations in calculus classes for centuries. Having a graphical
interface that makes this methodology more intuitive and discoverable is the
main goal of this work, and (valid) \kl{DnD} actions seem to be a good candidate as a
core principle for such a methodology.

\subsection{Polarity}\labsubsec{polarity}

The restrictions on polarities are captured formally by the following condition:


\begin{condition}[Polarity]\label{cond:pol}
  
  The following must be true for a \kl{logical linkage}
  $B\select{A}\link D\select{A'}$ to be valid:

  \begin{enumerate}
    \item the parity of $\inv(B\hole)$ is:

      \begin{enumerate}
        \item the same as $\inv(D\hole)$ if $\link = \back$;
        \item the opposite of $\inv(D\hole)$ if $\link = \forw$;
      \end{enumerate}\label{clause:opposite}

    \item if $\link = \back$ and $\inv(D\hole) = 0$, then $\inv(B\hole) =
    0$\label{clause:intuit}.
  \end{enumerate}

  The following must be true for a \kl{rewrite linkage} $B\select{t} @ D\select{t'}$
  to be valid:

  \begin{enumerate}
    \item if $B\hole$ holds the equality, then it must be:

      \begin{enumerate}
        \item positive if $\link = \back$;
        \item positive if $\link = \forw$;
      \end{enumerate}

    \item if $D\hole$ holds the equality, then it must be:

      \begin{enumerate} 
        \item negative if $\link = \back$;
        \item positive if $\link = \forw$.
      \end{enumerate}

  \end{enumerate}
\end{condition}

One understands that for \kl{rewrite linkages}, this simply guarantees that the
equality is in negative position. For \kl{logical linkages}, Clause
\ref{clause:opposite} ensures that the selected subformulas have opposite
polarities, and Clause \ref{clause:intuit} ensures that the \kl{linkage} makes sense
in our \emph{intuitionistic} setting.

Indeed one could imagine the following behavior
in \kl{classical} logic:
$$(\select{A}\limp B)\limp C\back \select{A}~~\steps{}~~ C\limp A$$ which gives a
proof of Peirce's law when replacing $C$ with $A$. We will come back to this
example in \refch{sfl-classical}, but for now we can just remark that there is
no way to handle it with the rules of \reffig{DISL} because we lack a rule for
redexes of the form $B\select{A} \limp C \back D\select{A'}$.

\subsection{Identity}\labsubsec{identity}

A context binds variables in the selected proposition. These variables
will be unifiable or not depending upon: (1) the nature of the quantifier
($\forall$ or $\exists$), (2) whether they occur in a hypothesis or a
conclusion, and (3) whether they occur on the left-hand of an (odd number
of) implication(s). Therefore, we start by splitting the list of
variables bound by a context in two parts.


\begin{definition}[Positive and negative variables]
Given a context $A\hole$ seen as a tree, one can always start from
the root and traverse the branch of $A\hole$ that leads to its hole
$\hole$. We write $\lvar(A\hole)$ the list of all variables
quantified along the way. This list is ordered, the variables closer
to the root coming first.

$\lvar(A\hole)$ can be seen as the interleaving of two sublists
$\lvarp(A\hole)$ and $\lvarn(A\hole)$ of \emph{positively} and
\emph{negatively unifiable} variables, in the following precise sense:
$x \in \lvarp(A\hole)$ (resp. $x \in \lvarn(A\hole))$ iff there
are contexts $B\hole$, $C^+\hole$ and $D^-\hole$ such that
$A\hole$ is either $C^+\select{\exists x. B\hole}$ or
$D^-\select{\forall x. B\hole}$ (resp. $D^-\select{\exists x. B\hole}$
or $C^+\select{\forall x. B\hole}$).

For instance, if $A\hole \syneq \forall x. \exists y. (B \land ((\exists x'.
\forall y'. \hole) \limp \forall z. C))$, then we have:
\begin{mathpar}
  \lvar(A\hole) = [x, y, x', y'] \and
  \lvarp(A\hole) = [y, y'] \and
  \lvarn(A\hole) = [x, x']
\end{mathpar}


% The three lists are defined recursively over the structure of $A\hole$ by the
% following equations:$$
% \begin{array}{c}
% \begin{array}{rclrcl}
%   \lvar(\hole)\eqb []&  \lvar(\forall x.A\hole) \eqb x::\lvar(A\hole)\\
%   \lvar(P(t_1, \dots , t_n)\eqb []& \qquad   \lvar(\exists x.A\hole) \eqb x::\lvar(A\hole)\\
% \end{array}\\
% \begin{array}{rcl}
%     \lvar(A\hole\vee B),  \lvar(B\vee A\hole),  \lvar(A\hole\wedge B), \lvar(B\wedge A\hole),&&\\ \lvar(A\hole\implies B), \lvar(B\implies A\hole)\eqb \lvar(A\hole)\\
%   \lvarn(\hole), \lvarp(\hole),
%   \lvarn(P(t_1, \dots , t_n)),\lvarp(P(t_1, \dots , t_n))\eqb []\\
  
%   \lvarn(\forall x.A\hole) = x::\lvarn(A\hole)&\qquad&
%   \lvarn(\exists x.A\hole) = \lvarn(A\hole)\\
%     \lvarn(A\hole\vee B),  \lvarn(B\vee A\hole),  \lvarn(A\hole\wedge B), \lvarn(B\wedge A\hole)\eqb \lvarn(A\hole)\\
%     \lvarn(A\hole\implies B)\eqb \lvarp(A\hole)\\
%       \lvarp(\forall x.A\hole) \eqb \lvarp(A\hole)\\
%   \lvarp(\exists x.A\hole \eqb x::\lvarp(A\hole)\\
%    \lvarp(A\hole\vee B),  \lvarp(B\vee A\hole),  \lvarp(A\hole\wedge B), \lvarp(B\wedge A\hole)\eqb \lvarp(A\hole)\\
%      \lvarp(A\hole\implies B)\eqb \lvarn(A\hole)\\
% \end{array}
% \end{array}
% $$
% The three lists are naturally extended to linkages as sets by taking the union
% of the lists associated with the two contexts of the linkage.
\end{definition}

% As we have seen, variables bound in linked formulas are unifiable or
% not, depending on their quantifier, the position of this quantifier,
% and whether they are in a hypothesis or the conclusion.

\begin{definition}[Unifiable variables]\label{def:uvars}
  The set $\uvars(\mathcal{L})$ of {\em unifiable variables} of a \kl{linkage}
  $\mathcal{L} \syneq B\select{A}\link C\select{A'}$ is:
  \begin{itemize}
  \item $\lvarn(B\hole)\cup\lvarp(C\hole)$ if $\link$ is $\back$, and
  \item $\lvarn(B\hole)\cup\lvarn(C\hole)$ if $\link$ is $\forw$.
  \end{itemize}
\end{definition}

The following notions of substitution and unification are the usual ones and we
do not go into details:
 
\begin{definition}[Substitution]
  A \emph{substitution} is a mapping from variables to terms such that $\{x ~|~
  \sigma(x)\not\syneq x\}$ is finite; we call this set the {\em domain} of $\sigma$.

  When $\sigma(x)\syneq x$ we say that $x$ is  {\em not instantiated} by
  $\sigma$.
  
  %% A mapping $\sigma$ from variables to terms is a substitution of
  %% domain $l$ when $\forall x\notin l, \sigma(x)\syneqx$. When
  %% $\sigma(x)\syneqx$, we also say that $x$ is
  
  Given a proposition $A$ and a substitution $\sigma$, we write
  $\sigma(A)$ for the \emph{application} of $\sigma$ to $A$ in the usual way.
  %% For contexts, substitution in the special propositional variable
  %% $\hole$ is defined trivially by $\hole[\sigma] = \hole$.
\end{definition}

\begin{definition}[Unification] Given two propositions $A$ and $A'$ and a list of variables
  $l$, we say that a substitution $\sigma$ \emph{unifies} $A$ and $A'$ over $l$
  when $\sigma(A)\syneq\sigma(A')$ and the domain of $\sigma$ is a subset of $l$. 

  If such a substitution exists, we say that $A$ and $A'$ are \emph{unifiable}
  over $l$.
\end{definition}
Given $A$, $A'$ and $l$, the well-known unification algorithm decides
whether $A$ and $A'$ are unifiable over $l$ and constructs the
substitution when it exists~\sidecite{Montanari}.

\begin{condition}[Identity]\label{cond:unif} 
  For a \kl{linkage} $B\select{A}\link C\select{A'}$ to be valid, the following must
  be true:
  \begin{enumerate}
   \item There exists a substitution $\sigma$ which unifies $A$ and
     $A'$ over the unifiable variables of the \kl{linkage}.\label{clause:unif}
   \item \label{lab:cond} Furthermore, the unification respects the order over
     the variables. More precisely, we request that there exists a list $l$
     which is an interleaving of $\lvar(B\hole)$ and $\lvar(C\hole)$ such
     that, given a unifiable variable $x$ in the domain of $\sigma$, all
     variables occuring in $\sigma(x)$ are placed before $x$ in $l$:
     $$\forall y\in\fv(\sigma(x))\cap(\lvar(B\hole)\cup\lvar(C\hole)), y <_l
     x.$$\label{clause:deps}
  \end{enumerate}
\end{condition}

The last condition ensures acyclicity and will prohibit invalid
\kl{linkages} as described in \refsec{acyclicity}. More precisely,
the list $l$ specifies the order in which the quantifiers will be
treated in the proof construction.


Finally we can state the full validity criterion for \kl{linkages}:

\begin{definition}[Valid linkage]\labdef{valid-linkage}
  We say that a \kl{linkage} $\mathcal{L}$ is \emph{valid} if it satisfies Conditions
  \ref{cond:pol} and \ref{cond:unif}.
\end{definition}

One can check that all the examples given up to here were based on valid
\kl{linkages}.

\section{Describing DnD actions}\labsec{action}

We are now equipped to specify how logical and \kl{rewrite linkages} translate
deterministically to the \kl(dnd){backward} and \kl(dnd){forward} proof steps shown in all examples.

First some remarks can be made about the \kl{rewriting rules} of \reffig{DISL}:
\begin{itemize}
\item The set of \kl{rewriting rules} is obviously non-confluent. 
\item It is also terminating, because the number of connectives or quantifiers
  under $\forw$ or $\back$ decreases\sidenote{Except for the \textsf{Fcomm} rule
  which is just meant to make the $\forw$ operator commutative; formally, the
  only infinite reduction paths end with an infinite iteration of
  \textsf{Fcomm}.}.
\end{itemize}

As for the rules of \reffig{DISL-U}, they are both terminating \emph{and}
confluent. Indeed, they define a function that eliminates redundant occurrences
of the units $\top$ and $\bot$.

Here is a high-level overview of the complete procedure followed to generate a
proof step:
\begin{enumerate}
\item \textbf{Selection:} the user selects two subterms in two items of the current goal; \label{step:selection}
\item \textbf{Linkage:} this either gives rise to a \kl{logical linkage} $B\select{A}
  \link C\select{A'}$ (resp. a \kl{rewrite linkage} $B\select{\select{t} = u} \link
  C\select{t'}$), or does not correspond to a known form of \kl{linkage}. In this case
  the procedure stops here, and the system does not propose any action to the
  user; \label{step:linkage}
\item \textbf{Validity:} the system verifies that the \kl{linkage} is \emph{valid},
  by performing successively the following checks:
  \begin{enumerate}
    \item \textbf{Polarity:} the \kl{linkage} must satisfy Condition \ref{cond:pol};
    \item \textbf{Unification:} the selected subterms $A$ and $A'$ (resp. $t$
    and $t'$) must be unifiable, yielding a substitution $\sigma$;
    \item \textbf{Dependencies:} the substitution $\sigma$ must satisfy
    Condition \ref{cond:unif}.
  \end{enumerate}
  The procedure stops if it fails at any of the above checks;
  \label{step:validity}
\item \textbf{Linking:} the system then chooses a rewriting start\-ing from the
  \kl{linkage}. Thanks to \refthm{productivity}, this re\-writing always ends with a
  proposition of the form $D\select{\sigma(A) \back \sigma(A')}$ $$\text{(resp.
  $D\select{\select{\sigma(t)} = u \link C_0\select{\sigma(t')}}$)}$$
  \label{step:linking}
\item \textbf{Interaction:} thus one can apply the {\rsf{id}} rule (resp. an equality rule in
$\{\mathsf{L\!\!=\!\!_1}, \mathsf{L\!\!=\!\!_2}, \mathsf{F\!\!=\!\!_1},
\mathsf{F\!\!=\!\!_2}\}$); \label{step:interaction}
\item \textbf{Unit elimination:} in the case of a logical action, this creates an occurrence of $\top$,
which is eliminated using the rules of \reffig{DISL-U}; \label{step:unit-elimination}
\item \textbf{Goal modification:} the two previous steps produced a formula $E$.
  In the case of a \kl(dnd){forward} \kl{linkage}, a hypothesis $E$ is added to the goal; in
  the case of a \kl(dnd){backward} \kl{linkage}, the goal's conclusion becomes $E$. In both
  cases, the logical soundness is guaranteed by
  Property~\ref{prop:soundness}. \label{step:goal-modification}
\end{enumerate}


\section{Soundness}\labsec{soundness}


All examples up to now followed the scheme for \kl{DnD} actions sketched in
\refsec{aristote}:
\begin{itemize}
  \item Given a blue item $A$ and a red item $B$, \kl(dnd){backward} proof steps produce a
  new conclusion $C$ by applying a sequence of \kl{rewriting rules} $A \back B \steps{}
  C$.
  \item Given two blue items $A$ and $B$, \kl(dnd){forward} proof steps produce a new
  hypothesis $C$ by applying a sequence of \kl{rewriting rules} $A \forw B \steps{} C$.
\end{itemize}

Thus for such actions to be logically sound, we have to make sure that our
\kl{rewriting system} satisfies the following property:

\begin{theorem}[Soundness]\label{prop:soundness}
  \phantom{a}
  \begin{itemize}
    \item If $A \back B \steps{} C$, then $A, C \seq B$.
    \item If $A \forw B \steps{} C$, then $A, B \seq C$.
  \end{itemize}
\end{theorem}

The following simple covariance and contravariance property will be used
extensively later on:
\begin{lemma}[Variance]\label{prop:cov}
  If $\, \Gamma, A\seq B$, then $\, \Gamma, C^+\select{A}\seq C^+\select{B}$
  and $\, \Gamma, D^-\select{B}\seq D^-\select{A}$.
\end{lemma}
\begin{proof}
  By induction on the depths of $C^+\hole$ and $D^-\hole$.
\end{proof}

For each rule, interpreting $\back$ as $\limp$ and $\forw$ as $\land$ is enough
to show that the rule satisfies Property \ref{prop:soundness} locally. Formally,
we can define a mapping from formulas containing linking operators to usual
formulas where they have been replaced by their interpretation:

\begin{definition}[Interpretation of linking operators]
  The mapping $\lint{\cdot}$ is defined inductively as follows:
  \begin{align*}
    \lint{A \back B} &= \lint{A} \limp \lint{B} & \\
    \lint{A \forw B} &= \lint{A} \land \lint{B} & \\
    \lint{A \mcirc B} &= \lint{A} \mcirc \lint{B} &\text{for $\mcirc \in \{\land, \lor, \limp\}$} \\
    \lint{\mdiam x. A} &= \mdiam x. \lint{A} &\text{for $\mdiam \in \{\forall, \exists\}$} \\
    \lint{\dagger} &= \dagger &\text{for $\dagger \in \{\top, \bot\}$} \\
    \lint{a} &= a &\text{for $a$ atomic} 
  \end{align*}
\end{definition}

For rewritings taking place deeper inside a proposition however, we need to
consider the polarity of their context.

% Using the notation of definition~\ref{def:polarity}, we can state:

\begin{lemma}[Local soundness]\label{lemma:rules-valid-in-context}
  \phantom{a}
  \begin{itemize}
    \item If $C^+\select{A\back B}\step{} D$ then $\lint{D} \seq C^+\select{A\limp B}$.
    \item If $C^-\select{A\back B} \step{} D$ then $C^-\select{A\limp B}\seq \lint{D}$.
    \item If $C^+\select{A \forw B} \step{} D$ then $ C^+\select{A \land B}\seq \lint{D}$.
    \item If $C^-\select{A \forw B} \step{} D$ then $\lint{D} \seq C^-\select{A \land B}$.
  \end{itemize}
\end{lemma}
\begin{proof}
  First notice that $D$ is necessarily of the form $C\select{D_0}$ where $A
  \link B \step{} D_0$\sidenote{Indeed an implicit assumption in this section,
  which is preserved by all the rules, is that a formula contains at most one
  linking operator. Thus if $C\select{A \link B} \step{} D$, the only possible
  redex is $A \link B$.}. Then by careful analysis of each rule, it is
  straightforward to show that $\lint{D_0} \seq A \limp B$ if $\link = \back$ or
  $A \land B \seq \lint{D_0}$ if $\link = \forw$. We can conclude in each case
  by applying Lemma \ref{prop:cov} with $C\hole$ accordingly.
\end{proof}
\begin{remark}
  For some rules, like \rsf{R\!\!\limp_1}, the left-hand and
  right-hand propositions are equivalent:
  $$A \limp B \limp C ~~~\Longleftrightarrow~~~ A \land B \limp C$$ Such rules are
  called {\em invertible} and their names are tagged by *. This point will be
  relevant in \refsec{invert}.
\end{remark}


An easy but important technical point is that \kl{rewriting rules} preserve the
polarity of contexts around redexes, in the following precise sense:
\begin{fact}[Polarity preservation]\label{prop:rules-preserve-polarity}
  \phantom{a}
  \begin{itemize}
    \item
      If $C\select{A\back B} \step{} C'\select{A'\back B'}$
      (resp. $C\select{A \forw B} \step{} $ $ C' \allowbreak\select{A' \forw B'}$) then
      $C\hole$ and $C'\hole$ have the same polarity.
    \item
      If $C\select{A\back B} \step{} C'\select{A'\forw B'}$ (resp.
      $C\select{A\forw B} \step{} C'\select{A'\back B'}$) then $C\hole$ and
      $C'\hole$ have opposite polarities.
  \end{itemize}
\end{fact}

Combining Lemma \ref{lemma:rules-valid-in-context} and Fact
\ref{prop:rules-preserve-polarity}, we obtain the central soundness result
about the \kl{rewriting rules}:
\begin{lemma}[Contextual soundness]\label{lemma:rewriting-valid-in-context}
  \phantom{a}
  \begin{itemize}
    \item If $C^+\select{A\back B}\steps{} D$ then $\lint{D} \seq C^+\select{A\limp B}$.
    \item If $C^-\select{A\back B} \steps{} D$ then $C^-\select{A\limp B}\seq \lint{D}$.
    \item If $C^+\select{A \forw B} \steps{} D$ then $ C^+\select{A \land B}\seq \lint{D}$.
    \item If $C^-\select{A \forw B} \steps{} D$ then $\lint{D} \seq C^-\select{A \land B}$.
  \end{itemize}
\end{lemma}
\begin{proof}
  By induction on the length of the derivation. The base case is trivial by
  reflexivity of entailment. We give the proof for the first statement in the
  list, other cases work similarly. We can assume without loss of generality
  that the derivation has the following shape:
  $$C^+\select{A \back B} \step{} C'\select{A' \link B'} \steps{} D$$
  Then we reason by case on the linking operator $\link$:
  \begin{itemize}
    \item $\link = \back$: by Fact \ref{prop:rules-preserve-polarity}, $C'$ must
    be positive. Therefore by induction hypothesis $\lint{D} \seq C'\select{A'
    \limp B'}$. By Lemma \ref{lemma:rules-valid-in-context} we have
    $C'\select{A' \limp B'} \seq C^+\select{A \limp B}$. Thus by transitivity
    $\lint{D} \seq C^+\select{A \limp B}$.
    \item $\link = \forw$: by Fact \ref{prop:rules-preserve-polarity}, $C'$ must
    be negative. Therefore by induction hypothesis $\lint{D} \seq C'\select{A'
    \land B'}$. By Lemma \ref{lemma:rules-valid-in-context} we have
    $C'\select{A' \land B'} \seq C^+\select{A \limp B}$. Thus by transitivity
    $\lint{D} \seq C^+\select{A \limp B}$.
  \end{itemize}
\end{proof}

% We do not detail the proof here, but it relies crucially on the covariance and
% contravariance property \ref{prop:cov}.

Finally, soundness (Theorem \ref{prop:soundness}) is obtained as the special
case where the rewriting starts in the (positive) empty context.

% Detailed proofs of all the lemmas can be found in \refsec{app:dnd-soundness}.

% In the previous chapter, we showed how to associate a logical behaviour to
% drag-and-drop actions involving two subterms in a sequent, by following a set of
% \kl{rewriting rules} on formulas. While we proved the soundness of these rules and a
% so-called \emph{productivity} property that makes them suitable for interactive
% proof exploration, we left untreated a central question when doing proof theory:
% \emph{completeness}. That is, can we prove any true statement using only these
% \kl{rewriting rules}? The answer is \emph{yes}, and \refsec{sfl-completeness} gives 

% \section{Intuitionistic Completeness}\labsec{sfl-completeness}

% \section{Classical Logic}\labsec{sfl-classical}


\section{Productivity}\labsec{productivity}

An important property of the linking step \ref{step:linking} is that there is
always a rewriting sequence that brings together the selected subterms, which
ensures that we can proceed to the interaction step \ref{step:interaction}.
% Thus linking can be seen as a generalization of the axiom rule, where we relax
% the requirement that both formulas be syntactically equal and at the
% top-level.

%% This process of evidence-providing appears as a good measure of progress in the
%% construction of a proof. In particular, it is stronger than the application of
%% basic \kl{introduction rules} such as those performed by PbP, since those only
%% capture the semantics of logical connectives, i.e. how they shape the
%% superficial structure of the proof. Thus we call this property
%% \emph{productivity}, and provide in the following an outline of its proof.

%% \begin{definition}[Size]
%% The \emph{size} of a context $A\hole$ is the positive integer
%% $\size{A\hole}$ corresponding to the depth at which the hole $\hole$ occurs
%% in $A\hole$.

%% The \emph{size} of a linkage $\mathcal{L} = B\select{A} \link C\select{A'}$ is
%% defined as $\size{\mathcal{L}} = \size{B\hole} + \size{C\hole}$.
%% \end{definition}

%% \begin{lemma}[Decreasing]\label{thm:decreasing}
%%   If $\mathcal{L} \step{} C\select{\mathcal{L'}}$, then $\size{\mathcal{L}} > \size{\mathcal{L'}}$.
%% \end{lemma}
%% \begin{proof}
%%   By straightforward inspection of all rules except \rsf{id}.
%% \end{proof}

Because the \kl{rewriting rules} are terminating, the important point is to show that
one can always apply a rule until one reaches an interaction rule on the
selected subterms. In other words, it is possible to find at least one rule
which preserves Conditions \ref{cond:pol} and \ref{cond:unif} on \kl{linkages}:

\begin{lemma}[Valid Progress]\label{thm:vprogress} If a \kl{linkage} $\mathcal{L} \syneq
  C\select{A} \link C'\select{A'}$ (resp. $C\select{t} \link C'\select{t'}$) is
  valid, then either:
  \begin{enumerate}
    \item $\mathcal{L} \syneq \select{A} \back \select{A}$ (resp. $C\hole \in \{\hole
    = u, u = \hole\}$ for some $u$ and $t \syneq t'$);
    % \item $\mathcal{L} \in \{
    %     \select{A}\back \select{A},\,
    %     \select{t} = u \back C\select{t},\,
    %     u = \select{t} \back C\select{t},\, 
    %     \select{t} = u \forw C\select{t},\,
    %     u = \select{t} \forw C\select{t}
    %   \}$ for some $A, C\hole, t, u$;
    \item or $\mathcal{L} \step{} E\select{\mathcal{L'}}$ for some $E\hole,
      \mathcal{L'}$ with $\mathcal{L'}$ valid.
      % and
      % $$\mathcal{L'} \syneq D\select{\sigma(A)} \link' D'\select{\sigma(A')}$$ (resp.
      % $D\select{\sigma(t)} \link' D'\select{\sigma(t')}$) for some $D\hole,
      % D'\hole$, linking operator $\link'$ and unifying substitution $\sigma$.
  \end{enumerate}
\end{lemma}

A detailed proof is given hereafter for the case of \kl{logical linkages}. It is not
fundamentally difficult, but understandably verbose. The two main points are:
\begin{itemize}
\item The rules involving a connective always preserve validity.
\item When one can apply a rule involving a quantifier $\forall x$ (resp.
  $\exists x$), one checks whether the substitution instantiates $x$ or not. In
  the first case one performs the instantiation rule \rsf{L\forall i} or
  \rsf{F\forall i} (resp. \rsf{R\exists i}); in the second case the
  corresponding \kl{switch rule} in {\small $\{\mathsf{L\forall s}, \mathsf{R\forall
  s}, \mathsf{F\forall s}\}$} (resp. {\small $\{\mathsf{L\exists s},
  \mathsf{R\exists s}, \mathsf{F\exists s}\}$}).
\end{itemize}
\begin{proof}
  Let $\mathcal{L} \syneq B\select{A} \link C\select{A'}$ be a valid \kl{linkage}.\\
  \begin{enumerate}[itemsep=0.8em]
    \item Suppose $B\hole \syneq C\hole \syneq \square$. By
    Condition~\ref{cond:pol}, we know that a \kl(dnd){forward} \kl{linkage} cannot verify
    $(\inv(B\square), \inv(C\square)) = (0,0)$, thus $\mathcal{L}$ must be a
    \kl(dnd){backward} \kl{linkage}. Also $\lvar(B\square)$ and $\lvar(C\square)$ are empty,
    hence by Condition~\ref{cond:unif} $A$ and $A'$ are unified by an empty
    substitution, which entails that $A \syneq A'$. Therefore we are in the
    first case where $\mathcal{L} \syneq \select{A} \back \select{A}$.

    \item Otherwise, either $B\hole$ or $C\hole$ is non-empty. In the following,
    we show that we can always apply a \kl{rewriting rule} that produces a new, valid
    \kl{linkage} $\mathcal{L'} \syneq B' \link' C'$.
    
    Let $\sigma$ and $\lvar$ be respectively the substitution and interleaving
    of the quantified variables of $B\square$ and $C\square$ given by Condition
    \ref{cond:unif}, with $\lvar$ decomposed as $x \Colon \lvar'$.
    
    \begin{itemize}
      \item If $x$ is quantified at the head of either $B\square$ or $C\square$,
        then we apply the associated quantifier rule:

        \begin{description}
          \item[Switch rule (\rsf{L\forall s}, \rsf{L\exists s},
          \rsf{R\forall s}, \rsf{R\exists s}, \rsf{F\forall s},
          \rsf{F\exists s})] Only if $x$ is not in the domain of $\sigma$. In
          \kl(dnd){forward} mode and when $B\square$ binds $x$, one must first apply the
          rule \rsf{Fcomm} to put $B\select{A}$ on the right of $\forw$, so
          that the \kl{switch rule} is applicable. Now we show that $\mathcal{L'}$ is
          valid:

          \begin{enumerate}[itemsep=0.4em]
            \renewcommand{\labelenumii}{\theenumii}
            \renewcommand{\theenumii}{\arabic{enumii}.}

            \item $\mathcal{L'}$ satisfies Condition \ref{cond:pol} trivially
            since none of the \kl{switch rules} changes the number of inversions.

            \item For each \kl{switch rule} we can show, using the fact that $x$ is
            not in the domain of $\sigma$, that $\uvars(\mathcal{L'}) =
            \uvars(\mathcal{L})$. Since the selected formulas $A$ and $A'$ stay
            untouched by the rule, we can choose $\sigma$ as a valid unifier
            that ranges over $\uvars(\mathcal{L'})$.
            
            \item In all \kl{switch rules}, we have $\lvar(\mathcal{L'}) = \lvar'$
            because the quantifier of $x$ is moved in the outer context of the
            \kl{linkage}. Thus we can just take $\lvar'$ as interleaving, and
            Condition \ref{cond:unif} will still be verified because $\lvar'$ is
            a sublist of $\lvar$.
          \end{enumerate}

          \item[Instantiation rule (\rsf{L\forall i}, \rsf{R\exists i},
          \rsf{F\forall i})] Only if $x$ is instantiated by $\sigma$, using
          $\sigma(x)$ as witness. Again one might need to apply \rsf{Fcomm}
          first. Then we check the validity of $\mathcal{L'}$:

          \begin{enumerate}[itemsep=0.4em]
            \renewcommand{\labelenumii}{\theenumii}
            \renewcommand{\theenumii}{\arabic{enumii}.}
            
            \item $\mathcal{L'}$ satisfies Condition \ref{cond:pol} trivially
            since none of the instantiation rules changes the number of
            inversions.

            \item For each instantiation rule we can show, using the fact that
            $x$ is instantiated by $\sigma$, that $\uvars(\mathcal{L'}) =
            \uvars(\mathcal{L}) \setminus \{x\}$. Then we take as unifier
            $\sigma$ where the binding for $x$ is removed, written $\sigma
            \setminus x$.

            Now we need to make sure that $\sigma \setminus x$ is indeed a
            unifier for the selected formulas. We consider only the case where
            $B\square$ binds $x$, the proof being exactly symmetric when
            $C\square$ binds $x$. Let $B_0\square$ be the direct subcontext of
            $B\square$, that is $B\square$ without the head quantifier binding
            $x$. \\

            First we can assert that $\subst{B_0\select{A}}{\sigma(x)}{x} \syneq
            \subst{B_0}{\sigma(x)}{x}\select{\subst{A}{\sigma(x)}{x}}$. Indeed,
            Clause \ref{clause:deps} of Condition \ref{cond:unif} guarantees
            that for any free variable $y$ of $\sigma(x)$, $y \not\in
            \lvar(B_0\square)$, and thus the above instantiation can propagate
            safely to $A$ without capture. To convince yourself that $y \not\in
            \lvar(B_0\square)$, suppose the contrary. Then $y \in
            \lvar(B\square)$, and by Clause \ref{clause:deps} $y$ must be placed
            before $x$ in $\lvar$. But this is impossible since $x$ is the first
            element of $\lvar$!\\
            
            So we know that the selected formula on the left of $\mathcal{L'}$
            is $\subst{A}{\sigma(x)}{x}$, while it is still $A'$ on the right.
            Thus it only remains to show that
            $$\subst{A}{\sigma(x)}{x}[\sigma \setminus x] \syneq A'[\sigma
            \setminus x].$$ On the left we have by definition that
            $\subst{A}{\sigma(x)}{x}[\sigma \setminus x] \syneq A[\sigma]$, and
            on the right we have $A'[\sigma \setminus x] \syneq A'[\sigma]$
            because $x$ cannot occur in $A'$ since it is bound in $B_0\square$
            (here we rely on the Barendregt convention).

            \item In all instantiation rules, we have $\lvar(\mathcal{L'}) =
            \lvar'$ because the quantifier of $x$ is removed by the
            instantiation. Thus we can again take $\lvar'$ as interleaving.
          \end{enumerate}
          
        \end{description}

      \item If $x$ is not quantified at the head of $B\hole$ or $C\hole$, then
      either both heads are propositional connectives, or one is a propositional
      connective and the other is empty. In both cases we can choose either a
      rule of the form {\rsf{L\mcirc_i}}, {\rsf{R\mcirc_i}} or
      {\rsf{F\mcirc_i}}, where $\mcirc$ is the connective and $i$ the index of
      the direct subcontext where $A$ or $A'$ occurs, or the {\rsf{Fcomm}}
      rule. Again we check the conditions of Definition \ref{def:valid-linkage}:
      
      \begin{enumerate}[itemsep=0.4em]
        \renewcommand{\labelenumii}{\theenumii}
        \renewcommand{\theenumii}{\arabic{enumii}.}
            
        \item In most rules the number of inversions stays unchanged. The only
        exceptions are \rsf{R\!\!\limp_1} and \rsf{F\!\!\limp_1}, which
        decrease the number of inversions of the right context $C\hole$ by $1$.
        But since they are also the only rules that change the linking operator,
        the truth of Clause \ref{clause:opposite} is preserved: if the parities
        were opposite (resp. identical) in $\mathcal{L}$, then $\mathcal{L}$
        must be \kl(dnd){forward} (resp. \kl(dnd){backward}). Thus $\mathcal{L'}$ is necessarily
        \kl(dnd){backward} (resp. \kl(dnd){forward}), and so the parities in $\mathcal{L'}$ must be
        identical (resp. opposite), which is the case thanks to the inversion
        decrement.

        For Clause \ref{clause:intuit}, we can distinguish two cases:
        \begin{itemize}
          \item If $\mathcal{L}$ is \kl(dnd){backward}, then either we apply the
          {\rsf{R{\limp}_1}} rule and $\mathcal{L'}$ is \kl(dnd){forward}, and thus
          satisfies Clause \ref{clause:intuit} trivially; or we apply another
          \kl(dnd){backward} rule and $\mathcal{L'}$ is \kl(dnd){backward}. Now suppose
          $\inv(C'\hole) = 0$. Then we must have $\inv(C\hole) = \inv(C'\hole) =
          0$ and $\inv(B\hole) = \inv(B'\hole)$ since all \kl(dnd){backward} rules other
          than {\rsf{R{\limp}_1}} preserve the number of inversions. And
          because $\mathcal{L}$ satisfies Clause \ref{clause:intuit} by
          validity, we can deduce that $\inv(B\hole) = 0$, and thus
          $\inv(B'\hole) = 0$.
          \item If $\mathcal{L}$ is \kl(dnd){forward}, then either we apply a \kl(dnd){forward} rule
          that is neither {\rsf{F{\limp}_1}} nor {\rsf{Fcomm}} and
          $\mathcal{L'}$ is \kl(dnd){forward}, and thus satisfies Clause
          \ref{clause:intuit} trivially; or we consider applying either
          {\rsf{F{\limp}_1}} of {\rsf{Fcomm}}. There are three cases:
          \begin{itemize}
            \item If $\inv(C\hole) > 1$, then we can safely apply
            {\rsf{F{\limp}_1}} since we have $\inv(C'\hole) = \inv(C\hole) - 1 >
            0$;
            \item If $\inv(C\hole) = 0$, then $C\hole$ is empty and we are
            forced to apply {\rsf{Fcomm}} so that we can apply the \kl(dnd){forward}
            rule corresponding to the head connective of $B\hole$. Then $C\hole$
            ends up on the left of $\forw$, thus if we apply
            {\rsf{F{\limp}_1}} for $B\hole$ Clause \ref{clause:intuit} will be
            satisfied trivially;
            \item If $\inv(C\hole = 1)$, then either $\inv(B\hole) = 0$ and we
            can safely apply {\rsf{F{\limp}_1}} since $\inv(B'\hole) =
            \inv(B\hole)$; or $\inv(B\hole) > 0$, and we cannot apply
            {\rsf{F{\limp}_1}} because we would end up with $\inv(C'\hole) =
            0$ and $\inv(B'\hole) > 0$, thus violating Clause
            \ref{clause:intuit}. Hence as in the previous case, we need to apply
            {\rsf{Fcomm}} first. Then it cannot be the case that $\inv(B\hole)
            = 1$ because we would have $\inv(B\hole) = \inv(C\hole)$, which
            violates Clause \ref{clause:opposite} from the validity of
            $\mathcal{L}$. Thus $\inv(B\hole) > 1$, which entails that we can
            safely apply {\rsf{F{\limp}_1}} on $B\hole$ as in the first case.
          \end{itemize}
          Notice that whenever we apply the {\rsf{Fcomm}} rule, it is to apply
          the rule corresponding to the head connective of $B\hole$ immediately
          afterwards: we never enter a loop by applying {\rsf{Fcomm}} twice in
          a row. Thus technically there are two reduction steps, but we treat
          them as one.
        \end{itemize}
        
        % suppose that $\mathcal{L'}$ is backward
        % and $\inv(C'\hole) = 0$. Then if we applied rule {\rsf{F{\limp}_1}} we
        % must have $\inv(C\hole) = 1$ and $\mathcal{L}$ is forward. Otherwise in
        % every other applicable rule the parity and operator are preserved, thus
        % $\inv(C\hole) = 0$ and $\mathcal{L}$ is backward. In both cases we have
        % that $\inv(B\hole) = \inv(B'\hole)$, and we want to show that
        % $\inv(B'\hole) = 0$. In the first case by Clause \ref{clause:opposite}
        % we know that the $\inv(B\hole)$ is even.
        
        % Thus for \rsf{R\!\!\limp_1} which is backward, we start with either
        % $(1,1)$ or $(0,2)$, and obtain $(1,0)$ or $(0,1)$ which are valid
        % according to Condition \ref{cond:pol} since $\mathcal{L'}$ is forward.
        % Conversely for \rsf{F\!\!\limp_1} which is forward, we must start with
        % $(1,0)$, and obtain $(0,0)$ which is valid since $\mathcal{L'}$ is
        % backward.

        \item Since we do not deal with quantifiers, we can just take the same
        unifier $\sigma$.

        \item Idem here, we take the same interleaving $l$.
      \end{enumerate}
    \end{itemize} 
  \end{enumerate}
\end{proof}

%% \begin{proof}
%%   See appendix \ref{prf:vprogress}.
%% \end{proof}

% Most of the magic occurs here, but by lack of space we had to move this proof to
% appendix \ref{?}.
Then we can state the following \emph{productivity theorem}, which is a direct
consequence of the previous lemma and the fact that the \kl{rewriting rules}
terminate:

\begin{theorem}[Productivity]\labthm{productivity}
If $\mathcal{L}$ is a valid \kl{linkage}, then there
is a sequence of reductions with one of the following forms:
\begin{mathpar}
  \mathcal{L} \steps{} D^+\select{\select{A} \back \select{A}} \\
  \mathcal{L} \steps{} D\select{\select{t} = u \link A\select{t}} \and
  \mathcal{L} \steps{} D\select{u = \select{t} \link A\select{t}}
\end{mathpar}
\end{theorem}

This is the formal counterpart to the notion of productivity mentioned in
\refsec{validity}. Intuitively, this theorem ensures non-trivial progress in the
reasoning: we managed to connect some dots in the problem and actually solve a
subgoal. That is, either the conclusion is \emph{strictly} weakened after a
\kl(dnd){backward} \kl{DnD}, or the assumptions are \emph{strictly} strengthened after a
\kl(dnd){forward} \kl{DnD}, instead of having just an equivalent goal written in a different
way\sidenote{This remark only applies to \kl{logical linkages} however, since
rewriting equalities can only produce equivalent statements. Some \kl{proof
assistants} provide facilities to rewrite arbitrary relations in subterms of
arbitrary depth, such as Coq with its \emph{generalized rewriting} mechanism
\cite{coqman-genrew}. This includes non-symmetric relations that can produce
non-equivalent statements, and there is no reason in principle it could not be
integrated in our paradigm, in the form of generalized substitution rules in
place of {\rsf{L{=}_i}} and {\rsf{F{=}_i}}.}. This again contrasts with the
release semantics of subformula linking which do not provide this guarantee of
productivity, or with the logical reasoning tactics of \kl{proof assistants} based on
\kl{natural deduction} rules.

\section{Focusing}\labsec{invert}

% \sidenote{There are 7 invertible rules, and 23 non-invertible
%     rules to consider in \reffig{DISL} (we exclude the {\rsf{id}} and
%     {\rsf{Fcomm}} rules).}

A last point to deal with is non-confluence and in particular choosing
between first simplifying the head connective on the right or the left
of $\forw$ or $\back$. For instance in
$\select{A}\lor B \back B\lor\select{A}$ one can apply either
\rsf{L\lor_1} or \rsf{R\lor_2}.

Interestingly, an answer is provided by {\em focusing}. It has been noticed by
Andreoli~\sidecite{andreoli1992} that, in bottom-up proof search, one should
apply the invertible logical rules first since they preserve provability. In our
framework, this translates into first applying the invertible \kl{rewriting rules} (the
ones marked by a *).
% \sidenote{Hopefully at some point, detailed proofs of
% non-invertibility for intuitionistic/classical logic based on counter-models
% will be provided in annex.}.
In the case of the example above, this means performing \rsf{L\lor_1} first,
which leads to the following behavior:
$$\select{A}\lor B \back B\lor\select{A}\steps{} B\limp B\lor A.$$
This is indeed the ``right'' choice, since applying \rsf{R\lor_2} first would
lead to a dead-end\sidenote{Interestingly in this case it creates a dead-end
only in \kl{intuitionistic} logic: in \kl{classical} logic both results are provable.}:
$$\select{A}\lor B \back B\lor\select{A}\steps{} B\lor(B\limp A).$$

The general scheme for choosing a rule to apply to a redex $C\select{A} \link
D\select{B}$ is the following\sidenote{A less deterministic version of this
scheme is already present implicitly in the proof of Lemma \ref{thm:vprogress}.}:
\begin{enumerate}
  \item If $C\hole \syneq D\hole \syneq \hole$, we just apply the {\rsf{id}} rule
  (assuming $A \syneq B$ by Lemma \ref{thm:vprogress}).
  \item If only one context is non-empty, say $C\hole$, we look at its head
  connective as well as the side where its hole resides:
  \begin{itemize}
    \item either $C\hole \syneq C_0\hole \mcirc E$ for some binary connective
    $\mcirc$, and we choose the rule {\rsf{L\mcirc_1}} (resp.
    {\rsf{F\mcirc_1}}) if $\link = \back$ (resp. $\link = \forw$);
    \item or $C\hole \syneq E \mcirc C_0\hole$ and we choose the rule
    {\rsf{L\mcirc_2}} (resp. {\rsf{F\mcirc_2}}) if $\link = \back$ (resp.
    $\link = \forw$).
  \end{itemize}
  In the case where it is $D\hole$ which is non-empty, we apply the same logic
  but with the right rules {\rsf{R\mcirc_i}} instead of the left rules
  {\rsf{L\mcirc_i}}.
  \item If both contexts are non-empty, then the previous logic determines one
  rule for $C\hole$ and one rule for $D\hole$, giving rise to the ambiguity
  described in the above example.
\end{enumerate}
  
There are three possibilities when analysing invertibility of the two rules in
the third case:
\begin{enumerate}
  \item if both are invertible, then the order of application does not matter
  since we preserve provability in the end;
  \item if only one is invertible, we apply it first following the focusing
  discipline;
  \item if neither are invertible, we want to choose the order that maximizes
  the preservation of provability. It turns out that in almost all cases the two
  rules commute, that is the formulas obtained in the two orderings are
  equivalent. The only exceptions are the critical pairs \rsf{F\!\!\lor_i
  /~F\!\!\limp_2} for $i \in \{1,2\}$, as was noted independently in
  \sidecite{DBLP:conf/cade/Chaudhuri21}. In this case, one should rely on
  information given by the user to choose the right ordering, which can be done
  by exploiting the \emph{orientation} of the associated \kl{DnD} action, that is
  distinguishing between the source path and the destination path\sidenote{Note
  that in the current implementation of Actema, we instead rely on an arbitrary
  prioritizing fixed in the system, which can hinder in some cases the ability
  to prove a goal through \kl{DnD} actions. In practice, one rarely encounters such
  cases in real examples.}.
\end{enumerate}
Currently we do not have detailed proofs of permutability for all pairs of
rules. The reason is mostly pragmatic: given the great number of rules, this
would take a lot of time to perform a full case analysis. Actually our claim of
permutability comes from \cite{DBLP:conf/cade/Chaudhuri21} which uses a
subformula linking system almost identical to ours. We hope we will be able to
provide rigorous proofs in annex when time permits.

\section{Completeness}\labsec{dnd-completeness}

To enable a fully graphical approach to theorem proving that does not rely on a
textual proof language, it is important to show that (a subset of) the set of
actions exposed to the user is \emph{complete} with respect to provability. That
is, any formula $A$ which is \emph{true} in our logic --- here \kl{intuitionistic}
\kl{FOL} --- can be proved by executing a sequence of graphical actions that reduces
it to the empty goal. We noticed in Remark \ref{rem:click-completeness} that
click actions are a sufficient basis for completeness. While we believe that a
combination of both click and \kl{DnD} actions is more comfortable to handle a
variety of proof situations, it is still interesting to consider the question of
completeness for \kl{DnD} actions alone. It turns out that the answer is positive:
the mechanism of \emph{subformula linking} underlying \kl{DnD} actions is powerful
enough to capture provability in \kl{FOL}. This has already been shown by Chaudhuri
in \cite{Chaudhuri2013} for linear logic, and \cite{DBLP:conf/cade/Chaudhuri21}
for \kl{intuitionistic} logic. Here we give a completeness proof for a system based
on a slight extension of our \kl{rewriting rules}, following ideas from these works.

\begin{remark}
  What we prove in this section is a \emph{weak} form of completeness: we show
  that for any true formula, there always exists a derivation in our subformula
  linking system, but this derivation might not be constructible by the
  deterministic procedure outlined in \refsec{invert}. There are two aspects
  that make the stronger version hard to prove in practice:
  \begin{itemize}
    \item To show that the choices performed by the focusing procedure always
    allow to find a proof when there exists one, it would be necessary to
    formulate and prove a \emph{focusing theorem} based on the permutability of
    rules mentioned at the end of \refsec{invert}.
    \item Even then, some additional rules of our subformula linking system are
    not simulated in any way by the \kl{DnD} procedure of \refsec{action}. We will come
    back to this point soon.
  \end{itemize}  
  % The gap between this theoretical completeness and the
  % practical completeness of the implementation should in part be bridged by the
  % focusing theorems of \refsec{focusing}, but we did not attempt a full
  % formalization of this. A venue for future work would be a certified
  % implementation of subformula linking in Coq as presented in \refch{plugin},
  % with an additional formal proof of completeness.
\end{remark}

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {C^+\select{A \limp B}}&\step{}&{C^+\select{A \back B}} &\mathsf{B}\\
        {C^-\select{A \land B}}&\step{}&{C^-\select{A \forw B}} &\mathsf{F}\\
    \end{array}
  \end{mathpar}
  \caption{\kl{Linkage} formation rules}
  \labfig{newlink}
\end{marginfigure}

To the \kl{rewriting rules} of \reffig{DISL} and \reffig{DISL-U}, we add \emph{\kl{linkage}
formation} rules (\reffig{newlink}), which are a deep generalization of \kl{linkage}
creation between two formulas of a sequent. Rule {\rsf{B}} creates a
\emph{\kl(dnd){backward}} \kl{linkage} between $\limp$-linked formulas in any positive context
$C^+\hole$, and dually rule {\rsf{F}} creates a \emph{\kl(dnd){forward}} \kl{linkage} between
$\land$-linked formulas in any negative context $C^-\hole$\sidenote{This is
reminiscent of the adjunction between the product and the exponential in
\emph{cartesian closed categories}, which respectively interpret conjunction and
implication in the Curry-Howard-Lambek correspondance for \kl{intuitionistic}
logic.}. Note that contrary to the rules considered up to now, \kl{linkage} formation
rules are not closed under arbitrary contexts: indeed the polarity restrictions
are necessary to ensure soundness, as reviewed in \refsec{soundness}. A \kl(dnd){backward}
\kl{linkage} in a sequent $\Gamma, D\select{A} \seq E\select{B}$ would be encoded by
the instance
$$C^+\select{D\select{A} \limp E\select{B}} \step{} C^+\select{D\select{A} \back
E\select{B}}$$ of {\rsf{B}} where $C^+\hole \syneq \bigwedge Γ \limp \hole$, while
a \kl(dnd){forward} \kl{linkage} in a sequent $\Gamma, D\select{A}, E\select{B} \seq F$ would
be encoded by the instance $$C^-\select{D\select{A} \land E\select{B}} \step{}
C^-\select{D\select{A} \forw E\select{B}}$$ of {\rsf{F}} where $C^-\hole \syneq
\bigwedge \Gamma \wedge \hole \limp F$.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {C^-\select{A}}&\step{}&{C^-\select{A \land A}} &\mathsf{conn}\\
        {C^-\select{A}}&\step{}&{C^-\select{\top}} &\mathsf{weak}\\
    \end{array}
  \end{mathpar}
  \caption{Resource rules}
  \labfig{reslink}
\end{marginfigure}

Another necessary ingredient is the addition of a deep version of the
\emph{\kl{structural rules}} of \kl{sequent calculus}. We already have the
{\rsf{Fcomm}} rule to handle commutativity of the $\forw$ operator, which acts
as a kind of \kl{exchange} rule. Then we add the equivalent of \kl{contraction}
and \kl{weakening} with the rules {\rsf{conn}} and {\rsf{weak}}
(\reffig{reslink}). These allow to erase and duplicate hypotheses at will, by
identifying any subformula occurring in a negative context as a hypothesis. Thus
once again we need to be careful about polarity, and cannot close these rules
under arbitrary contexts.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\,}c@{\,}lr}
        {C^+\select{A \limp B}}&\step{}&{C^+\select{A \limp (A \back B)}} &\mathsf{Bconn}\\
        {C^-\select{A \land B}}&\step{}&{C^-\select{A \land (A \forw B) \land B}} &\mathsf{Fconn}\\
    \end{array}
  \end{mathpar}
  \caption{Duplicating \kl{linkage} formation rules}
  \labfig{newreslink}
\end{marginfigure}

An alternative to the full \kl{contraction} rule {\rsf{conn}} is to systematically
duplicate negative formulas in the \kl{linkage} formation rules, giving the rules
{\rsf{Bconn} and \rsf{Fconn}} of \reffig{newreslink}. This models more
closely what we do in Actema, where hypotheses involved in a \kl{DnD} action are
always preserved in the new goal. This is important from a usability standpoint,
because this ensures the user needs not fret with manual duplication of
hypotheses in order to complete a proof. The downside is that the context always
grows bigger, but this can be balanced by exposing the \kl{weakening} rule in the
interface. In Actema it is mapped to a ``delete'' button placed next to every
hypothesis (the little gray trashbin icons in \reffig{oneplusone}).
% But for the purpose of completeness only, the weakening rule {\rsf{weak}}
% should be admissible since its role is already played by the absorption rules
% {\rsf{absl}, \rsf{absr} and \rsf{absq}} of \reffig{DISL-U}.
In our completeness proof we will use all the rules in $\{\text{{\rsf{conn}},
{\rsf{weak}}, {\rsf{B}}, {\rsf{F}}, {\rsf{Bconn}}, {\rsf{Fconn}}}\}$,
in order to make derivations more concise.

Because \kl{linkages} created by rules {\rsf{B} and \rsf{F}} are not
necessarily valid, one needs to add the so-called \emph{release} rules already
mentioned in \refsec{linkages} (\reffig{rellink}). In fact these rules are
crucial in order to simulate rules from \kl{sequent calculus}, which will be the
backbone of our completeness proof as in \cite{Chaudhuri2013}. It is interesting
to consider the question of completeness without release rules, especially since
we do not use them in the semantics of \kl{DnD} actions. We conjecture that it should
hold but would require a completely different argument, maybe of a more semantic
nature like the original proof of Gödel with Tarski models\sidenote{Or Kripke
models in an \kl{intuitionistic} setting. }. Another possibility might be to use a
more canonical representation of proofs that is in-between syntax and semantics,
like the combinatorial proofs of Hughes \sidecite{Hughes_2006} which are known
to be closely related to \kl{deep inference} proofs.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {t = t}&\step{}&{\top} &\mathsf{refl}\\
    \end{array}
  \end{mathpar}
  \caption{Reflexivity rule for $=$}
  \labfig{refl-rule}
\end{marginfigure}

Lastly, a trivial but necessary addition is the rule {\rsf{refl}} of
\reffig{refl-rule} stating the reflexivity of $=$. It was not introduced before
because it is already handled by click actions on red items in Actema
(\refsec{clicks}), but here we want a self-contained system that models as
closely as possible the space of proofs that can be built through \kl{DnD} actions
only. In this context, one could imagine restricting the usage of the
{\rsf{refl}} rule to the unit elimination phase (\refsec{action}), where it
would play the same role as the rules of \reffig{DISL-U}. Thus adding this rule
does not correspond morally to modelling a click action, nor to a modification
of the semantics of \kl{DnD} actions as for release rules.

Then we simply rely on the completeness of \kl{sequent calculus} by performing a
proof by simulation. There are many variants of \kl{sequent calculi} for
\kl{intuitionistic} \kl{first-order logic} described in the literature. In our case the
choice mostly does not matter: all we need is that it is \emph{analytic}, i.e.
satisifies the subformula property. Indeed the very idea of subformula linking
is based on analyticity: one should be able to prove a statement by the sole act
of linking sub-sentences already present in the statement.

\begin{marginfigure}
\begin{mathpar}
  \R[Ref]
    {\Gamma, t = t \seq C}
    {\Gamma \seq C}
  \\
  \top \quad\step{} \quad t = t \quad \mathsf{ref}
\end{mathpar}
\caption{Non-analytic reflexivity rules}
\labfig{ref-rule}
\end{marginfigure}

We chose the calculus \kl{G3i} from \cite{negri_structural_2001} as our basis,
because all \kl{structural rules} are admissible in it (but they would be
straightforward to simulate apart from the cut rule). The first modification we
do is that we model hypotheses in sequents as lists instead of multisets, to
make the translation from sequents to formulas completely deterministic. Thus we
need to add the \kl{exchange} rule {\rnm{exch}}, which is simulated
straightforwardly with the {\rsf{Fcomm}} rule as mentioned earlier. The second
modification we do is adding \kl{introduction rules} {\rnm{{=}R}} and
{\rnm{{=}L}} for equality. The \kl{left introduction rule} {\rnm{{=}L}} captures
Leibniz's elimination scheme, and is in fact the rule {\rnm{Repl}} from
\cite{negri_structural_2001} (modulo the fact that we use single-succedant
instead of multi-succedant sequents). The \kl{right introduction rule}
{\rnm{{=}R}} is an axiomatic reflexivity rule, instead of the {\rnm{Ref}} rule
from \cite{negri_structural_2001} (\reffig{ref-rule}). The reason is that we
cannot simulate the latter directly without adding its equivalent rule
{\rsf{ref}} to our calculus (\reffig{ref-rule}), which we do not want to do
because it would break the subformula property. We conjecture that using
{\rnm{{=}R}} instead of {\rnm{Ref}} does not break the cut admissibility theorem
from \cite{negri_structural_2001}.

% We could not find mention of such a rule in the literature, where instead people
% use either a non-analytic rule\todo{cite nlab at
% https://ncatlab.org/nlab/show/sequent+calculus}, a unification-based
% rule\todo{cite Dale Miller's paper}, or the more traditional axiomatic
% formulation of equality as a congruent equivalence relation instead of a left
% \kl{introduction rule}\todo{cite someone}. But our rule looks reasonable enough, so
% we did not attempt a formal justification of soundness/completeness that would
% certainly lead us astray. The only problematic aspect would be if it breaks cut
% admissibility. We conjecture it does not, and postpone an update of the cut
% admissibility proof of \cite{negri_structural_2001} with our equality rules as
% future work.

\begin{theorem}[Completeness of subformula linking]\labthm{sfl-completeness}
  If $\Gamma \seq A$ is provable in $\text{\kl{G3i}} + \{exch, {=}R, {=}L\}$,
  then $\bigwedge \Gamma \limp A \steps{} \top$.
\end{theorem}
\begin{proof}
  By induction on the derivation of $\Gamma \seq A$. The base case simulates the
  rules {\rnm{ax}}, {\rnm{\top R}}, {\rnm{\bot L}} and {\rnm{{=}R}}. Other rules
  are simulated as usual by composing the derivations obtained from the
  induction hypotheses, making a crucial use of the release rules {\rsf{Brel}}
  and {\rsf{Frel}}. The full mapping from \kl{sequent calculus} rules to
  derivations in our subformula linking calculus is given in the following
  table. Note that we treat conjunctive formulas modulo associativity to avoid
  bureaucratic details.

  \begin{align*}
    \R[ax]
      {\Gamma, A \seq A}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \limp A & \\
      \step{} & \Gamma \land A \back A & \mathsf{B} \\
      \step{} & A \back A & \mathsf{L\land_2} \\
      \step{} & \top & \mathsf{id}
    \end{array}
    \\\\
    \R[exch]
      {\summ[$\pi_1$]{\Gamma, B, A, \Gamma' \seq C}}
      {\Gamma, A, B, \Gamma' \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \land B \land \Gamma' \limp C & \\
      \step{} & \Gamma \land (A \forw B) \land \Gamma' \limp C & \mathsf{F} \\
      \step{} & \Gamma \land (B \forw A) \land \Gamma' \limp C & \mathsf{Fcomm} \\
      \step{} & \Gamma \land B \land A \land \Gamma' \limp C & \mathsf{Frel} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\top R]
      {}
      {\Gamma \seq \top}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \top & \\
      \step{} & \top & \mathsf{absr}
    \end{array}
    \\\\
    \R[\land R]
      {\summ[$\pi_1$]{\Gamma \seq A}}
      {\summ[$\pi_2$]{\Gamma \seq B}}
      {\Gamma \seq A \land B}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A \land B & \\
      \step{} & \Gamma \limp (\Gamma \back A \land B) & \mathsf{Bconn} \\
      \step{} & \Gamma \limp (\Gamma \back A) \land B & \mathsf{R\land_1} \\
      \step{} & \Gamma \limp (\Gamma \limp A) \land B & \mathsf{Brel} \\
      \steps{} & \Gamma \limp \top \land B & IH(\pi_1) \\
      \step{} & \Gamma \limp B & \mathsf{neul} \\
      \steps{} & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[\lor R_i]
      {\summ[$\pi_1$]{\Gamma \seq A_i}}
      {\Gamma \seq A_0 \lor A_1}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A_0 \lor A_1 & \\
      \step{} & \Gamma \back A_0 \lor A_1 & \mathsf{B} \\
      \step{} & (\Gamma \back A_i) \lor A_{1 - i} & \mathsf{R\lor_i} \\
      \step{} & (\Gamma \limp A_i) \lor A_{1 - i} & \mathsf{Brel} \\
      \steps{} & \top \lor A_{1 - i} & IH(\pi_1) \\
      \step{} & \top & \mathsf{absl}
    \end{array}
    \\\\
    \R[{\limp}R]
      {\summ[$\pi_1$]{\Gamma, A \seq B}}
      {\Gamma \seq A \limp B}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A \limp B & \\
      \step{} & \Gamma \back A \limp B & \mathsf{B} \\
      \step{} & (\Gamma \forw A) \limp B & \mathsf{R{\limp}_1} \\
      \step{} & \Gamma \land A \limp B & \mathsf{Frel} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \prftree[r][l]{$\forall R$}{$x \not\in \fv(\Gamma)$}
      {\summ[$\pi_1$]{\Gamma \seq A}}
      {\Gamma \seq \forall x. A}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \forall x. A & \\
      \step{} & \Gamma \back \forall x. A & \mathsf{B} \\
      \step{} & \forall x. (\Gamma \back A) & \mathsf{R\forall s} \\
      \step{} & \forall x. \Gamma \limp A & \mathsf{Brel} \\
      \steps{} & \forall x. \top & IH(\pi_1) \\
      \step{} & \top & \mathsf{absq}
    \end{array}
    \\\\
    \R[\exists R]
      {\summ[$\pi_1$]{\Gamma \seq \subst{A}{t}{x}}}
      {\Gamma \seq \exists x. A}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \exists x. A & \\
      \step{} & \Gamma \back \exists x. A & \mathsf{B} \\
      \step{} & \Gamma \back \subst{A}{t}{x} & \mathsf{R\exists i} \\
      \step{} & \Gamma \limp \subst{A}{t}{x} & \mathsf{Brel} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[{=}R]
      {}
      {\Gamma \seq t = t}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp t = t & \\
      \step{} & \Gamma \limp \top & \mathsf{refl} \\
      \step{} & \top & \mathsf{absr}
    \end{array}
    \\\\
    \R[\top L]
      {\summ[$\pi_1$]{\Gamma \seq C}}
      {\Gamma, \top \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land \top \limp C & \\
      \step{} & \Gamma \limp C & \mathsf{neur} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\bot L]
      {}
      {\Gamma, \bot \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land \bot \limp C & \\
      \step{} & \Gamma \land \bot \back C & \mathsf{B} \\
      \step{} & \bot \back C & \mathsf{L\land_2} \\
      \step{} & \bot \limp C & \mathsf {Brel} \\
      \step{} & \top & \mathsf{efq}
    \end{array}
    \\\\
    \R[\land L]
      {\summ[$\pi_1$]{\Gamma, A, B \seq C}}
      {\Gamma, A \land B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \land B \limp C & \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\lor L]
      {\summ[$\pi_1$]{\Gamma, A \seq C}}
      {\summ[$\pi_2$]{\Gamma, B \seq C}}
      {\Gamma, A \lor B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (A \lor B) \limp C & \\
      \step{} & \Gamma \land (A \lor B) \limp (\Gamma \land (A \lor B) \back C) & \mathsf{Bconn} \\
      \step{} & \Gamma \land \top \limp (\Gamma \land (A \lor B) \back C) & \mathsf{weak} \\
      \step{} & \Gamma \limp (\Gamma \land (A \lor B) \back C) & \mathsf{neur} \\
      \step{} & \Gamma \limp (A \lor B \back C) & \mathsf{L\land_2} \\
      \step{} & \Gamma \limp (A \back C) \land (B \limp C) & \mathsf{L\lor_1} \\
      \step{} & \Gamma \limp (A \limp C) \land (B \limp C) & \mathsf{Brel} \\
      \step{} & \Gamma \limp (\Gamma \back (A \limp C) \land (B \limp C)) & \mathsf{Bconn} \\
      \step{} & \Gamma \limp (\Gamma \back A \limp C) \land (B \limp C) & \mathsf{R\land_1} \\
      \step{} & \Gamma \limp ((\Gamma \forw A) \limp C) \land (B \limp C) & \mathsf{R{\limp}_1} \\
      \step{} & \Gamma \limp (\Gamma \land A \limp C) \land (B \limp C) & \mathsf{Frel} \\
      \steps{} & \Gamma \limp \top \land (B \limp C) & IH(\pi_1) \\
      \step{} & \Gamma \limp B \limp C & \mathsf{neul} \\
      \step{} & \Gamma \back B \limp C & \mathsf{B} \\
      \step{} & (\Gamma \forw B) \limp C & \mathsf{R{\limp}_1} \\
      \step{} & \Gamma \land B \limp C & \mathsf{Frel} \\
      \steps{} & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[{\limp}L]
      {\summ[$\pi_1$]{\Gamma, A \limp B \seq A}}
      {\summ[$\pi_2$]{\Gamma, B \seq C}}
      {\Gamma, A \limp B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (A \limp B) \limp C & \\
      \step{} & \Gamma \land \Gamma \land (A \limp B) \limp C & \mathsf{conn} \\
      \step{} & \Gamma \land (\Gamma \forw A \limp B) \limp C & \mathsf{F} \\
      \step{} & \Gamma \land ((\Gamma \back A) \limp B) \limp C & \mathsf{F{\limp}_1} \\
      \step{} & \Gamma \land ((\Gamma \limp A) \limp B) \limp C & \mathsf{Brel} \\
      \steps{} & \Gamma \land (\top \limp B) \limp C & IH(\pi_1) \\
      \step{} & \Gamma \land B \limp C & \mathsf{neul} \\
      \steps{} & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[\forall L]
      {\summ[$\pi_1$]{\Gamma, \subst{A}{t}{x} \seq C}}
      {\Gamma, \forall x. A \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (\forall x. A) \limp C & \\
      \step{} & \Gamma \land (\forall x. A) \limp (\Gamma \land (\forall x. A) \back C) & \mathsf{Bconn} \\
      \step{} & \Gamma \land (\forall x. A) \limp (\forall x. A \back C) & \mathsf{L\land_2} \\
      \step{} & \Gamma \land \top \limp (\forall x. A \back C) & \mathsf{weak} \\
      \step{} & \Gamma \limp (\forall x. A \back C) & \mathsf{neur} \\
      \step{} & \Gamma \limp (\subst{A}{t}{x} \back C) & \mathsf{L\forall i} \\
      \step{} & \Gamma \limp (\subst{A}{t}{x} \limp C) & \mathsf{Brel} \\
      \step{} & \Gamma \back (\subst{A}{t}{x} \limp C) & \mathsf{B} \\
      \step{} & (\Gamma \forw \subst{A}{t}{x}) \limp C & \mathsf{R{\limp}_1} \\
      \step{} & \Gamma \land \subst{A}{t}{x} \limp C & \mathsf{Frel} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
    \\\\
    \prftree[r][l]{$\exists L$}{$x \not\in \fv(\Gamma) \cup \fv(C)$}
      {\summ[$\pi_1$]{\Gamma, A \seq C}}
      {\Gamma, \exists x. A \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (\exists x. A) \limp C & \\
      \step{} & \Gamma \land (\exists x. A) \limp (\Gamma \land (\exists x. A) \back C) & \mathsf{Bconn} \\
      \step{} & \Gamma \land (\exists x. A) \limp (\exists x. A \back C) & \mathsf{L\land_2} \\
      \step{} & \Gamma \land \top \limp (\exists x. A \back C) & \mathsf{weak} \\
      \step{} & \Gamma \limp (\exists x. A \back C) & \mathsf{neur} \\
      \step{} & \Gamma \limp \forall x. (A \back C) & \mathsf{L\exists s} \\
      \step{} & \Gamma \limp \forall x. A \limp C & \mathsf{Brel} \\
      \step{} & \Gamma \back \forall x. A \limp C & \mathsf{B} \\
      \step{} & \forall x. (\Gamma \back A \limp C) & \mathsf{R\forall s} \\
      \step{} & \forall x. (\Gamma \forw A) \limp C & \mathsf{R{\limp}_1} \\
      \step{} & \forall x. \Gamma \land A \limp C & \mathsf{Frel} \\
      \steps{} & \forall x. \top & IH(\pi_1) \\
      \step{} & \top & \mathsf{absq}
    \end{array}
    \\\\
    \R[{=}L]
      {\summ[$\pi_1$]{\Gamma, t = u, \subst{A}{u}{x}, \subst{A}{t}{x} \seq C}}
      {\Gamma, t = u, \subst{A}{t}{x} \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land t = u \land \subst{A}{t}{x} \limp C & \\
      \step{} & \Gamma \land t = u \land (t = u \forw \subst{A}{t}{x}) \land \subst{A}{t}{x} \limp C & \mathsf{Fconn} \\
      \step{} & \Gamma \land t = u \land \subst{A}{u}{x} \land \subst{A}{t}{x} \limp C & \mathsf{F{=}_1} \\
      \steps{} & \top & IH(\pi_1)
    \end{array}
  \end{align*}
\end{proof}

\section{Related works}\labsec{sfl-rw}

\paragraph{Subformula linking under quantifiers}

Very recently, Mulder and Krebbers \sidecite{mulder_unification_2024} proposed
an improvement over both our method of subformula linking implemented in Actema,
and the method of Chaudhuri implemented in the \intro{Profint} prototype
\sidecite{DBLP:conf/cade/Chaudhuri21}. Like us, they perform \textit{a priori}
unification on the linked subformulas to determine appropriate substitutions for
instantiating quantifiers and rule out unwanted links. But their method improves
upon ours by being able to link subformulas with non-trivial quantifier
instantiation, such as the following \kl{linkage} that currently fails in
Actema\todo{Try to understand precisely why it fails, and if this invalidates
the productivity theorem.}:
$$(\forall x. P(x) \limp \exists y. \select{Q(x,y)}) \back \exists y. \exists z.
\select{Q(f(z),y)}$$ Because of the intended application of their method to
\emph{automated} theorem proving in the Iris framework for program verification
in Coq \sidecite{jung_iris_2018}, it is for now limited to \emph{\kl(dnd){backward}}
\kl{linkages}. They provide a detailed formalization in Coq that relates their method
with that of Actema and \kl{Profint}, based on \emph{linking judgments} of the
form $H \curlywedge [O]\VDash G$ that have a derivation precisely when $H \back
G \steps{} O$\sidenote{Contrary to our usage in this chapter, they use the
terminology ``linkage'' to denote derivations of these linking judgments, rather
than paths to selected subformulas.}.

\paragraph{Canonical proofs}

The idea of reducing a proof to a collection of links between its dual formulas
is not new, and can be traced back to the \emph{matings} of Andrews
\sidecite{1674698} in the context of automated deduction. Matings are
\emph{sets} of links covering all \emph{atomic} occurrences, and proofs are
matings satisfying certain conditions. Our work differs in that we are
interested in \emph{interactive} deduction, and thus consider links as a
mechanism of inference rather than a syntactic criterion to discriminate proofs.
Then a proof is better understood as a \emph{list} of links, and the atomicity
constraint is relaxed to gain expressivity, since the creation of links is
offloaded to the user instead of the search procedure.

Another line of work, starting with the \emph{proof nets} of Girard
\sidecite{girard-linear-1987}, is concerned with the more fundamental problem of
\emph{proof identity}, which requires a canonical notion of proof object
\sidecite{strasburger-problem-2019}. In the case of unit-free multiplicative
linear logic, the absence of any form of duplication/sharing/removal mechanism
allows to completely characterize a proof net by the set of its \emph{axiom}
links\sidenote{The difference with matings is that correctness of a proof
structure can be checked in \emph{polynomial} instead of exponential time.},
because of the absence of duplication. This is because adding additives or
exponentials, which can encode \kl{intuitionistic} and \kl{classical} logic, requires
additional structure to represent uses of \kl{weakening} and \kl{contraction}. The
\emph{combinatorial proofs} of Hughes
\sidecite{Hughes_2006,heijltjes_intuitionistic_2019} are examples of
polynomially-checkable proof objects exhibiting such structure, and have
recently been extended to handle \kl{first-order} \kl{classical} quantifiers
\sidecite{hughes2019firstorder} (\kl{intuitionistic} quantifiers are still an open
problem). This compartmentalization of axiom links and \kl{structural rules}
resembles the distinction between interaction phases and manual applications of
{\rsf{conn}} and {\rsf{weak}} (\reffig{reslink}), which is itself inspired by
the \emph{decomposition theorem} of the \kl{calculus of structures}
\sidecite[][Theorem~4.1.3]{tubella:hal-02390267}.

There is also an analogy between the correctness criterions of proof nets, and
the validity criterion of \kl{linkages}:
\begin{itemize}
  \item they both identify a subset of valid objects among a larger set of
  structures characterized by links on formulas;
  \item they both allow many different \emph{sequential} readings, that is
  \kl{sequent calculus} proofs for proof nets, and \kl{CoS} proofs for
  \kl{linkages}\sidenote{Note that invalid \kl{linkages} still give rise to \kl{CoS}-style
  \emph{derivations}, but not \emph{proofs} since they do not end with $\top$.
  The incorrect proof structures of Girard are in a sense more parallel as they
  cannot always be mapped to correct \kl{sequent calculus} derivations.}.
\end{itemize}
Hence, our approach to subformula linking seems to exhibit some properties of
canonical proofs, but at the level of \emph{partial proofs}: valid \kl{linkages} make
for \emph{compact-parallel-spatial} representations of inferences, whose
operational meaning is given by their \emph{detailed-sequential-temporal} \kl{CoS}
derivations.


% More
% broadly, we envision a generalization of the linking method to any set of
% subexpressions, which might or might not be equivalent or have equivalent types,
% depending on the semantics of the action under consideration. Such a generalized
% method might be dubbed \emph{``link inference''}. Subformula linking then
% corresponds to a particular kind of action, which justifies any occurrence of
% proposition given another known, unifiable proposition.

\paragraph{Tangible functional programming}

We noticed an interesting connection with the work of Conal Elliott on
tangible functional programming \sidecite{elliott-tangible}. His concept
of \emph{deep application} of \kl{$\lambda$-terms} seems related to the
notion of subformula linking, when viewing function and product types
as implications and conjunctions through the formulae-as-types
interpretation. He also devised a system of basic combinators which
are composed sequentially to compute the result of a \kl{DnD}, though it
follows a more complex dynamic than our \kl{rewriting rules}. Even if the
mapping between proofs and programs is not exact in this case, it
suggests a possible interesting field of application for the
Curry-Howard correspondance, in the realm of graphical
proving/programming environments.
