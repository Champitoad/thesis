\setchapterpreamble[u]{\margintoc}
\chapter{Subformula Linking}
\labch{sfl}

In this chapter, we engage in a thorough analysis of the logical semantics of
DnD actions, which were introduced informally through examples in \refch{pba}.
We do this mainly from the formal perspective of deep inference proof theory,
following the original work of K. Chaudhuri on his \emph{subformula linking}
paradigm \cite{Chaudhuri2013}. But we always keep in mind the intended
application to proof assistants, by motivating various design choices --- actual
or prospective --- as ways to improve the UX of interactive proof building.

The chapter is organized as follows: \refsec{linkages} introduces the notions of
context and polarity, and explains how DnD actions are specified by the user
interactively through schemas called \emph{linkages}. \refsec{validity} explains
how one can identify a subset of linkages that guarantees a \emph{productivity}
property on DnD actions. \refsec{action} describes the overall structure of how
linkages translate into logical steps, and \refsec{invert} discusses some
subtleties of this translation that are related to the concept of
\emph{focusing} in automated proof search. \refsec{soundness} shows that the
logical steps are sound, and \refsec{productivity} states and proves formally
the productivity property. Finally, \refsec{dnd-completeness} shows how DnD
actions can be turned into a complete deductive system without any need for
click actions.

\section{Linkages}\labsec{linkages}

Like most rewriting systems on terms (that is, tree-shaped data), the rewrite
rules of \reffig{DISL} and \reffig{DISL-U} apply at any depth inside formulas.
However logically, the shape of the \emph{context} in which this rewriting
occurs can provide important information, either to ensure soundness of the
performed transformation (\refsec{soundness}, \refsec{dnd-completeness}), or 
to understand the status of quantified variables (\refsec{identity}).

As is standard in term rewriting, our notion of context will correspond to that
of a (formula) tree with a distinguished leaf called its
\emph{hole}\sidenote{More precisely, our contexts correspond to what is called a
``structure'' in the first deep inference formalism to date, the \emph{Calculus
of Structures} (see \refsec{cos}).}.

\begin{definition}[Context]\label{def:context}
  A context, written $A\hole$, is a proposition containing exactly
  one occurrence of a specific propositional variable $\hole$ which is
  not used elsewhere.

  Given another proposition $B$, we write $A\select{B}$ for the
  proposition obtained by replacing $\hole$ in $A\hole$ by $B$. Note
  that this replacement is not a substitution because it allows variable
  capture. For instance $\forall x.\select{P(x)}$ is the proposition
  $\forall x.P(x)$.
\end{definition}

\begin{definition}[Path]\label{def:path}
A path is a proposition where one subformula has been
selected. Formally, a path is a pair $(A\hole, B)$ formed by one
context and one proposition:
\begin{itemize}
\item $A\hole$ is called the {\em context} of the path,
\item $B$ is called the {\em selection} of the path.
\end{itemize}

The path $(A\hole, B)$ can be viewed as the proposition
$A\select{B}$.  For readability, we will generally also write
$A\select{B}$ for the path $(A\hole, B)$.
\end{definition}

\begin{definition}[Inversions]
  Given a context $A\hole$, the number of inversions in $A\hole$,
  written $\inv(A\hole)$, is the number of subterms of $A\hole$
  which are of the form $C\hole\limp D$; that is the number of
  times the hole is on the left-hand side of an implication.
  \end{definition}
\noindent  For instance:
  $$
\begin{array}{rcl}
    \inv(D\land \hole)&=& 0\\
    \inv((D\land \hole)\limp E)&=&1\\
    \inv((\hole\limp C)\limp D) &=& 2
\end{array}
$$

\begin{definition}[Polarity of a context]\label{def:polarity}
We will write $A^+\hole$ to specify that a context is {\em positive},
meaning that $\inv(A^+\hole)$ is even. Symmetrically, $A^-\hole$ will be
used for {\em negative} contexts, meaning that $\inv(A^-\hole)$ is odd.
\end{definition}

% In demonstrating soundness, we focused solely on the two \emph{items} involved
% in a DnD action. But

In addition to the items involved, every DnD action specifies the
\emph{selection} of a subterm in each item, which can be expressed formally as a
path (Definition \ref{def:path}). We call \emph{linkage} the combined data of
the two items together with the selection, since the intent is to \emph{link}
the subterms to make them interact in some way.

\begin{remark}
In this thesis we only consider linkages between two subterms, but as noted in
\refsec{equality}, rewriting is an example of action that can benefit from
allowing multiple selections\sidenote{A restricted kind of multi-occurrence
rewrite is already available in the standalone version of Actema: one needs to
enter \emph{selection mode}, by either toggling the dedicated button (the one
with the mouse cursor in \reffig{aristote}), or holding down the \texttt{shift}
key. Then one can click successively on all occurrences of a term $t$ that are
to be rewritten, in order to add them to the selection. To perform the rewriting
to some other term $u$, the last step is to drag an equality hypothesis $t = u$
(or $u = t$) and drop it on any item holding one of the selected occurrences of
$t$.}.
\end{remark}

Each kind of DnD action is mapped in the system to a specific form of linkage,
which is designed to hold all the information necessary for the correct
execution of the action. In this way the system can automatically search for
linkages of a certain form, and propose to the user all well-defined actions
associated to these linkages.

\begin{remark}
  In the future, one can imagine several DnD actions associated to a given
  linkage. In this case, the user could be queried to choose the action to be
  performed (typically with a pop-up menu). However with the actions considered
  in this thesis, such ambiguities never arise.
\end{remark}

On the ``items axis'', we already distinguished between backward and forward
linkages, written respectively $A \back B$ and $A \forw B$. If the items are
unspecified, we will write $A \link B$.

Using the ``selection axis'', we can specify a further distinction that was
informal up to now: that of \emph{logical} action and \emph{rewrite} action.
\begin{itemize}
  \item \emph{Logical} linkages link two subformulas. Thus they have the form
  $B\select{A} \link C\select{A'}$.
  \item \emph{Rewrite} linkages link one side of an equality with a first-order
  term. Using liberally the notations from Definitions \ref{def:context} and
  \ref{def:path}, they thus have the form
  $$B\select{\select{t} = u} \link C
  \select{t'} \text{\ (or symmetrically $B\select{u = \select{t}} \link C
  \select{t'}$)}$$
  % \item \emph{Instanciation} actions correspond to specialization of universal
  % hypotheses, and witness choice for existential goals.
\end{itemize}

By forgetting the information on which subterms are selected, one can see any
linkage as a formula whose topmost connective is a linking operator $\link \in
\{\back,\forw\}$. Then it is natural to view linkages as the redexes of the
rewrite rules of \reffig{DISL}, although from the user's standpoint linkages
only happen at the top level\sidenote{In fact this is more of a limitation of
Actema's current interface: one cannot link two subterms that live in the same
item, because dragging actions can only be performed on \emph{entire} items. But
in the original formulation and implementation of subformula linking
\cite{Chaudhuri2013}, linkages can be created between arbitrary subformulas. We
come back to this issue in \refsec{dnd-completeness}.}.

\section{Validity}\labsec{validity}

\begin{marginfigure}
\begin{mathpar}
  \begin{array}{r@{\quad}c@{\quad}lr}
    {A \back B}&\step&A \limp B &\mathsf{Brel}\\
    {A \forw B}&\step&A \land B &\mathsf{Frel}
  \end{array}
\end{mathpar}
\caption{Release rules}
\labfig{rellink}
\end{marginfigure}

In the original formulation of subformula linking \sidecite{Chaudhuri2013}, a
semantics is associated to every logical linkage, even when the selected
subformulas $A$ and $A'$ are not unifiable. This is made possible by the
addition of so-called \emph{release} rules\sidenote{A terminology coming from
the line of works on \emph{focusing} in proof theory \cite{andreoli1992}, see
also \refsec{invert}.}, which simply turn linking operators into their
associated logical connective. In our setting this would give the rewrite rules
of \reffig{rellink}. However in this work we opt for a different approach:
instead we define a \emph{validity criterion} on linkages, which guarantees that
they give rise to the behaviors described in the previous sections. The
criterion tackles two issues:
\begin{itemize}
  \item \textbf{Polarity:} the selected subterms must have opposite
  \emph{polarities}, so that the \emph{negative} subterm justifies the
  \emph{positive} one;
  \item \textbf{Identity:} the selected subterms must be \emph{unifiable}, so
  that after instantiating some quantifiers in their context they can interact
  through the {\rnmsf{id}} rule or the equality rules {\rnmsf{L{=}_i},
  \rnmsf{F{=}_i}}.
\end{itemize}
One benefit of using this criterion is that it filters out all linkages whose
semantics relies on release rules, capturing intuitively a notion of
\emph{productivity}: instead of just moving around subformulas, we know for sure
that some simplification occurs, either a justification with the {\rnmsf{id}}
rule on logical linkages, or a rewriting with the equality rules on rewrite
linkages. This will be stated more formally in \refsec{productivity}.

Validity is very useful to support the \emph{suggestion} mechanism implemented
in Actema. The idea is that when the user starts dragging an item, this
indicates to the system that she wants to perform a DnD action involving
subterms of this item. Then the system can suggest such possible actions by
highlighting subterms in the goal which form a valid linkage with the dragged
item. Typically in the example of \reffig{aristote}, dragging the hypothesis
$\forall x. \human(x) \limp \mortal(x)$ will have the effect of highlighting
exactly $\mortal(\socrates)$ in the conclusion and $\human(\socrates)$ in the
other hypothesis as possible drop targets. In this case this corresponds to all
subterms in the goal which are not contained in the dragged item. But if one
were to drag the $\human(\socrates)$ hypothesis, then only the subterm
$\human(x)$ in the other hypothesis would be suggested as a drop target. This
could not work with the ``release'' semantics mentioned earlier, since then all
subterms would again be highlighted, providing no useful information to the
user.

We believe that in more complex situations, this filtering can be quite helpful
to guide the user towards the right path to follow in their reasoning. Although
non-trivial arguments are often based on ``guessing'' the right value or lemma
to be used, a large part of mathematical reasoning also consists in ``connecting
the dots'' with information already at hand. Our DnD actions capture this
metaphor quite directly, and thus shall be especially useful to beginners
unfamiliar with proving, who often show difficulties in understanding how to
build a proof from scratch. More generally, proof assistants have the potential
to provide a well-defined and rigorous methodology in the art of crafting
proofs, in the same way that we have been teaching precise algorithms for
solving equations in calculus classes for centuries. Having a graphical
interface that makes this methodology more intuitive and discoverable is the
main goal of this work, and (valid) DnD actions seem to be a good candidate as a
core principle for such a methodology.

\subsection{Polarity}\labsec{polarity}

The restrictions on polarities are captured formally by the following condition:

% \begin{condition}[Polarity]\label{cond:pol}
  
%   Given a goal $Γ \seq C$, the following must be true for a logical linkage
%   $B\select{A}\link D\select{A'}$ to be valid:
%   \begin{enumerate}
%     \item $B\select{A}\in \Gamma$;\label{clause:hyp}
%     \item $D\select{A'} \begin{cases}
%       \equiv C & \text{if $\link$ is $\back$} \\
%       \in \Gamma & \text{if $\link$ is $\forw$}
%       \end{cases}$\label{clause:concl}
%     \item $\inv(B\hole)$ has the same (resp. opposite) parity as $\inv(D\hole)$
%     if $\link$ is $\back$ (resp. $\forw$);\label{clause:opposite}
%     \item if $\link$ is $\back$ and $\inv(D\hole) = 0$, then $\inv(B\hole) =
%     0$\label{clause:intuit}.
%   \end{enumerate}

%   Similarly, the following must be true for a rewrite linkage
%   $$B\select{\select{t} = u} {\link} D\select{t'}$$ to be valid:
%   either $B\select{\select{t} = u}\in \Gamma$, then
%   \begin{enumerate}
%     \item $D\select{t'} \begin{cases}
%       \equiv C & \text{if $\link$ is $\back$} \\
%       \in \Gamma & \text{if $\link$ is $\forw$}
%       \end{cases}$
%     \item $B\hole$ is positive
%   \end{enumerate}
%   or $B\select{\select{t} = u} \equiv C$, $\link$ is $\back$ and
%   \begin{enumerate}
%     \item $D\select{t'} \in \Gamma$;
%     \item $B\hole$ is negative.
%   \end{enumerate}
% \end{condition}

% One understands that for rewrite linkages, this simply guarantees that the
% equality is in negative position in the given goal. For logical linkages,
% Clauses \ref{clause:hyp}, \ref{clause:concl}, \ref{clause:opposite} ensure that
% the selected subformulas have opposite polarities, and Clause
% \ref{clause:intuit} ensures that the linkage makes sense in our
% \emph{intuitionistic} setting.

\begin{condition}[Polarity]\label{cond:pol}
  
  The following must be true for a logical linkage
  $B\select{A}\link D\select{A'}$ to be valid:
  \begin{enumerate}
    \item the parity of $\inv(B\hole)$ is:
      \begin{enumerate}
        \item the same as $\inv(D\hole)$ if $\link = \back$
        \item the opposite of $\inv(D\hole)$ if $\link = \forw$
      \end{enumerate}\label{clause:opposite}
    \item if $\link$ is $\back$ and $\inv(D\hole) = 0$, then $\inv(B\hole) =
    0$\label{clause:intuit}.
  \end{enumerate}

  The following must be true for a rewrite linkage $B\select{t} @ D\select{t'}$
  to be valid:
  \begin{enumerate}
    \item if $B\hole$ holds the equality, then it must be:
      \begin{enumerate}
        \item positive if $\link = \back$;
        \item positive if $\link = \forw$;
      \end{enumerate}
    \item if $D\hole$ holds the equality, then it must be:
      \begin{enumerate} 
        \item negative if $\link = \back$;
        \item positive if $\link = \forw$.
      \end{enumerate}
  \end{enumerate}
\end{condition}

One understands that for rewrite linkages, this simply guarantees that the
equality is in negative position. For logical linkages, Clause
\ref{clause:opposite} ensures that the selected subformulas have opposite
polarities, and Clause \ref{clause:intuit} ensures that the linkage makes sense
in our \emph{intuitionistic} setting.

Indeed one could imagine the following behavior
in classical logic:
$$(\select{A}\limp B)\limp C\back \select{A}~~\steps~~ C\limp A$$ which gives a
proof of Peirce's law when replacing $C$ with $A$. We will come back to this
example in \refch{sfl-classical}, but for now we can just remark that there is
no way to handle it with the rules of \reffig{DISL} because we lack a rule for
redexes of the form $B\select{A} \limp C \back D\select{A'}$.

\subsection{Identity}\labsec{identity}

A context binds variables in the selected proposition. These variables
will be unifiable or not depending upon: (1) the nature of the quantifier
($\forall$ or $\exists$), (2) whether they occur in a hypothesis or a
conclusion, and (3) whether they occur on the left-hand of an (odd number
of) implication(s). Therefore, we start by splitting the list of
variables bound by a context in two parts.


\begin{definition}[Positive and negative variables]
Given a context $A\hole$ seen as a tree, one can always start from
the root and traverse the branch of $A\hole$ that leads to its hole
$\hole$. We write $\lvar(A\hole)$ the list of all variables
quantified along the way. This list is ordered, the variables closer
to the root coming first.

$\lvar(A\hole)$ can be seen as the interleaving of two sublists
$\lvarp(A\hole)$ and $\lvarn(A\hole)$ of \emph{positively} and
\emph{negatively unifiable} variables, in the following precise sense:
$x \in \lvarp(A\hole)$ (resp. $x \in \lvarn(A\hole))$ iff there
are contexts $B\hole$, $C^+\hole$ and $D^-\hole$ such that
$A\hole$ is either $C^+\select{\exists x. B\hole}$ or
$D^-\select{\forall x. B\hole}$ (resp. $D^-\select{\exists x. B\hole}$
or $C^+\select{\forall x. B\hole}$).

For instance, if $A\hole \equiv \forall x. \exists y. (B \land ((\exists x'.
\forall y'. \hole) \limp \forall z. C))$, then we have:
\begin{mathpar}
  \lvar(A\hole) = [x, y, x', y'] \and
  \lvarp(A\hole) = [y, y'] \and
  \lvarn(A\hole) = [x, x']
\end{mathpar}


% The three lists are defined recursively over the structure of $A\hole$ by the
% following equations:$$
% \begin{array}{c}
% \begin{array}{rclrcl}
%   \lvar(\hole)\eqb []&  \lvar(\forall x.A\hole) \eqb x::\lvar(A\hole)\\
%   \lvar(P(t_1, \dots , t_n)\eqb []& \qquad   \lvar(\exists x.A\hole) \eqb x::\lvar(A\hole)\\
% \end{array}\\
% \begin{array}{rcl}
%     \lvar(A\hole\vee B),  \lvar(B\vee A\hole),  \lvar(A\hole\wedge B), \lvar(B\wedge A\hole),&&\\ \lvar(A\hole\implies B), \lvar(B\implies A\hole)\eqb \lvar(A\hole)\\
%   \lvarn(\hole), \lvarp(\hole),
%   \lvarn(P(t_1, \dots , t_n)),\lvarp(P(t_1, \dots , t_n))\eqb []\\
  
%   \lvarn(\forall x.A\hole) = x::\lvarn(A\hole)&\qquad&
%   \lvarn(\exists x.A\hole) = \lvarn(A\hole)\\
%     \lvarn(A\hole\vee B),  \lvarn(B\vee A\hole),  \lvarn(A\hole\wedge B), \lvarn(B\wedge A\hole)\eqb \lvarn(A\hole)\\
%     \lvarn(A\hole\implies B)\eqb \lvarp(A\hole)\\
%       \lvarp(\forall x.A\hole) \eqb \lvarp(A\hole)\\
%   \lvarp(\exists x.A\hole \eqb x::\lvarp(A\hole)\\
%    \lvarp(A\hole\vee B),  \lvarp(B\vee A\hole),  \lvarp(A\hole\wedge B), \lvarp(B\wedge A\hole)\eqb \lvarp(A\hole)\\
%      \lvarp(A\hole\implies B)\eqb \lvarn(A\hole)\\
% \end{array}
% \end{array}
% $$
% The three lists are naturally extended to linkages as sets by taking the union
% of the lists associated with the two contexts of the linkage.
\end{definition}

% As we have seen, variables bound in linked formulas are unifiable or
% not, depending on their quantifier, the position of this quantifier,
% and whether they are in a hypothesis or the conclusion.

\begin{definition}[Unifiable variables]\label{def:uvars}
  The set $\uvars(\mathcal{L})$ of {\em unifiable variables} of a linkage
  $\mathcal{L} \equiv B\select{A}\link C\select{A'}$ is:
  \begin{itemize}
  \item $\lvarn(B\hole)\cup\lvarp(C\hole)$ if $\link$ is $\back$, and
  \item $\lvarn(B\hole)\cup\lvarn(C\hole)$ if $\link$ is $\forw$.
  \end{itemize}
\end{definition}

The following notions of substitution and unification are the usual ones and we
do not go into details:
 
\begin{definition}[Substitution]
  A \emph{substitution} is a mapping from variables to terms such that $\{x ~|~
  \sigma(x)\not\equiv x\}$ is finite; we call this set the {\em domain} of $\sigma$.

  When $\sigma(x)\equiv x$ we say that $x$ is  {\em not instantiated} by
  $\sigma$.
  
  %% A mapping $\sigma$ from variables to terms is a substitution of
  %% domain $l$ when $\forall x\notin l, \sigma(x)\equivx$. When
  %% $\sigma(x)\equivx$, we also say that $x$ is
  
  Given a proposition $A$ and a substitution $\sigma$, we write
  $\sigma(A)$ for the \emph{application} of $\sigma$ to $A$ in the usual way.
  %% For contexts, substitution in the special propositional variable
  %% $\hole$ is defined trivially by $\hole[\sigma] = \hole$.
\end{definition}

\begin{definition}[Unification] Given two propositions $A$ and $A'$ and a list of variables
  $l$, we say that a substitution $\sigma$ \emph{unifies} $A$ and $A'$ over $l$
  when $\sigma(A)\equiv\sigma(A')$ and the domain of $\sigma$ is a subset of $l$. 

  If such a substitution exists, we say that $A$ and $A'$ are \emph{unifiable}
  over $l$.
\end{definition}
Given $A$, $A'$ and $l$, the well-known unification algorithm decides
whether $A$ and $A'$ are unifiable over $l$ and constructs the
substitution when it exists~\sidecite{Montanari}.

\begin{condition}[Identity]\label{cond:unif} 
  For a linkage $B\select{A}\link C\select{A'}$ to be valid, the following must
  be true:
  \begin{enumerate}
   \item There exists a substitution $\sigma$ which unifies $A$ and
     $A'$ over the unifiable variables of the linkage.\label{clause:unif}
   \item \label{lab:cond} Furthermore, the unification respects the order over
     the variables. More precisely, we request that there exists a list $l$
     which is an interleaving of $\lvar(B\hole)$ and $\lvar(C\hole)$ such
     that, given a unifiable variable $x$ in the domain of $\sigma$, all
     variables occuring in $\sigma(x)$ are placed before $x$ in $l$:
     $$\forall y\in\fv(\sigma(x))\cap(\lvar(B\hole)\cup\lvar(C\hole)), y <_l
     x.$$\label{clause:deps}
  \end{enumerate}
\end{condition}

The last condition ensures acyclicity and will prohibit invalid
linkages as described in \refsec{acyclicity}. More precisely,
the list $l$ specifies the order in which the quantifiers will be
treated in the proof construction.


% \begin{condition}[Unification]\label{cond:unif}
%   The linked subterms ($A$ and $A'$ or $t$ and $t'$) must be \emph{unifiable}
%   with respect to the variables quantified in their contexts $B\hole$, $C\hole$.
%   We do not detail all the constraints of this unification problem, but the
%   essential idea is that a variable is unifiable if and only if its quantifier
%   is \emph{instantiable}. This in turn can be determined by the polarity of the
%   context surrounding the quantifier. One also needs to check that the unifier
%   does not create circular dependencies between variables.
% \end{condition}

Finally we can state the full validity criterion for linkages:

\begin{definition}[Valid linkage]\labdef{valid-linkage}
  We say that a linkage $\mathcal{L}$ is \emph{valid} if it satisfies Conditions
  \ref{cond:pol} and \ref{cond:unif}.
\end{definition}

One can check that all the examples given up to here were based on valid
linkages.

% \subsection{Unification}
% [à simplifier]
% % We can now describe which links are legal, that is can yield a logical
% % proof construction step.
% A context binds variables in the selected proposition. These variables
% will be unifiable or not depending upon: (1) the nature of the quantifier
% ($\forall$ or $\exists$), (2) whether they occur in a hypothesis or a
% conclusion, and (3) whether they occur on the left-hand of an (odd number
% of) implication(s). Therefore, we start by splitting the list of
% variables bound by a context in two parts.

% \begin{definition}[Positive and negative variables]
% Given a context $A\hole$ seen as a tree, one can always start from
% the root and traverse the branch of $A\hole$ that leads to its hole
% $\hole$. We write $\lvar(A\hole)$ the list of all variables
% quantified along the way. This list is ordered, the variables closer
% to the root coming first.

% $\lvar(A\hole)$ can be seen as the interleaving of two sublists
% $\lvarp(A\hole)$ and $\lvarn(A\hole)$ of \emph{positively} and
% \emph{negatively unifiable} variables, in the following precise sense:
% $x \in \lvarp(A\hole)$ (resp. $x \in \lvarn(A\hole))$ iff there
% are contexts $B\hole$, $P\hole$ and $N\hole$ such that
% $A\hole$ is either $P\select{\exists x. B\hole}$ or
% $N\select{\forall x. B\hole}$ (resp. $N\select{\exists x. B\hole}$
% or $P\select{\forall x. B\hole}$).

% For instance, if $A\hole = \forall x. \exists y. (B \land ((\exists x'. \forall
% y'. \hole) \limp \forall z. C))$, then we have:
% \begin{mathpar}
%   \lvar(A\hole) = [x, y, x', y'] \and
%   \lvarp(A\hole) = [y, y'] \and
%   \lvarn(A\hole) = [x, x']
% \end{mathpar}


% % The three lists are defined recursively over the structure of $A\hole$ by the
% % following equations:$$
% % \begin{array}{c}
% % \begin{array}{rclrcl}
% %   \lvar(\hole)\eqb []&  \lvar(\forall x.A\hole) \eqb x::\lvar(A\hole)\\
% %   \lvar(P(t_1, \dots , t_n)\eqb []& \qquad   \lvar(\exists x.A\hole) \eqb x::\lvar(A\hole)\\
% % \end{array}\\
% % \begin{array}{rcl}
% %     \lvar(A\hole\lor B),  \lvar(B\lor A\hole),  \lvar(A\hole\land B), \lvar(B\land A\hole),&&\\ \lvar(A\hole\limp B), \lvar(B\limp A\hole)\eqb \lvar(A\hole)\\
% %   \lvarn(\hole), \lvarp(\hole),
% %   \lvarn(P(t_1, \dots , t_n)),\lvarp(P(t_1, \dots , t_n))\eqb []\\
  
% %   \lvarn(\forall x.A\hole) = x::\lvarn(A\hole)&\qquad&
% %   \lvarn(\exists x.A\hole) = \lvarn(A\hole)\\
% %     \lvarn(A\hole\lor B),  \lvarn(B\lor A\hole),  \lvarn(A\hole\land B), \lvarn(B\land A\hole)\eqb \lvarn(A\hole)\\
% %     \lvarn(A\hole\limp B)\eqb \lvarp(A\hole)\\
% %       \lvarp(\forall x.A\hole) \eqb \lvarp(A\hole)\\
% %   \lvarp(\exists x.A\hole \eqb x::\lvarp(A\hole)\\
% %    \lvarp(A\hole\lor B),  \lvarp(B\lor A\hole),  \lvarp(A\hole\land B), \lvarp(B\land A\hole)\eqb \lvarp(A\hole)\\
% %      \lvarp(A\hole\limp B)\eqb \lvarn(A\hole)\\
% % \end{array}
% % \end{array}
% % $$
% % The three lists are naturally extended to linkages as sets by taking the union
% % of the lists associated with the two contexts of the linkage.
% \end{definition}

% As we have seen, variables bound in linked formulas are unifiable or
% not, depending on their quantifier, the position of this quantifier,
% and whether they are in a hypothesis or the conclusion.

% \begin{definition}\label{def:uvars}
%   The set $\uvars(\mathcal{L})$ of {\em unifiable variables} of a linkage $\mathcal{L}$ is:
%   \begin{itemize}
%   \item $\lvarn(B\hole)\cup\lvarp(C\hole)$ if $\mathcal{L} = B\select{A}\back C\select{A'}$, and
%   \item $\lvarn(B\hole)\cup\lvarn(C\hole)$ if $\mathcal{L} = B\select{A}* C\select{A'}$.
%   \end{itemize}
% \end{definition}

% The following notions of substitutions and unification are the usual
% ones and we do not go into detail:
 
% \begin{definition}
%   A \emph{substitution} is a mapping from variables to terms such that $\{x ~|~
%   \sigma(x)\neq x\}$ is finite; we call this set the {\em domain} of $\sigma$.

%   When $\sigma(x)=x$ we say that $x$ is  {\em not instantiated} by
%   $\sigma$.
  
%   %% A mapping $\sigma$ from variables to terms is a substitution of
%   %% domain $l$ when $\forall x\notin l, \sigma(x)=x$. When
%   %% $\sigma(x)=x$, we also say that $x$ is
  
%   Given a proposition $A$ and a substitution $\sigma$, we write
%   $A[\sigma]$ for the \emph{application} of $\sigma$ to $A$ in the usual way.
%   %% For contexts, substitution in the special propositional variable
%   %% $\hole$ is defined trivially by $\hole[\sigma] = \hole$.
% \end{definition}

% \begin{definition} Given two propositions $A$ and $A'$ and a list of variables
%   $l$, we say that a substitution $\sigma$ \emph{unifies} $A$ and $A'$ over $l$
%   when $A[\sigma]=A'[\sigma]$ and the domain of $\sigma$ is a subset of $l$. 

%   If such a substitution exists, we say that $A$ and $A'$ are \emph{unifiable}
%   over $l$.
% \end{definition}
% Given $A$, $A'$ and $l$, the well-known unification algorithm decides
% whether $A$ and $A'$ are unifiable over $l$ and constructs the
% substitution when it exists~\cite{Montanari}.


% \begin{definition}[Valid Linkage]\label{def:valid-linkage} 
%   A linkage $B\select{A}\link C\select{A'}$ is valid if and only if the
%   following conditions hold:
%   \begin{enumerate}
%   \item The numbers of inversions in $B\hole$ and $C\hole$ verify
%     condition~\ref{cond:pol}.
%    \item There exists a substitution $\sigma$ which unifies $A$ and
%      $A'$ over the unifiable variables of the linkage.
%    \item \label{lab:cond} Furthermore, the unification respects the order over
%      the variables. More precisely, we request that there exists a list $l$
%      which is an interleaving of $\lvar(B\hole)$ and $\lvar(C\hole)$ such
%      that, given a unifiable variable $x$ in the domain of $\sigma$, all
%      variables occuring in $\sigma(x)$ are placed before $x$ in $l$:
%      $$\forall
%      y\in\FV(\sigma(x))\cap(\lvar(B\hole)\cup\lvar(C\hole)), y <_l x.$$
%      \end{enumerate}
% \end{definition}

% The last condition ensures acyclicity and will prohibit invalid
% linkages as described in section~\refsec{acyclicity}. More precisely,
% the list $l$ specifies the order in which the quantifiers will be
% treated in the proof construction.


% \begin{definition}[Linkage]
%   A linkage is given by a pair of paths. More precisely, we
%   distinguish between two cases; a linkage is either:
%   \begin{itemize}
%   \item a {\em forward} linkage, written $B\select{A} * C\select{A'}$,
%   \item a {\em backward} linkage, written $B\select{A} \vdash C\select{A'}$.
%   \end{itemize}
%   We will use letters $\mathcal{L}, \mathcal{L'}\dots$ to range over linkages, and may write $B\select{A} \link C\select{A'}$ to denote a linkage which is either backward or forward.
% \end{definition}

% \begin{definition}[Holes]
%   Given an integer $i > 0$, one can form a special proposition $\hole_i$ called
%   a \emph{propositional hole}, as well as a special first-order term $\fhole_i$
%   called a \emph{first-order hole}.
% \end{definition}

% \begin{definition}[Propositional contexts]
%   A \emph{propositional context}, written $A\hole$, is a proposition containing
%   at least one hole, and at most one occurrence of $\hole_i$ and $\fhole_i$ for
%   any $i > 0$.

%   Given propositions $B_1$, \ldots, $B_n$ and first-order terms $u_1$, \ldots,
%   $u_m$, we write $A\select{B_1, \ldots, B_n}\select{u_1, \ldots, u_m}$ for the
%   proposition obtained by replacing $\hole_i$ by $B_i$ and $\fhole_j$ by $u_j$
%   in $A\hole$, for $1 \leqslant i \leqslant n$ and $1 \leqslant j \leqslant m$.
%   Note that this replacement is not a substitution because it allows variable
%   capture. For instance $\forall x.\select{P(x)}$ is the proposition $\forall
%   x.P(x)$.
% \end{definition}

% \begin{definition}[First-order contexts]
%   A \emph{first-order context}, written $t\fhole$, is a first-order term
%   containing at least one first-order hole, and at most one occurrence of
%   $\fhole_i$ for any $i > 0$.

%   Given first-order terms $u_1, \ldots, u_m$, we write $t\select{u_1, \ldots,
%   u_m}$ for the first-order term obtained by replacing $\fhole_j$ by $u_j$ in
%   $t\fhole$, for $1 \leqslant j \leqslant m$.
% \end{definition}

% \begin{definition}[Contexts]
%   General \emph{contexts}, denoted by the letters $\chi, \xi, \zeta$, are the
%   (disjoint) union of propositional and first-order contexts. Operations of
%   context filling are lifted to general contexts in the natural way.
% \end{definition}

% \begin{definition}[Path]
% A path is a term where subterms have been selected. Formally, a path is a
% triplet $(\chi, \ell_B, \ell_u)$ formed by a context, a list $\ell_B$ of
% propositions and a list $\ell_u$ of first-order terms:
% \begin{itemize}
% \item $\chi$ is called the {\em context} of the path,
% \item $\ell_B$ and $\ell_u$ are called the {\em selection} of the path.
% \end{itemize}

% The path $(\chi, \ell_B, \ell_u)$ can be viewed as the term
% $\chi\select{\ell_B}\select{\ell_u}$. For readability, we will generally also
% write $\chi\select{\ell_B}\select{\ell_u}$ for the path $(\chi, \ell_B,
% \ell_u)$.
% \end{definition}


\section{Describing DnD Actions}\labsec{action}

We are now equipped to specify how logical and rewrite linkages translate
deterministically to the backward and forward proof steps shown in all examples.

First some remarks can be made about the rewrite rules of \reffig{DISL}:
\begin{itemize}
\item The set of rewrite rules is obviously non-confluent. 
\item It is also terminating, because the number of connectives or quantifiers
  under $\forw$ or $\back$ decreases\sidenote{Except for the \textsf{Fcomm} rule
  which is just meant to make the $\forw$ operator commutative; formally, the
  only infinite reduction paths end with an infinite iteration of
  \textsf{Fcomm}.}.
\end{itemize}

As for the rules of \reffig{DISL-U}, they are both terminating
\emph{and} confluent. Indeed they define a function that eliminates redundant
occurrences of the units $\top$ and $\bot$.

Here is a high-level overview of the complete procedure followed to generate a
proof step:
\begin{enumerate}
\item \textbf{Selection:} the user selects two subterms in two items of the current goal; \label{step:selection}
\item \textbf{Linkage:} this either gives rise to a logical linkage $B\select{A}
  \link C\select{A'}$ (resp. a rewrite linkage $B\select{\select{t} = u} \link
  C\select{t'}$), or does not correspond to a known form of linkage. In this case
  the procedure stops here, and the system does not propose any action to the
  user; \label{step:linkage}
\item \textbf{Validity:} the system verifies that the linkage is \emph{valid},
  by performing successively the following checks:
  \begin{enumerate}
    \item \textbf{Polarity:} the linkage must satisfy Condition \ref{cond:pol};
    \item \textbf{Unification:} the selected subterms $A$ and $A'$ (resp. $t$
    and $t'$) must be unifiable, yielding a substitution $\sigma$;
    \item \textbf{Dependencies:} the substitution $\sigma$ must satisfy
    Condition \ref{cond:unif}.
  \end{enumerate}
  The procedure stops if it fails at any of the above checks;
  \label{step:validity}
\item \textbf{Linking:} the system then chooses a rewriting start\-ing from the
  linkage. Thanks to \refthm{productivity}, this re\-writing always ends with a
  proposition of the form $D\select{\sigma(A) \back \sigma(A')}$ $$\text{(resp.
  $D\select{\select{\sigma(t)} = u \link C_0\select{\sigma(t')}}$);}$$
  \label{step:linking}
\item \textbf{Interaction:} thus one can apply the {\rnmsf{id}} rule (resp. an equality rule in
$\{\mathsf{L\!\!=\!\!_1}, \mathsf{L\!\!=\!\!_2}, \mathsf{F\!\!=\!\!_1},
\mathsf{F\!\!=\!\!_2}\}$); \label{step:interaction}
\item \textbf{Unit elimination:} in the case of a logical action, this creates an occurrence of $\top$,
which is eliminated using the rules of \reffig{DISL-U}; \label{step:unit-elimination}
\item \textbf{Goal modification:} the two previous steps produced a formula $E$.
  In the case of a forward linkage, a hypothesis $E$ is added to the goal; in
  the case of a backward linkage, the goal's conclusion becomes $E$. In both
  cases, the logical soundness is guaranteed by
  Property~\ref{prop:soundness}. \label{step:goal-modification}
\end{enumerate}

%% =======




%% We will use a similar approach, but with logical rules inspired by
%% deep inference. More precisely, our rules deal with formulas instead
%% of sequents.
%% \begin{itemize}
%%   \item The formulas are of the form $A\select{B\vdash C}$ of
%%     $A\select{B * C}$,
%%   \item in the first case, $A\hole$ is a positive context (that is
%%     $\inv(A\hole)$ is even, in the second case, it is a negative
%%     context ($\inv(A\hole)$ is odd),
%%   \item logically, $A\select{B\vdash C}$ is equivalent to
%%     $A\select{B\limp C}$ and $A\select{B*C}$ is equivalent to
%%     $A\select{B\land C}$.
%% \end{itemize}
%% For conciseness, the context is omitted in the writing of the logical
%% rules. For instance the rule ... stands for:
%% $$
%%   \Rsf{L \land_1}
%%         {D\select{C \limp \select{B \vdash A}}}
%%         {D\select{(B \land C) \vdash A}}
%% $$
%% The proof-by-linking process is as follows:
%% \begin{enumerate}
%% \item The user selects two paths in two items of the current goal,
%% \item this gives rise to a linkage which is either forward linkage
%%   $B\select{A}*C\select{A'}$ or
%%   a backward linkage $B\select{A}\vdash C\select{A'}$,
%%   depending upon whether the conclusion is among the
%%   involved items or not,
%% \item the system unifies the selected formulas which yields a
%%   substitution $\sigma$,
%% \item the system then builds a proof derivation using the rules, whose
%%  conclusion is the original linkage, that is either $B\select{A}*C\select{A'}$ or $B\select{A}\vdash C\select{A'}$.
%% \end{enumerate}
%% There is one single leaf at the top of this derivation; let us call
%% this formula $D$.
%% \begin{itemize}
%%  \item  In the case of a backward linkage, the derivation is of the
%%   form :
%%   $$\Rsf{}{D}
%%   {\Rsf{}{\dots}{B\select{A}\vdash C\select{A'}}}
%%   $$
%%   Then, the conclusion of the goal is replaced by $D$.
%%  \item   In the case of a forward linkage, the derivation is of the
%%   form:
%%   $$\Rsf{}{D}
%%   {\Rsf{}{\dots}{B\select{A}* C\select{A'}}}
%%   $$
%%   Then, a new hypothesis $D$ is added to the goal.
%% \end{itemize}

\section{Soundness}\labsec{soundness}


All examples up to now followed the scheme for DnD actions sketched in
\refsec{aristote}:
\begin{itemize}
  \item Given a blue item $A$ and a red item $B$, backward proof steps produce a
  new conclusion $C$ by applying a sequence of rewrite rules $A \back B \steps
  C$.
  \item Given two blue items $A$ and $B$, forward proof steps produce a new
  hypothesis $C$ by applying a sequence of rewrite rules $A \forw B \steps C$.
\end{itemize}

Thus for such actions to be logically sound, we have to make sure that our
rewrite system satisfies the following property:

\begin{theorem}[Soundness]\label{prop:soundness}
  \phantom{a}
  \begin{itemize}
    \item If $A \back B \steps C$, then $A, C \seq B$.
    \item If $A \forw B \steps C$, then $A, B \seq C$.
  \end{itemize}
\end{theorem}

The following simple covariance and contravariance property will be used
extensively later on:
\begin{lemma}[Variance]\label{prop:cov}
  If $\, \Gamma, A\seq B$, then $\, \Gamma, C^+\select{A}\seq C^+\select{B}$
  and $\, \Gamma, D^-\select{B}\seq D^-\select{A}$.
\end{lemma}
\begin{proof}
  By induction on the depths of $C^+\hole$ and $D^-\hole$.
\end{proof}

For each rule, interpreting $\back$ as $\limp$ and $\forw$ as $\land$ is enough
to show that the rule satisfies Property \ref{prop:soundness} locally. Formally,
we can define a mapping from formulas containing linking operators to usual
formulas where they have been replaced by their interpretation:

\begin{definition}[Interpretation of linking operators]
  The mapping $\lint{\cdot}$ is defined inductively as follows:
  \begin{align*}
    \lint{A \back B} &= \lint{A} \limp \lint{B} & \\
    \lint{A \forw B} &= \lint{A} \land \lint{B} & \\
    \lint{A \mcirc B} &= \lint{A} \mcirc \lint{B} &\text{for $\mcirc \in \{\land, \lor, \limp\}$} \\
    \lint{\mdiam x. A} &= \mdiam x. \lint{A} &\text{for $\mdiam \in \{\forall, \exists\}$} \\
    \lint{\dagger} &= \dagger &\text{for $\dagger \in \{\top, \bot\}$} \\
    \lint{a} &= a &\text{for $a$ atomic} 
  \end{align*}
\end{definition}

For rewritings taking place deeper inside a proposition however, we need to
consider the polarity of their context.

% Using the notation of definition~\ref{def:polarity}, we can state:

\begin{lemma}[Local soundness]\label{lemma:rules-valid-in-context}
  \phantom{a}
  \begin{itemize}
    \item If $C^+\select{A\back B}\step D$ then $\lint{D} \seq C^+\select{A\limp B}$.
    \item If $C^-\select{A\back B} \step D$ then $C^-\select{A\limp B}\seq \lint{D}$.
    \item If $C^+\select{A \forw B} \step D$ then $ C^+\select{A \land B}\seq \lint{D}$.
    \item If $C^-\select{A \forw B} \step D$ then $\lint{D} \seq C^-\select{A \land B}$.
  \end{itemize}
\end{lemma}
\begin{proof}
  First notice that $D$ is necessarily of the form $C\select{D_0}$ where $A
  \link B \step D_0$\sidenote{Indeed an implicit assumption in this section,
  which is preserved by all the rules, is that a formula contains at most one
  linking operator. Thus if $C\select{A \link B} \step D$, the only possible
  redex is $A \link B$.}. Then by careful analysis of each rule, it is
  straightforward to show that $\lint{D_0} \seq A \limp B$ if $\link = \back$ or
  $A \land B \seq \lint{D_0}$ if $\link = \forw$. We can conclude in each case
  by applying Lemma \ref{prop:cov} with $C\hole$ accordingly.
\end{proof}
\begin{remark}
  For some rules, like \rnmsf{R\!\!\limp_1}, the left-hand and
  right-hand propositions are equivalent:
  $$A \limp B \limp C ~~~\Longleftrightarrow~~~ A \land B \limp C$$ Such rules are
  called {\em invertible} and their names are tagged by *. This point will be
  relevant in \refsec{invert}.
\end{remark}


An easy but important technical point is that rewrite rules preserve the
polarity of contexts around redexes, in the following precise sense:
\begin{fact}[Polarity preservation]\label{prop:rules-preserve-polarity}
  \phantom{a}
  \begin{itemize}
    \item
      If $C\select{A\back B} \step C'\select{A'\back B'}$
      (resp. $C\select{A \forw B} \step $ $ C' \allowbreak\select{A' \forw B'}$) then
      $C\hole$ and $C'\hole$ have the same polarity.
    \item
      If $C\select{A\back B} \step C'\select{A'\forw B'}$ (resp.
      $C\select{A\forw B} \step C'\select{A'\back B'}$) then $C\hole$ and
      $C'\hole$ have opposite polarities.
  \end{itemize}
\end{fact}

Combining Lemma \ref{lemma:rules-valid-in-context} and Fact
\ref{prop:rules-preserve-polarity}, we obtain the central soundness result
about the rewrite rules:
\begin{lemma}[Contextual soundness]\label{lemma:rewriting-valid-in-context}
  \phantom{a}
  \begin{itemize}
    \item If $C^+\select{A\back B}\steps D$ then $\lint{D} \seq C^+\select{A\limp B}$.
    \item If $C^-\select{A\back B} \steps D$ then $C^-\select{A\limp B}\seq \lint{D}$.
    \item If $C^+\select{A \forw B} \steps D$ then $ C^+\select{A \land B}\seq \lint{D}$.
    \item If $C^-\select{A \forw B} \steps D$ then $\lint{D} \seq C^-\select{A \land B}$.
  \end{itemize}
\end{lemma}
\begin{proof}
  By induction on the length of the derivation. The base case is trivial by
  reflexivity of entailment. We give the proof for the first statement in the
  list, other cases work similarly. We can assume without loss of generality
  that the derivation has the following shape:
  $$C^+\select{A \back B} \step C'\select{A' \link B'} \steps D$$
  Then we reason by case on the linking operator $\link$:
  \begin{itemize}
    \item $\link = \back$: by Fact \ref{prop:rules-preserve-polarity}, $C'$ must
    be positive. Therefore by induction hypothesis $\lint{D} \seq C'\select{A'
    \limp B'}$. By Lemma \ref{lemma:rules-valid-in-context} we have
    $C'\select{A' \limp B'} \seq C^+\select{A \limp B}$. Thus by transitivity
    $\lint{D} \seq C^+\select{A \limp B}$.
    \item $\link = \forw$: by Fact \ref{prop:rules-preserve-polarity}, $C'$ must
    be negative. Therefore by induction hypothesis $\lint{D} \seq C'\select{A'
    \land B'}$. By Lemma \ref{lemma:rules-valid-in-context} we have
    $C'\select{A' \land B'} \seq C^+\select{A \limp B}$. Thus by transitivity
    $\lint{D} \seq C^+\select{A \limp B}$.
  \end{itemize}
\end{proof}

% We do not detail the proof here, but it relies crucially on the covariance and
% contravariance property \ref{prop:cov}.

Finally, soundness (Theorem \ref{prop:soundness}) is obtained as the special
case where the rewriting starts in the (positive) empty context.

% Detailed proofs of all the lemmas can be found in \refsec{app:dnd-soundness}.

% In the previous chapter, we showed how to associate a logical behaviour to
% drag-and-drop actions involving two subterms in a sequent, by following a set of
% rewrite rules on formulas. While we proved the soundness of these rules and a
% so-called \emph{productivity} property that makes them suitable for interactive
% proof exploration, we left untreated a central question when doing proof theory:
% \emph{completeness}. That is, can we prove any true statement using only these
% rewrite rules? The answer is \emph{yes}, and \refsec{sfl-completeness} gives 

% \section{Intuitionistic Completeness}\labsec{sfl-completeness}

% \section{Classical Logic}\labsec{sfl-classical}


\section{Productivity}\labsec{productivity}

An important property of the linking step \ref{step:linking} is that there is
always a rewriting sequence that brings together the selected subterms, which
ensures that we can proceed to the interaction step \ref{step:interaction}.
% Thus linking can be seen as a generalization of the axiom rule, where we relax
% the requirement that both formulas be syntactically equal and at the
% top-level.

%% This process of evidence-providing appears as a good measure of progress in the
%% construction of a proof. In particular, it is stronger than the application of
%% basic introduction rules such as those performed by PbP, since those only
%% capture the semantics of logical connectives, i.e. how they shape the
%% superficial structure of the proof. Thus we call this property
%% \emph{productivity}, and provide in the following an outline of its proof.

%% \begin{definition}[Size]
%% The \emph{size} of a context $A\hole$ is the positive integer
%% $\size{A\hole}$ corresponding to the depth at which the hole $\hole$ occurs
%% in $A\hole$.

%% The \emph{size} of a linkage $\mathcal{L} = B\select{A} \link C\select{A'}$ is
%% defined as $\size{\mathcal{L}} = \size{B\hole} + \size{C\hole}$.
%% \end{definition}

%% \begin{lemma}[Decreasing]\label{thm:decreasing}
%%   If $\mathcal{L} \step C\select{\mathcal{L'}}$, then $\size{\mathcal{L}} > \size{\mathcal{L'}}$.
%% \end{lemma}
%% \begin{proof}
%%   By straightforward inspection of all rules except \rnmsf{id}.
%% \end{proof}

Because the rewrite rules are terminating, the important point is to show that
one can always apply a rule until one reaches an interaction rule on the
selected subterms. In other words, it is possible to find at least one rule
which preserves Conditions \ref{cond:pol} and \ref{cond:unif} on linkages:

\begin{lemma}[Valid Progress]\label{thm:vprogress} If a linkage $\mathcal{L} \equiv
  C\select{A} \link C'\select{A'}$ (resp. $C\select{t} \link C'\select{t'}$) is
  valid, then either:
  \begin{enumerate}
    \item $\mathcal{L} \equiv \select{A} \back \select{A}$ (resp. $C\hole \in \{\hole
    = u, u = \hole\}$ for some $u$ and $t \equiv t'$);
    % \item $\mathcal{L} \in \{
    %     \select{A}\back \select{A},\,
    %     \select{t} = u \back C\select{t},\,
    %     u = \select{t} \back C\select{t},\, 
    %     \select{t} = u \forw C\select{t},\,
    %     u = \select{t} \forw C\select{t}
    %   \}$ for some $A, C\hole, t, u$;
    \item or $\mathcal{L} \step E\select{\mathcal{L'}}$ for some $E\hole,
      \mathcal{L'}$ with $\mathcal{L'}$ valid.
      % and
      % $$\mathcal{L'} \equiv D\select{\sigma(A)} \link' D'\select{\sigma(A')}$$ (resp.
      % $D\select{\sigma(t)} \link' D'\select{\sigma(t')}$) for some $D\hole,
      % D'\hole$, linking operator $\link'$ and unifying substitution $\sigma$.
  \end{enumerate}
\end{lemma}

A detailed proof is given hereafter for the case of logical linkages. It is not
fundamentally difficult, but understandably verbose. The two main points are:
\begin{itemize}
\item The rules involving a connective always preserve validity.
\item When one can apply a rule involving a quantifier $\forall x$ (resp.
  $\exists x$), one checks whether the substitution instantiates $x$ or not. In
  the first case one performs the instantiation rule \rnmsf{L\forall i} or
  \rnmsf{F\forall i} (resp. \rnmsf{R\exists i}); in the second case the
  corresponding switch rule in {\small $\{\mathsf{L\forall s}, \mathsf{R\forall
  s}, \mathsf{F\forall s}\}$} (resp. {\small $\{\mathsf{L\exists s},
  \mathsf{R\exists s}, \mathsf{F\exists s}\}$}).
\end{itemize}
\begin{proof}
  Let $\mathcal{L} \equiv B\select{A} \link C\select{A'}$ be a valid linkage.\\
  \begin{enumerate}
    \setlength{\itemsep}{1em}
    \item Suppose $B\hole \equiv C\hole \equiv \square$. By
    Condition~\ref{cond:pol}, we know that a forward linkage cannot verify
    $(\inv(B\square), \inv(C\square)) = (0,0)$, thus $\mathcal{L}$ must be a
    backward linkage. Also $\lvar(B\square)$ and $\lvar(C\square)$ are empty,
    hence by Condition~\ref{cond:unif} $A$ and $A'$ are unified by an empty
    substitution, which entails that $A \equiv A'$. Therefore we are in the
    first case where $\mathcal{L} \equiv \select{A} \back \select{A}$.

    \item Otherwise, either $B\hole$ or $C\hole$ is non-empty. In the following,
    we show that we can always apply a rewrite rule that produces a new, valid
    linkage $\mathcal{L'} \equiv B' \link' C'$.
    
    Let $\sigma$ and $\lvar$ be respectively the substitution and interleaving
    of the quantified variables of $B\square$ and $C\square$ given by Condition
    \ref{cond:unif}, with $\lvar$ decomposed as $x \Colon \lvar'$.
    
    \begin{itemize}
      \item If $x$ is quantified at the head of either $B\square$ or $C\square$,
        then we apply the associated quantifier rule:

        \begin{description}
          \item[Switch rule (\rnmsf{L\forall s}, \rnmsf{L\exists s},
          \rnmsf{R\forall s}, \rnmsf{R\exists s}, \rnmsf{F\forall s},
          \rnmsf{F\exists s})] Only if $x$ is not in the domain of $\sigma$. In
          forward mode and when $B\square$ binds $x$, one must first apply the
          rule \rnmsf{Fcomm} to put $B\select{A}$ on the right of $\forw$, so
          that the switch rule is applicable. Now we show that $\mathcal{L'}$ is
          valid:

          \begin{enumerate}
            \setlength{\itemsep}{0.8em}
            \renewcommand{\labelenumii}{\theenumii}
            \renewcommand{\theenumii}{\arabic{enumii}.}

            \item $\mathcal{L'}$ satisfies Condition \ref{cond:pol} trivially
            since none of the switch rules changes the number of inversions.

            \item For each switch rule we can show, using the fact that $x$ is
            not in the domain of $\sigma$, that $\uvars(\mathcal{L'}) =
            \uvars(\mathcal{L})$. Since the selected formulas $A$ and $A'$ stay
            untouched by the rule, we can choose $\sigma$ as a valid unifier
            that ranges over $\uvars(\mathcal{L'})$.
            
            \item In all switch rules, we have $\lvar(\mathcal{L'}) = \lvar'$
            because the quantifier of $x$ is moved in the outer context of the
            linkage. Thus we can just take $\lvar'$ as interleaving, and
            Condition \ref{cond:unif} will still be verified because $\lvar'$ is
            a sublist of $\lvar$.
          \end{enumerate}

          \item[Instantiation rule (\rnmsf{L\forall i}, \rnmsf{R\exists i},
          \rnmsf{F\forall i})] Only if $x$ is instantiated by $\sigma$, using
          $\sigma(x)$ as witness. Again one might need to apply \rnmsf{Fcomm}
          first. Then we check the validity of $\mathcal{L'}$:

          \begin{enumerate}
            \setlength{\itemsep}{0.8em}
            \renewcommand{\labelenumii}{\theenumii}
            \renewcommand{\theenumii}{\arabic{enumii}.}
            
            \item $\mathcal{L'}$ satisfies Condition \ref{cond:pol} trivially
            since none of the instantiation rules changes the number of
            inversions.

            \item For each instantiation rule we can show, using the fact that
            $x$ is instantiated by $\sigma$, that $\uvars(\mathcal{L'}) =
            \uvars(\mathcal{L}) \setminus \{x\}$. Then we take as unifier
            $\sigma$ where the binding for $x$ is removed, written $\sigma
            \setminus x$.\\

            Now we need to make sure that $\sigma \setminus x$ is indeed a
            unifier for the selected formulas. We consider only the case where
            $B\square$ binds $x$, the proof being exactly symmetrical when
            $C\square$ binds $x$. Let $B_0\square$ be the direct subcontext of
            $B\square$, that is $B\square$ without the head quantifier binding
            $x$. \\

            First we can assert that $\subst{B_0\select{A}}{\sigma(x)}{x} \equiv
            \subst{B_0}{\sigma(x)}{x}\select{\subst{A}{\sigma(x)}{x}}$. Indeed,
            Clause \ref{clause:deps} of Condition \ref{cond:unif} guarantees
            that for any free variable $y$ of $\sigma(x)$, $y \not\in
            \lvar(B_0\square)$, and thus the above instantiation can propagate
            safely to $A$ without capture. To convince yourself that $y \not\in
            \lvar(B_0\square)$, suppose the contrary. Then $y \in
            \lvar(B\square)$, and by Clause \ref{clause:deps} $y$ must be placed
            before $x$ in $\lvar$. But this is impossible since $x$ is the first
            element of $\lvar$!\\
            
            So we know that the selected formula on the left of $\mathcal{L'}$
            is $\subst{A}{\sigma(x)}{x}$, while it is still $A'$ on the right.
            Thus it only remains to show that
            $$\subst{A}{\sigma(x)}{x}[\sigma \setminus x] \equiv A'[\sigma
            \setminus x].$$ On the left we have by definition that
            $\subst{A}{\sigma(x)}{x}[\sigma \setminus x] \equiv A[\sigma]$, and
            on the right we have $A'[\sigma \setminus x] \equiv A'[\sigma]$
            because $x$ cannot occur in $A'$ since it is bound in $B_0\square$
            (here we rely on the Barendregt convention).

            \item In all instantiation rules, we have $\lvar(\mathcal{L'}) =
            \lvar'$ because the quantifier of $x$ is removed by the
            instantiation. Thus we can again take $\lvar'$ as interleaving.
          \end{enumerate}
          
        \end{description}

      \item If $x$ is not quantified at the head of $B\hole$ or $C\hole$, then
      either both heads are propositional connectives, or one is a propositional
      connective and the other is empty. In both cases we can choose either a
      rule of the form {\rnmsf{L\mcirc_i}}, {\rnmsf{R\mcirc_i}} or
      {\rnmsf{F\mcirc_i}}, where $\mcirc$ is the connective and $i$ the index of
      the direct subcontext where $A$ or $A'$ occurs, or the {\rnmsf{Fcomm}}
      rule. Again we check the conditions of Definition \ref{def:valid-linkage}:
      
      \begin{enumerate}
        \setlength{\itemsep}{0.8em}
        \renewcommand{\labelenumii}{\theenumii}
        \renewcommand{\theenumii}{\arabic{enumii}.}
            
        \item In most rules the number of inversions stays unchanged. The only
        exceptions are \rnmsf{R\!\!\limp_1} and \rnmsf{F\!\!\limp_1}, which
        decrease the number of inversions of the right context $C\hole$ by $1$.
        But since they are also the only rules that change the linking operator,
        the truth of Clause \ref{clause:opposite} is preserved: if the parities
        were opposite (resp. identical) in $\mathcal{L}$, then $\mathcal{L}$
        must be forward (resp. backward). Thus $\mathcal{L'}$ is necessarily
        backward (resp. forward), and so the parities in $\mathcal{L'}$ must be
        identical (resp. opposite), which is the case thanks to the inversion
        decrement.

        For Clause \ref{clause:intuit}, we can distinguish two cases:
        \begin{itemize}
          \item If $\mathcal{L}$ is backward, then either we apply the
          {\rnmsf{R{\limp}_1}} rule and $\mathcal{L'}$ is forward, and thus
          satisfies Clause \ref{clause:intuit} trivially; or we apply another
          backward rule and $\mathcal{L'}$ is backward. Now suppose
          $\inv(C'\hole) = 0$. Then we must have $\inv(C\hole) = \inv(C'\hole) =
          0$ and $\inv(B\hole) = \inv(B'\hole)$ since all backward rules other
          than {\rnmsf{R{\limp}_1}} preserve the number of inversions. And
          because $\mathcal{L}$ satisfies Clause \ref{clause:intuit} by
          validity, we can deduce that $\inv(B\hole) = 0$, and thus
          $\inv(B'\hole) = 0$.
          \item If $\mathcal{L}$ is forward, then either we apply a forward rule
          that is neither {\rnmsf{F{\limp}_1}} nor {\rnmsf{Fcomm}} and
          $\mathcal{L'}$ is forward, and thus satisfies Clause
          \ref{clause:intuit} trivially; or we consider applying either
          {\rnmsf{F{\limp}_1}} of {\rnmsf{Fcomm}}. There are three cases:
          \begin{itemize}
            \item If $\inv(C\hole) > 1$, then we can safely apply
            {\rnmsf{F{\limp}_1}} since we have $\inv(C'\hole) = \inv(C\hole) - 1 >
            0$;
            \item If $\inv(C\hole) = 0$, then $C\hole$ is empty and we are
            forced to apply {\rnmsf{Fcomm}} so that we can apply the forward
            rule corresponding to the head connective of $B\hole$. Then $C\hole$
            ends up on the left of $\forw$, thus if we apply
            {\rnmsf{F{\limp}_1}} for $B\hole$ Clause \ref{clause:intuit} will be
            satisfied trivially;
            \item If $\inv(C\hole = 1)$, then either $\inv(B\hole) = 0$ and we
            can safely apply {\rnmsf{F{\limp}_1}} since $\inv(B'\hole) =
            \inv(B\hole)$; or $\inv(B\hole) > 0$, and we cannot apply
            {\rnmsf{F{\limp}_1}} because we would end up with $\inv(C'\hole) =
            0$ and $\inv(B'\hole) > 0$, thus violating Clause
            \ref{clause:intuit}. Hence as in the previous case, we need to apply
            {\rnmsf{Fcomm}} first. Then it cannot be the case that $\inv(B\hole)
            = 1$ because we would have $\inv(B\hole) = \inv(C\hole)$, which
            violates Clause \ref{clause:opposite} from the validity of
            $\mathcal{L}$. Thus $\inv(B\hole) > 1$, which entails that we can
            safely apply {\rnmsf{F{\limp}_1}} on $B\hole$ as in the first case.
          \end{itemize}
          Notice that whenever we apply the {\rnmsf{Fcomm}} rule, it is to apply
          the rule corresponding to the head connective of $B\hole$ immediately
          afterwards: we never enter a loop by applying {\rnmsf{Fcomm}} twice in
          a row. Thus technically there are two reduction steps, but we treat
          them as one.
        \end{itemize}
        
        % suppose that $\mathcal{L'}$ is backward
        % and $\inv(C'\hole) = 0$. Then if we applied rule {\rnmsf{F{\limp}_1}} we
        % must have $\inv(C\hole) = 1$ and $\mathcal{L}$ is forward. Otherwise in
        % every other applicable rule the parity and operator are preserved, thus
        % $\inv(C\hole) = 0$ and $\mathcal{L}$ is backward. In both cases we have
        % that $\inv(B\hole) = \inv(B'\hole)$, and we want to show that
        % $\inv(B'\hole) = 0$. In the first case by Clause \ref{clause:opposite}
        % we know that the $\inv(B\hole)$ is even.
        
        % Thus for \rnmsf{R\!\!\limp_1} which is backward, we start with either
        % $(1,1)$ or $(0,2)$, and obtain $(1,0)$ or $(0,1)$ which are valid
        % according to Condition \ref{cond:pol} since $\mathcal{L'}$ is forward.
        % Conversely for \rnmsf{F\!\!\limp_1} which is forward, we must start with
        % $(1,0)$, and obtain $(0,0)$ which is valid since $\mathcal{L'}$ is
        % backward.

        \item Since we do not deal with quantifiers, we can just take the same
        unifier $\sigma$.

        \item Idem here, we take the same interleaving $l$.
      \end{enumerate}
    \end{itemize} 
  \end{enumerate}
\end{proof}

%% \begin{proof}
%%   See appendix \ref{prf:vprogress}.
%% \end{proof}

% Most of the magic occurs here, but by lack of space we had to move this proof to
% appendix \ref{?}.
Then we can state the following \emph{productivity theorem}, which is a direct
consequence of the previous lemma and the fact that the rewrite rules
terminate:

\begin{theorem}[Productivity]\labthm{productivity}
If $\mathcal{L}$ is a valid linkage, then there
is a sequence of reductions with one of the following forms:
\begin{mathpar}
  \mathcal{L} \steps D^+\select{\select{A} \back \select{A}} \\
  \mathcal{L} \steps D\select{\select{t} = u \link A\select{t}} \and
  \mathcal{L} \steps D\select{u = \select{t} \link A\select{t}}
\end{mathpar}
\end{theorem}

This is the formal counterpart to the notion of productivity mentioned in
\refsec{validity}. Intuitively, this theorem ensures non-trivial progress in the
reasoning: we managed to connect some dots in the problem and actually solve a
subgoal. That is, either the conclusion is \emph{strictly} weakened after a
backward DnD, or the assumptions are \emph{strictly} strengthened after a
forward DnD, instead of having just an equivalent goal written in a different
way\sidenote{This remark only applies to logical linkages however, since
rewriting equalities can only produce equivalent statements. Some proof
assistants provide facilities to rewrite arbitrary relations in subterms of
arbitrary depth, such as Coq with its \emph{generalized rewriting} mechanism
\cite{coqman-genrew}. This includes non-symmetric relations that can produce
non-equivalent statements, and there is no reason in principle it could not be
integrated in our paradigm, in the form of generalized substitution rules in
place of {\rnmsf{L{=}_i}} and {\rnmsf{F{=}_i}}.}. This again contrasts with the
release semantics of subformula linking which do not provide this guarantee of
productivity, or with the logical reasoning tactics of proof assistants based on
natural deduction rules.

%% \begin{proof}
%%   By recurrence on $\size{\mathcal{L}}$:
%%   \begin{itemize}
%%     \item If $\size{\mathcal{L}} = 0$, by lemma \ref{thm:vprogress} we know
%%     that $\mathcal{L} = \select{A} \vdash \select{A}$, thus we have the empty
%%     reduction.

%%     \item If $\size{\mathcal{L}} > 0$, by lemma \ref{thm:vprogress} we get
%%     $\mathcal{L} \step D\select{\mathcal{L'}}$ with $\mathcal{L'}$ valid, by lemma
%%     \ref{thm:decreasing} we know that $\size{\mathcal{L}} >
%%     \size{\mathcal{L'}}$, and thus we can apply the induction hypothesis on
%%     $\mathcal{L'}$ and conclude.
%%   \end{itemize}
%% \end{proof}

% \begin{lemma}[Progress]\label{thm:progress}
%   If a linkage $B\select{A}\vdash C\select{A'}$ (resp.
%   $B\select{A}* C\select{A'}$) is valid, then there exists a
%   derivation ending in $B\select{A}\vdash C\select{A'}$
%   (resp. $B\select{A} * C\select{A'}$).
% \end{lemma}
% \begin{proof}
%   By induction over the sizes of $B\hole$ and $C\hole$...
% \end{proof}


\section{Focusing}\labsec{invert}

% \sidenote{There are 7 invertible rules, and 23 non-invertible
%     rules to consider in \reffig{DISL} (we exclude the {\rnmsf{id}} and
%     {\rnmsf{Fcomm}} rules).}

A last point to deal with is non-confluence and in particular choosing
between first simplifying the head connective on the right or the left
of $\forw$ or $\back$. For instance in
$\select{A}\lor B \back B\lor\select{A}$ one can apply either
\rnmsf{L\lor_1} or \rnmsf{R\lor_2}.

Interestingly, an answer is provided by {\em focusing}. It has been noticed by
Andreoli~\sidecite{andreoli1992} that, in bottom-up proof search, one should
apply the invertible logical rules first since they preserve provability. In our
framework, this translates into first applying the invertible rewrite rules (the
ones marked by a *)\sidenote{Hopefully at some point, detailed proofs of
non-invertibility for intuitionistic/classical logic based on counter-models
will be provided in annex.}. In the case of the example above, this means
performing \rnmsf{L\lor_1} first, which leads to the following behavior:
$$\select{A}\lor B \back B\lor\select{A}\steps B\limp B\lor A.$$
This is indeed the ``right'' choice, since applying \rnmsf{R\lor_2} first would
lead to a dead-end\sidenote{Interestingly in this case it creates a dead-end
only in intuitionistic logic: in classical logic both results are provable.}:
$$\select{A}\lor B \back B\lor\select{A}\steps B\lor(B\limp A).$$

The general scheme for choosing a rule to apply to a redex $C\select{A} \link
D\select{B}$ is the following\sidenote{A less deterministic version of this
scheme is already present implicitly in the proof of Lemma \ref{thm:vprogress}}:
\begin{enumerate}
  \item If $C\hole \equiv D\hole \equiv \hole$, we just apply the {\rnmsf{id}} rule
  (assuming $A \equiv B$ by Lemma \ref{thm:vprogress}).
  \item If only one context is non-empty, say $C\hole$, we look at its head
  connective as well as the side where its hole resides:
  \begin{itemize}
    \item either $C\hole \equiv C_0\hole \mcirc E$ for some binary connective
    $\mcirc$, and we choose the rule {\rnmsf{L\mcirc_1}} (resp.
    {\rnmsf{F\mcirc_1}}) if $\link = \back$ (resp. $\link = \forw$);
    \item or $C\hole \equiv E \mcirc C_0\hole$ and we choose the rule
    {\rnmsf{L\mcirc_2}} (resp. {\rnmsf{F\mcirc_2}}) if $\link = \back$ (resp.
    $\link = \forw$).
  \end{itemize}
  In the case where it is $D\hole$ which is non-empty, we apply the same logic
  but with the right rules {\rnmsf{R\mcirc_i}} instead of the left rules
  {\rnmsf{L\mcirc_i}}.
  \item If both contexts are non-empty, then the previous logic determines one
  rule for $C\hole$ and one rule for $D\hole$, giving rise to the ambiguity
  described in the above example.
\end{enumerate}
  
There are three possibilities when analysing invertibility of the two rules in
the third case:
\begin{enumerate}
  \item if both are invertible, then the order of application does not matter
  since we preserve provability in the end;
  \item if only one is invertible, we apply it first following the focusing
  discipline;
  \item if neither are invertible, we want to choose the order that maximizes
  the preservation of provability. It turns out that in almost all cases the two
  rules commute, that is the formulas obtained in the two orderings are
  equivalent. The only exceptions are the critical pairs \rnmsf{F\!\!\lor_i
  /~F\!\!\limp_2} for $i \in \{1,2\}$, as was noted independently in
  \sidecite{DBLP:conf/cade/Chaudhuri21}. In this case, one should rely on
  information given by the user to choose the right ordering, which can be done
  by exploiting the \emph{orientation} of the associated DnD action, that is
  distinguishing between the source path and the destination path\sidenote{Note
  that in the current implementation of Actema, we instead rely on an arbitrary
  prioritizing fixed in the system, which can hinder in some cases the ability
  to prove a goal through DnD actions. In practice, one rarely encounters such
  cases in real examples.}.
\end{enumerate}
Currently we do not have detailed proofs of permutability for all pairs of
rules. The reason is mostly pragmatic: given the great number of rules, this
would take a lot of time to perform a full case analysis. Actually our claim of
permutability comes from \cite{DBLP:conf/cade/Chaudhuri21} which uses a
subformula linking system almost identical to ours. We hope we will be able to
provide rigorous proofs in annex when time permits.

% In the third case when two invertible rules can be applied, the order is
% irrelevant since provability is preserved in the end. There are cases where two
% non-invertible rules can be applied. The vast majority of them commute in terms
% of provability, but not necessarily in the shape of the resulting
% formula\sidenote{In fact the only rules which do not give equivalent results
% when commuted are the critical pairs \rnmsf{F\!\!\lor_i /~F\!\!\limp_2} for $i
% \in \{1,2\}$, as was noted independently in \cite{DBLP:conf/cade/Chaudhuri21}.}.
% Therefore our specification still leaves room for some choices. Currently, we
% have a heuristic prioritizing of the rules that sticks to what is presented in
% examples. One could also choose to leave the disambiguation to the user, e.g. by
% looking at the \emph{orientation} of drag-and-drops. This is the solution chosen
% in \cite{Chaudhuri2013} and \cite{DBLP:conf/cade/Chaudhuri21}.
% One might also just forbid cases that are too ambiguous or
% counter-intuitive (e.g. ).

% When two invertible rules can be applied, the order is irrelevant. There are
% many cases where two non-invertible rules can be applied. The vast majority of
% them commute in terms of provability\sidenote{Ideally, we might want a system
% where all non-invertible rules commute in provability, as is the case with
% standard focusing proof systems in the literature (\cite{}, \cite{}). Currently
% we treat problematic pairs on a case by case and ad hoc basis, the only known
% one being \rnmsf{F\!\!\limp_1}/\rnmsf{F\lor_1}.}, but not necessarily in the
% shape of the resulting formula. Therefore our specification still leaves room
% for some choices, which are currently based on a few heuristics in order to
% obtain the examples presented in sections \refsec{back}, \refsec{forw} and
% \refsec{quant}.


%% sect. title could be changed. Say that reversible rules are applied
%% first.

%% Let us take a break from technical considerations, and go back to our initial motivation : the
%% specification of an intuitive drag-and-drop tactic. To summarize, this tactic should take two unifiable subformulas $A$ and $A'$ of opposite polarities as input, and output a new formula $A''$.
%% Furthermore, this tactic can act in two modes, depending on the ``colors'' of the items $B\select{A}$ and $C\select{A'}$ holding our input :

%% \begin{itemize}
%%   \item the first mode corresponds to a drag-and-drop involving a blue item (hypothesis) and a
%%   red item (conclusion) that produces a red item, and is captured by the formal notion of a backward linkage $B\select{A} \vdash C\select{A'}$;
%%   \item the second mode corresponds to a drag-and-drop involving two blue items (hypotheses) that produces a blue item, and is captured by the formal notion of a forward linkage $B\select{A} \forw C\select{A'}$.
%% \end{itemize}

%% Now, what lemmas \ref{thm:correctness} and \ref{thm:productivity} guarantee is
%% that all valid linkages can be given meaning with a valid derivation, whose
%% conclusion is the linkage, and whose premise is the output formula $A''$. What
%% they do not specify is what precise shape this derivation should have (apart
%% from ending in an instance of the identity rule between $A$ and $A'$), and
%% consequently what will $A''$ look like. Of course if we want a consistent
%% behavior, the most basic requirement is that the tactic should be deterministic,
%% and always produce the same $A''$ given the same $B\select{A}$ and
%% $C\select{A'}$. Hence arises the question of \emph{choosing} a particular
%% derivation among all the possible ones. Unfortunately, the proof of lemma
%% \ref{thm:vprogress} shows that building such a derivation requires to choose at
%% each step which of the two formula contexts to decompose, which makes the number
%% of possible derivations exponential in t!!he size of $B$ and $C$. Thus we need a
%% principled way to tame this non-determinism.

%% This is where \emph{focusing} comes into play. Introduced originally in the context of linear logic programming by Andreoli \cite{andreoli_logic_1992}, focusing is a technique for eliminating all choices in the bottom-up search for a proof of a given goal. This is precisely what we are looking for, except that in our framework the goal is a linkage instead of a sequent, and we only require a partial proof, that is $A''$ can be any formula and not just $\top$. This removes the need for backtracking, which reflects the intended use of our tactic as a manual goal-manipulation tool, rather than an automated search procedure (but reveals an interesting connection between these seemingly unrelated approaches).

%% The main discovery of focusing is that every goal can be proved by following a strict 2-phases strategy:

%% \begin{enumerate}
%%   \item First, eagerly apply \emph{invertible} rules, that is rules whose conclusion entails their premisses.
%%   \item When no invertible rule is applicable, apply non-invertible rules until an invertible rule is available (and thus go back to phase 1).
%% \end{enumerate}

%% % TODO
%% \textit{Ajouter des exemples}

%% Not all proof systems are amenable to focusing, but most well-studied logics do have focusing systems, including first-order intuitionistic logic (\cite{liang_focusing_2009}). In those, rules can be applied in arbitrary order in both phases, thanks to a result known as the permutability of same-invertibility inference rules. While the original technique of focusing further exploits this result to impose a recursive decomposition strategy for connectives with non-invertible rules, we choose following \cite{Chaudhuri2013} to leave this order unspecified. This gives us more freedom in the design of the tactic, and in particular allows us to shape the output formula $A''$ in, we argue, a more intuitive way.\sidenote{We should mention that the ordering strategy currently implemented in our prototype is based on a few heuristics, whose consequences on the visible behavior of the tactic are still poorly understood.}

%% % TODO
%% \textit{On devrait mentionner quelquepart que la propriété de permutabilité n'est pas tout à fait vérifiée dans notre système à cause des règles forward pour $\lor$ et $\Rightarrow$ (et peut-être d'autres paires de règles).}

\section{Completeness}\labsec{dnd-completeness}

To enable a fully graphical approach to theorem proving that does not rely on a
textual proof language, it is important to show that (a subset of) the set of
actions exposed to the user is \emph{complete} with respect to provability. That
is, any formula $A$ which is \emph{true} in our logic --- here intuitionistic
FOL --- can be proved by executing a sequence of graphical actions that reduces
it to the empty goal. We noticed in Remark \ref{rem:click-completeness} that
click actions are a sufficient basis for completeness. While we believe that a
combination of both click and DnD actions is more comfortable to handle a
variety of proof situations, it is still interesting to consider the question of
completeness for DnD actions alone. It turns out that the answer is positive:
the mechanism of \emph{subformula linking} underlying DnD actions is powerful
enough to capture provability in FOL. This has already been shown by Chaudhuri
in \cite{Chaudhuri2013} for linear logic, and \cite{DBLP:conf/cade/Chaudhuri21}
for intuitionistic logic. Here we give a completeness proof for a system based
on a slight extension of our rewrite rules, following ideas from these works.

\begin{remark}
  What we prove in this section is a \emph{weak} form of completeness: we show
  that for any true formula, there always exists a derivation in our subformula
  linking system, but this derivation might not be constructible by the
  deterministic procedure outlined in \refsec{invert}. There are two aspects
  that make the stronger version hard to prove in practice:
  \begin{itemize}
    \item To show that the choices performed by the focusing procedure always
    allow to find a proof when there exists one, it would be necessary to
    formulate and prove a \emph{focusing theorem} based on the permutability of
    rules mentioned at the end of \refsec{invert}.
    \item Even then, some additional rules of our subformula linking system are
    not simulated in any way by the DnD procedure of \refsec{action}. We will come
    back to this point soon.
  \end{itemize}  
  % The gap between this theoretical completeness and the
  % practical completeness of the implementation should in part be bridged by the
  % focusing theorems of \refsec{focusing}, but we did not attempt a full
  % formalization of this. A venue for future work would be a certified
  % implementation of subformula linking in Coq as presented in \refch{plugin},
  % with an additional formal proof of completeness.
\end{remark}

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {C^+\select{A \limp B}}&\step&{C^+\select{A \back B}} &\mathsf{B}\\
        {C^-\select{A \land B}}&\step&{C^-\select{A \forw B}} &\mathsf{F}\\
    \end{array}
  \end{mathpar}
  \caption{Linkage formation rules}
  \labfig{newlink}
\end{marginfigure}

To the rewrite rules of \reffig{DISL} and \reffig{DISL-U}, we add \emph{linkage
formation} rules (\reffig{newlink}), which are a deep generalization of linkage
creation between two formulas of a sequent. Rule {\rnmsf{B}} creates a
\emph{backward} linkage between $\limp$-linked formulas in any positive context
$C^+\hole$, and dually rule {\rnmsf{F}} creates a \emph{forward} linkage between
$\land$-linked formulas in any negative context $C^-\hole$\sidenote{This is
reminiscent of the adjunction between the product and the exponential in
\emph{cartesian closed categories}, which respectively interpret conjunction and
implication in the Curry-Howard-Lambek correspondance for intuitionistic
logic.}. Note that contrary to the rules considered up to now, linkage formation
rules are not closed under arbitrary contexts: indeed the polarity restrictions
are necessary to ensure soundness, as reviewed in \refsec{soundness}. A backward
linkage in a sequent $\Gamma, D\select{A} \seq E\select{B}$ would be encoded by
the instance
$$C^+\select{D\select{A} \limp E\select{B}} \step C^+\select{D\select{A} \back
E\select{B}}$$ of {\rnmsf{B}} where $C^+\hole \equiv \bigwedge Γ \limp \hole$, while
a forward linkage in a sequent $\Gamma, D\select{A}, E\select{B} \seq F$ would
be encoded by the instance $$C^-\select{D\select{A} \land E\select{B}} \step
C^-\select{D\select{A} \forw E\select{B}}$$ of {\rnmsf{F}} where $C^-\hole \equiv
\bigwedge \Gamma \wedge \hole \limp F$.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {C^-\select{A}}&\step&{C^-\select{A \land A}} &\mathsf{conn}\\
        {C^-\select{A}}&\step&{C^-\select{\top}} &\mathsf{weak}\\
    \end{array}
  \end{mathpar}
  \caption{Resource rules}
  \labfig{reslink}
\end{marginfigure}

Another necessary ingredient is the addition of a deep version of the
\emph{structural rules} of sequent calculus. We already have the {\rnmsf{Fcomm}}
rule to handle commutativity of the $\forw$ operator, which acts as a kind of
\emph{exchange} rule. Then we add the equivalent of \emph{contraction} and
\emph{weakening} with the rules {\rnmsf{conn}} and {\rnmsf{weak}}
(\reffig{reslink}). These allow to erase and duplicate hypotheses at will, by
identifying any subformula occurring in a negative context as a hypothesis. Thus
once again we need to be careful about polarity, and cannot close these rules
under arbitrary contexts.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\,}c@{\,}lr}
        {C^+\select{A \limp B}}&\step&{C^+\select{A \limp (A \back B)}} &\mathsf{Bconn}\\
        {C^-\select{A \land B}}&\step&{C^-\select{A \land (A \forw B) \land B}} &\mathsf{Fconn}\\
    \end{array}
  \end{mathpar}
  \caption{Duplicating linkage formation rules}
  \labfig{newreslink}
\end{marginfigure}

An alternative to the full contraction rule {\rnmsf{conn}} is to systematically
duplicate negative formulas in the linkage formation rules, giving the rules
{\rnmsf{Bconn} and \rnmsf{Fconn}} of \reffig{newreslink}. This models more
closely what we do in Actema, where hypotheses involved in a DnD action are
always preserved in the new goal. This is important from a usability standpoint,
because this ensures the user needs not fret with manual duplication of
hypotheses in order to complete a proof. The downside is that the context always
grows bigger, but this can be balanced by exposing the weakening rule in the
interface. In Actema it is mapped to a ``delete'' button placed next to every
hypothesis (the little gray trashbin icons in \reffig{oneplusone}).
% But for the purpose of completeness only, the weakening rule {\rnmsf{weak}}
% should be admissible since its role is already played by the absorption rules
% {\rnmsf{absl}, \rnmsf{absr} and \rnmsf{absq}} of \reffig{DISL-U}.
In our completeness proof we will use all the rules in $\{\text{{\rnmsf{conn}},
{\rnmsf{weak}}, {\rnmsf{B}}, {\rnmsf{F}}, {\rnmsf{Bconn}}, {\rnmsf{Fconn}}}\}$,
in order to make derivations more concise.

Because linkages created by rules {\rnmsf{B} and \rnmsf{F}} are not
necessarily valid, one needs to add the so-called \emph{release} rules already
mentioned in \refsec{linkages} (\reffig{rellink}). In fact these rules are
crucial in order to simulate rules from sequent calculus, which will be the
backbone of our completeness proof as in \cite{Chaudhuri2013}. It is interesting
to consider the question of completeness without release rules, especially since
we do not use them in the semantics of DnD actions. We conjecture that it should
hold but would require a completely different argument, maybe of a more semantic
nature like the original proof of Gödel with Tarski models\sidenote{Or Kripke
models in an intuitionistic setting. }. Another possibility might be to use a
more canonical representation of proofs that is in-between syntax and semantics,
like the combinatorial proofs of Hughes \sidecite{Hughes_2006} which are known
to be closely related to deep inference proofs.

\begin{marginfigure}
  \begin{mathpar}
    \begin{array}{r@{\quad}c@{\quad}lr}
        {t = t}&\step&{\top} &\mathsf{refl}\\
    \end{array}
  \end{mathpar}
  \caption{Reflexivity rule for $=$}
  \labfig{refl-rule}
\end{marginfigure}

Lastly, a trivial but necessary addition is the rule {\rnmsf{refl}} of
\reffig{refl-rule} stating the reflexivity of $=$. It was not introduced before
because it is already handled by click actions on red items in Actema
(\refsec{clicks}), but here we want a self-contained system that models as
closely as possible the space of proofs that can be built through DnD actions
only. In this context, one could imagine restricting the usage of the
{\rnmsf{refl}} rule to the unit elimination phase (\refsec{action}), where it
would play the same role as the rules of \reffig{DISL-U}. Thus adding this rule
does not correspond morally to modelling a click action, nor to a modification
of the semantics of DnD actions as for release rules.

\todo{ It seems that the question of admissibility of the release rules is very
  similar to the question of admissibility of the {\rnmsf{fence}} rule in the
  flower calculus. Both serve the same purpose of enabling a local simulation of
  sequent calculus rules, but do not appear in real proofs because of the
  arbitrary deep/global power of link formation/iteration rules. }

Then we simply rely on the completeness of sequent calculus by performing a
proof by simulation. There are many variants of sequent calculi for
intuitionistic first-order logic described in the literature. In our case the
choice mostly does not matter: all we need is that it is \emph{analytic}, i.e.
satisifies the subformula property. Indeed the very idea of subformula linking
is based on analyticity: one should be able to prove a statement by the sole act
of linking sub-sentences already present in the statement.

\begin{marginfigure}
\begin{mathpar}
  \R[Ref]
    {\Gamma, t = t \seq C}
    {\Gamma \seq C}
  \\
  \top \quad\step\quad t = t \quad \mathsf{ref}
\end{mathpar}
\caption{Non-analytic reflexivity rules}
\labfig{ref-rule}
\end{marginfigure}

We chose the calculus \sys{G3i} from \cite{negri_structural_2001} as our basis,
because all structural rules are admissible in it (but they would be
straightforward to simulate apart from the cut rule). The first modification we
do is that we model hypotheses in sequents as lists instead of multisets, to
make the translation from sequents to formulas completely deterministic. Thus we
need to add the exchange rule {\rnm{exch}}, which is simulated straightforwardly
with the {\rnmsf{Fcomm}} rule as mentioned earlier. The second modification we
do is adding introduction rules {\rnm{{=}R}} and {\rnm{{=}L}} for equality. The
left introduction rule {\rnm{{=}L}} captures Leibniz's elimination scheme, and
is in fact the rule {\rnm{Repl}} from \cite{negri_structural_2001} (modulo the
fact that we use single-succedant instead of multi-succedant sequents). The
right introduction rule {\rnm{{=}R}} is an axiomatic reflexivity rule, instead
of the {\rnm{Ref}} rule from \cite{negri_structural_2001} (\reffig{ref-rule}).
The reason is that we cannot simulate the latter directly without adding its
equivalent rule {\rnmsf{ref}} to our calculus (\reffig{ref-rule}), which we do
not want to do because it would break the subformula property. We conjecture
that using {\rnm{{=}R}} instead of {\rnm{Ref}} does not break the cut
admissibility theorem from \cite{negri_structural_2001}.

% We could not find mention of such a rule in the literature, where instead people
% use either a non-analytic rule\todo{cite nlab at
% https://ncatlab.org/nlab/show/sequent+calculus}, a unification-based
% rule\todo{cite Dale Miller's paper}, or the more traditional axiomatic
% formulation of equality as a congruent equivalence relation instead of a left
% introduction rule\todo{cite someone}. But our rule looks reasonable enough, so
% we did not attempt a formal justification of soundness/completeness that would
% certainly lead us astray. The only problematic aspect would be if it breaks cut
% admissibility. We conjecture it does not, and postpone an update of the cut
% admissibility proof of \cite{negri_structural_2001} with our equality rules as
% future work.

\begin{theorem}[Completeness of subformula linking]\labthm{sfl-completeness}
  If $\Gamma \seq A$ is provable in $\text{\sys{G3i}} + \{exch, {=}R, {=}L\}$,
  then $\bigwedge \Gamma \limp A \steps \top$.
\end{theorem}
\begin{proof}
  By induction on the depth of the derivation of $\Gamma \seq A$. The base case
  simulates the rules {\rnm{ax}}, {\rnm{\top R}}, {\rnm{\bot L}} and
  {\rnm{{=}R}}. Other rules are simulated as usual by composing the derivations
  obtained from the induction hypotheses, making a crucial use of the release
  rules {\rnmsf{Brel}} and {\rnmsf{Frel}}. The full mapping from sequent
  calculus rules to derivations in our subformula linking calculus is given in
  the following table. Note that we treat conjunctive formulas modulo
  associativity to avoid bureaucratic details.

  \newcommand{\mt}{\qquad\mapsto\qquad}
  \begin{align*}
    \R[ax]
      {\Gamma, A \seq A}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \limp A & \\
      \step & \Gamma \land A \back A & \mathsf{B} \\
      \step & A \back A & \mathsf{L\land_2} \\
      \step & \top & \mathsf{id}
    \end{array}
    \\\\
    \R[exch]
      {\summ[$\pi_1$]{\Gamma, B, A, \Gamma' \seq C}}
      {\Gamma, A, B, \Gamma' \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \land B \land \Gamma' \limp C & \\
      \step & \Gamma \land (A \forw B) \land \Gamma' \limp C & \mathsf{F} \\
      \step & \Gamma \land (B \forw A) \land \Gamma' \limp C & \mathsf{Fcomm} \\
      \step & \Gamma \land B \land A \land \Gamma' \limp C & \mathsf{Frel} \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\top R]
      {}
      {\Gamma \seq \top}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \top & \\
      \step & \top & \mathsf{absr}
    \end{array}
    \\\\
    \R[\land R]
      {\summ[$\pi_1$]{\Gamma \seq A}}
      {\summ[$\pi_2$]{\Gamma \seq B}}
      {\Gamma \seq A \land B}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A \land B & \\
      \step & \Gamma \limp (\Gamma \back A \land B) & \mathsf{Bconn} \\
      \step & \Gamma \limp (\Gamma \back A) \land B & \mathsf{R\land_1} \\
      \step & \Gamma \limp (\Gamma \limp A) \land B & \mathsf{Brel} \\
      \steps & \Gamma \limp \top \land B & IH(\pi_1) \\
      \step & \Gamma \limp B & \mathsf{neul} \\
      \steps & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[\lor R_i]
      {\summ[$\pi_1$]{\Gamma \seq A_i}}
      {\Gamma \seq A_0 \lor A_1}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A_0 \lor A_1 & \\
      \step & \Gamma \back A_0 \lor A_1 & \mathsf{B} \\
      \step & (\Gamma \back A_i) \lor A_{1 - i} & \mathsf{R\lor_i} \\
      \step & (\Gamma \limp A_i) \lor A_{1 - i} & \mathsf{Brel} \\
      \steps & \top \lor A_{1 - i} & IH(\pi_1) \\
      \step & \top & \mathsf{absl}
    \end{array}
    \\\\
    \R[{\limp}R]
      {\summ[$\pi_1$]{\Gamma, A \seq B}}
      {\Gamma \seq A \limp B}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp A \limp B & \\
      \step & \Gamma \back A \limp B & \mathsf{B} \\
      \step & (\Gamma \forw A) \limp B & \mathsf{R{\limp}_1} \\
      \step & \Gamma \land A \limp B & \mathsf{Frel} \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \prftree[r][l]{$\forall R$}{$x \not\in \fv(\Gamma)$}
      {\summ[$\pi_1$]{\Gamma \seq A}}
      {\Gamma \seq \forall x. A}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \forall x. A & \\
      \step & \Gamma \back \forall x. A & \mathsf{B} \\
      \step & \forall x. (\Gamma \back A) & \mathsf{R\forall s} \\
      \step & \forall x. \Gamma \limp A & \mathsf{Brel} \\
      \steps & \forall x. \top & IH(\pi_1) \\
      \step & \top & \mathsf{absq}
    \end{array}
    \\\\
    \R[\exists R]
      {\summ[$\pi_1$]{\Gamma \seq \subst{A}{t}{x}}}
      {\Gamma \seq \exists x. A}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp \exists x. A & \\
      \step & \Gamma \back \exists x. A & \mathsf{B} \\
      \step & \Gamma \back \subst{A}{t}{x} & \mathsf{R\exists i} \\
      \step & \Gamma \limp \subst{A}{t}{x} & \mathsf{Brel} \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[{=}R]
      {}
      {\Gamma \seq t = t}
    &\mt
    \begin{array}{lll}
            & \Gamma \limp t = t & \\
      \step & \Gamma \limp \top & \mathsf{refl} \\
      \step & \top & \mathsf{absr}
    \end{array}
    \\\\
    \R[\top L]
      {\summ[$\pi_1$]{\Gamma \seq C}}
      {\Gamma, \top \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land \top \limp C & \\
      \step & \Gamma \limp C & \mathsf{neur} \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\bot L]
      {}
      {\Gamma, \bot \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land \bot \limp C & \\
      \step & \Gamma \land \bot \back C & \mathsf{B} \\
      \step & \bot \back C & \mathsf{L\land_2} \\
      \step & \bot \limp C & \mathsf {Brel} \\
      \step & \top & \mathsf{efq}
    \end{array}
    \\\\
    \R[\land L]
      {\summ[$\pi_1$]{\Gamma, A, B \seq C}}
      {\Gamma, A \land B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land A \land B \limp C & \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \R[\lor L]
      {\summ[$\pi_1$]{\Gamma, A \seq C}}
      {\summ[$\pi_2$]{\Gamma, B \seq C}}
      {\Gamma, A \lor B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (A \lor B) \limp C & \\
      \step & \Gamma \land (A \lor B) \limp (\Gamma \land (A \lor B) \back C) & \mathsf{Bconn} \\
      \step & \Gamma \land \top \limp (\Gamma \land (A \lor B) \back C) & \mathsf{weak} \\
      \step & \Gamma \limp (\Gamma \land (A \lor B) \back C) & \mathsf{neur} \\
      \step & \Gamma \limp (A \lor B \back C) & \mathsf{L\land_2} \\
      \step & \Gamma \limp (A \back C) \land (B \limp C) & \mathsf{L\lor_1} \\
      \step & \Gamma \limp (A \limp C) \land (B \limp C) & \mathsf{Brel} \\
      \step & \Gamma \limp (\Gamma \back (A \limp C) \land (B \limp C)) & \mathsf{Bconn} \\
      \step & \Gamma \limp (\Gamma \back A \limp C) \land (B \limp C) & \mathsf{R\land_1} \\
      \step & \Gamma \limp ((\Gamma \forw A) \limp C) \land (B \limp C) & \mathsf{R{\limp}_1} \\
      \step & \Gamma \limp (\Gamma \land A \limp C) \land (B \limp C) & \mathsf{Frel} \\
      \steps & \Gamma \limp \top \land (B \limp C) & IH(\pi_1) \\
      \step & \Gamma \limp B \limp C & \mathsf{neul} \\
      \step & \Gamma \back B \limp C & \mathsf{B} \\
      \step & (\Gamma \forw B) \limp C & \mathsf{R{\limp}_1} \\
      \step & \Gamma \land B \limp C & \mathsf{Frel} \\
      \steps & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[{\limp}L]
      {\summ[$\pi_1$]{\Gamma, A \limp B \seq A}}
      {\summ[$\pi_2$]{\Gamma, B \seq C}}
      {\Gamma, A \limp B \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (A \limp B) \limp C & \\
      \step & \Gamma \land \Gamma \land (A \limp B) \limp C & \mathsf{conn} \\
      \step & \Gamma \land (\Gamma \forw A \limp B) \limp C & \mathsf{F} \\
      \step & \Gamma \land ((\Gamma \back A) \limp B) \limp C & \mathsf{F{\limp}_1} \\
      \step & \Gamma \land ((\Gamma \limp A) \limp B) \limp C & \mathsf{Brel} \\
      \steps & \Gamma \land (\top \limp B) \limp C & IH(\pi_1) \\
      \step & \Gamma \land B \limp C & \mathsf{neul} \\
      \steps & \top & IH(\pi_2)
    \end{array}
    \\\\
    \R[\forall L]
      {\summ[$\pi_1$]{\Gamma, \subst{A}{t}{x} \seq C}}
      {\Gamma, \forall x. A \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (\forall x. A) \limp C & \\
      \step & \Gamma \land (\forall x. A) \limp (\Gamma \land (\forall x. A) \back C) & \mathsf{Bconn} \\
      \step & \Gamma \land (\forall x. A) \limp (\forall x. A \back C) & \mathsf{L\land_2} \\
      \step & \Gamma \land \top \limp (\forall x. A \back C) & \mathsf{weak} \\
      \step & \Gamma \limp (\forall x. A \back C) & \mathsf{neur} \\
      \step & \Gamma \limp (\subst{A}{t}{x} \back C) & \mathsf{L\forall i} \\
      \step & \Gamma \limp (\subst{A}{t}{x} \limp C) & \mathsf{Brel} \\
      \step & \Gamma \back (\subst{A}{t}{x} \limp C) & \mathsf{B} \\
      \step & (\Gamma \forw \subst{A}{t}{x}) \limp C & \mathsf{R{\limp}_1} \\
      \step & \Gamma \land \subst{A}{t}{x} \limp C & \mathsf{Frel} \\
      \steps & \top & IH(\pi_1)
    \end{array}
    \\\\
    \prftree[r][l]{$\exists L$}{$x \not\in \fv(\Gamma) \cup \fv(C)$}
      {\summ[$\pi_1$]{\Gamma, A \seq C}}
      {\Gamma, \exists x. A \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land (\exists x. A) \limp C & \\
      \step & \Gamma \land (\exists x. A) \limp (\Gamma \land (\exists x. A) \back C) & \mathsf{Bconn} \\
      \step & \Gamma \land (\exists x. A) \limp (\exists x. A \back C) & \mathsf{L\land_2} \\
      \step & \Gamma \land \top \limp (\exists x. A \back C) & \mathsf{weak} \\
      \step & \Gamma \limp (\exists x. A \back C) & \mathsf{neur} \\
      \step & \Gamma \limp \forall x. (A \back C) & \mathsf{L\exists s} \\
      \step & \Gamma \limp \forall x. A \limp C & \mathsf{Brel} \\
      \step & \Gamma \back \forall x. A \limp C & \mathsf{B} \\
      \step & \forall x. (\Gamma \back A \limp C) & \mathsf{R\forall s} \\
      \step & \forall x. (\Gamma \forw A) \limp C & \mathsf{R{\limp}_1} \\
      \step & \forall x. \Gamma \land A \limp C & \mathsf{Frel} \\
      \steps & \forall x. \top & IH(\pi_1) \\
      \step & \top & \mathsf{absq}
    \end{array}
    \\\\
    \R[{=}L]
      {\summ[$\pi_1$]{\Gamma, t = u, \subst{A}{u}{x}, \subst{A}{t}{x} \seq C}}
      {\Gamma, t = u, \subst{A}{t}{x} \seq C}
    &\mt
    \begin{array}{lll}
            & \Gamma \land t = u \land \subst{A}{t}{x} \limp C & \\
      \step & \Gamma \land t = u \land (t = u \forw \subst{A}{t}{x}) \land \subst{A}{t}{x} \limp C & \mathsf{Fconn} \\
      \step & \Gamma \land t = u \land \subst{A}{u}{x} \land \subst{A}{t}{x} \limp C & \mathsf{F{=}_1} \\
      \steps & \top & IH(\pi_1)
    \end{array}
  \end{align*}
\end{proof}